# Guide Converter - Portable Document

This document enables any AI assistant to convert llm-d guide configurations into llm-d-benchmark scenario and experiment files.

## How to Use

1. Copy this entire document into your AI assistant (ChatGPT, Copilot, Cursor, or any other)
2. Then provide your conversion request with one of these formats:
   - "Convert https://github.com/llm-d/llm-d/tree/main/guides/inference-scheduling"
   - "Convert the pd-disaggregation guide with guidellm harness"
   - "Convert this values.yaml: [paste content]"
3. The AI will generate scenario and experiment files for you

---

## Purpose

This converter transforms llm-d deployment guides (Helm values files) into llm-d-benchmark scenario and experiment files. It:

- Extracts configuration from Helm values YAML
- Maps values to `LLMDBENCH_*` environment variables
- Generates scenario files (shell scripts with exports)
- Generates experiment files (YAML for parameter sweeps)

## Workflow

### Step 1: Parse the Guide

Extract from the guide's values.yaml:
- Model configuration (`modelArtifacts.name`, `modelArtifacts.size`)
- Decode settings (`decode.replicas`, `decode.parallelism.*`, `decode.containers[0].args`)
- Prefill settings (if present)
- Environment variables and volumes
- vLLM launch arguments

### Step 2: Map to LLMDBENCH Variables

Use the mapping reference below to convert Helm values to environment variables.

### Step 3: Generate Files

Create:
1. **Scenario file**: `scenarios/guides/<guide-name>.sh`
2. **Experiment file**: `experiments/<guide-name>.yaml`

---

## Mapping Reference

### Core Mappings

| Helm Path | LLMDBENCH Variable |
|-----------|-------------------|
| `modelArtifacts.name` | `LLMDBENCH_DEPLOY_MODEL_LIST` |
| `modelArtifacts.size` | `LLMDBENCH_VLLM_COMMON_PVC_MODEL_CACHE_SIZE` |
| `decode.replicas` | `LLMDBENCH_VLLM_MODELSERVICE_DECODE_REPLICAS` |
| `decode.parallelism.tensor` | `LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM` |
| `decode.parallelism.data` | `LLMDBENCH_VLLM_MODELSERVICE_DECODE_DATA_PARALLELISM` |
| `decode.containers[0].resources.requests.cpu` | `LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_NR` |
| `decode.containers[0].resources.requests.memory` | `LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_MEM` |
| `prefill.replicas` | `LLMDBENCH_VLLM_MODELSERVICE_PREFILL_REPLICAS` |
| `prefill.parallelism.tensor` | `LLMDBENCH_VLLM_MODELSERVICE_PREFILL_TENSOR_PARALLELISM` |

### vLLM Arguments

| Argument | LLMDBENCH Variable |
|----------|-------------------|
| `--max-model-len` | `LLMDBENCH_VLLM_COMMON_MAX_MODEL_LEN` |
| `--block-size` | `LLMDBENCH_VLLM_COMMON_BLOCK_SIZE` |
| `--gpu-memory-utilization` | `LLMDBENCH_VLLM_COMMON_ACCELERATOR_MEM_UTIL` |
| `--tensor-parallel-size` | Use decode/prefill parallelism variables |
| `--enable-prefix-caching` | Include in `DECODE_EXTRA_ARGS` |
| `--kv-transfer-config` | Include in `DECODE_EXTRA_ARGS` for P/D disaggregation |

### Common Variables

| LLMDBENCH Variable | Default | Description |
|-------------------|---------|-------------|
| `LLMDBENCH_VLLM_COMMON_TENSOR_PARALLELISM` | `1` | Tensor parallel size |
| `LLMDBENCH_VLLM_COMMON_MAX_MODEL_LEN` | `16384` | Maximum model length |
| `LLMDBENCH_VLLM_COMMON_BLOCK_SIZE` | `64` | KV cache block size |
| `LLMDBENCH_VLLM_COMMON_CPU_NR` | `4` | CPU cores |
| `LLMDBENCH_VLLM_COMMON_CPU_MEM` | `40Gi` | CPU memory |
| `LLMDBENCH_VLLM_COMMON_SHM_MEM` | `16Gi` | Shared memory |
| `LLMDBENCH_VLLM_COMMON_ACCELERATOR_MEM_UTIL` | `0.95` | GPU memory utilization |
| `LLMDBENCH_VLLM_COMMON_PVC_MODEL_CACHE_SIZE` | `300Gi` | PVC size for model cache |
| `LLMDBENCH_HARNESS_NAME` | `inference-perf` | Default load generator |
| `LLMDBENCH_HARNESS_EXPERIMENT_PROFILE` | `sanity_random.yaml` | Default workload profile |

### GPU Count Calculation

GPU count is automatically calculated as `tensor_parallelism * data_local_parallelism`. Use `LLMDBENCH_VLLM_MODELSERVICE_DECODE_ACCELERATOR_NR=auto` (recommended).

---

## Output Templates

### Scenario File Template

```bash
# <GUIDE_NAME> WELL LIT PATH
# Based on <SOURCE_URL_OR_PATH>
# Generated by llm-d-benchmark guide converter

# IMPORTANT NOTE
# All parameters not defined here will use default values from setup/env.sh

# Model parameters
export LLMDBENCH_DEPLOY_MODEL_LIST="<model_name>"

# PVC parameters
export LLMDBENCH_VLLM_COMMON_PVC_MODEL_CACHE_SIZE=<size>

# Common vLLM parameters
export LLMDBENCH_VLLM_COMMON_MAX_MODEL_LEN=<value>
export LLMDBENCH_VLLM_COMMON_TENSOR_PARALLELISM=<value>

# Decode parameters
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_REPLICAS=<value>
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM=$LLMDBENCH_VLLM_COMMON_TENSOR_PARALLELISM
# ... additional decode settings

# Prefill parameters (if applicable)
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_REPLICAS=<value>
# ... additional prefill settings

# Workload parameters
export LLMDBENCH_HARNESS_NAME=<harness>
export LLMDBENCH_HARNESS_EXPERIMENT_PROFILE=<profile>.yaml

# Local directory for results
export LLMDBENCH_CONTROL_WORK_DIR=~/data/<guide_name>
```

### Experiment File Template

```yaml
setup:
  factors:
    - <FACTOR_NAME>
  levels:
    <FACTOR_NAME>: "<value1>, <value2>, ..."
  treatments:
    - "<value1>"
    - "<value2>"

run:
  factors:
    - <run_factor>
  levels:
    <run_factor>: "<values>"
  treatments:
    - "<treatment1>"
    - "<treatment2>"
```

---

## Snapshot Examples

### Example Scenario: inference-scheduling.sh

This is a real scenario file from the llm-d-benchmark repository showing the expected format and patterns:

```bash
# INFERENCE SCHEDULING WELL LIT PATH
# Based on https://github.com/llm-d/llm-d/tree/main/guides/inference-scheduling
# Removed pod monitoring; can be added using LLMDBENCH_VLLM_MODELSERVICE_EXTRA_POD_CONFIG
# Removed extra volumes metrics-volume and torch-compile-volume; they are not needed for this model and tested hardware.
# Use LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS and LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES to add them if needed.

# IMPORTANT NOTE
# All parameters not defined here or exported externally will be the default values found in setup/env.sh
# Many commonly defined values were left blank (default) so that this scenario is applicable to as many environments as possible.

# Model parameters
#export LLMDBENCH_DEPLOY_MODEL_LIST="Qwen/Qwen3-0.6B"
export LLMDBENCH_DEPLOY_MODEL_LIST="Qwen/Qwen3-32B"

# PVC parameters
#             Storage class (leave uncommented to automatically detect the "default" storage class)
#export LLMDBENCH_VLLM_COMMON_PVC_STORAGE_CLASS=standard-rwx
export LLMDBENCH_VLLM_COMMON_PVC_MODEL_CACHE_SIZE=1Ti

# Routing configuration (via modelservice)
export LLMDBENCH_VLLM_MODELSERVICE_INFERENCE_MODEL=true # (default is "false")

# Common parameters across standalone and llm-d (prefill and decode) pods
export LLMDBENCH_VLLM_COMMON_MAX_MODEL_LEN=16000
export LLMDBENCH_VLLM_COMMON_BLOCK_SIZE=64
export LLMDBENCH_VLLM_COMMON_CPU_NR=16
export LLMDBENCH_VLLM_COMMON_CPU_MEM=64Gi
export LLMDBENCH_VLLM_COMMON_SHM_MEM=16Gi
export LLMDBENCH_VLLM_COMMON_TENSOR_PARALLELISM=1
export LLMDBENCH_VLLM_COMMON_DATA_PARALLELISM=1

export LLMDBENCH_VLLM_COMMON_PREPROCESS="python3 /setup/preprocess/set_llmdbench_environment.py; source \$HOME/llmdbench_env.sh"

# Environment variables exported to vLLM containers (via temp file)
export LLMDBENCH_VLLM_COMMON_ENVVARS_TO_YAML=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_COMMON_ENVVARS_TO_YAML
- name: UCX_TLS
  value: "sm,cuda_ipc,cuda_copy,tcp"
- name: UCX_SOCKADDR_TLS_PRIORITY
  value: "tcp"
- name: VLLM_NIXL_SIDE_CHANNEL_PORT
  value: "REPLACE_ENV_LLMDBENCH_VLLM_COMMON_NIXL_SIDE_CHANNEL_PORT"
- name: VLLM_LOGGING_LEVEL
  value: INFO
- name: VLLM_ALLOW_LONG_MAX_MODEL_LEN
  value: "1"
EOF

# Extra container configuration (ports)
export LLMDBENCH_VLLM_COMMON_EXTRA_CONTAINER_CONFIG=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_COMMON_EXTRA_CONTAINER_CONFIG}
ports:
  - containerPort: REPLACE_ENV_LLMDBENCH_VLLM_COMMON_NIXL_SIDE_CHANNEL_PORT
    protocol: TCP
  - containerPort: REPLACE_ENV_LLMDBENCH_VLLM_COMMON_METRICS_PORT
    name: metrics
    protocol: TCP
EOF

# Extra volume mounts
export LLMDBENCH_VLLM_COMMON_EXTRA_VOLUME_MOUNTS=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_COMMON_EXTRA_VOLUME_MOUNTS}
- name: dshm
  mountPath: /dev/shm
- name: preprocesses
  mountPath: /setup/preprocess
EOF

# Extra volumes
export LLMDBENCH_VLLM_COMMON_EXTRA_VOLUMES=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_COMMON_EXTRA_VOLUMES}
- name: preprocesses
  configMap:
    defaultMode: 0755
    name: llm-d-benchmark-preprocesses
- name: dshm
  emptyDir:
    medium: Memory
    sizeLimit: REPLACE_ENV_LLMDBENCH_VLLM_COMMON_SHM_MEM
EOF

# Prefill parameters: 0 prefill pods (decode-only mode)
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_REPLICAS=0

# Decode parameters: 1 decode pod
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_REPLICAS=1
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM=$LLMDBENCH_VLLM_COMMON_TENSOR_PARALLELISM
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_NR=$LLMDBENCH_VLLM_COMMON_CPU_NR
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_MEM=$LLMDBENCH_VLLM_COMMON_CPU_MEM
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_SHM_MEM=$LLMDBENCH_VLLM_COMMON_SHM_MEM
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_TO_YAML=${LLMDBENCH_VLLM_COMMON_ENVVARS_TO_YAML}
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_CONTAINER_CONFIG=${LLMDBENCH_VLLM_COMMON_EXTRA_CONTAINER_CONFIG}
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS=${LLMDBENCH_VLLM_COMMON_EXTRA_VOLUME_MOUNTS}
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES=${LLMDBENCH_VLLM_COMMON_EXTRA_VOLUMES}
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_ACCELERATOR_NR=auto
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_MODEL_COMMAND=custom
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS=$LLMDBENCH_VLLM_COMMON_PREPROCESS

# Custom vLLM serve command
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS
REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS; \
vllm serve /model-cache/models/REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--host 0.0.0.0 \
--served-model-name REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--port REPLACE_ENV_LLMDBENCH_VLLM_COMMON_METRICS_PORT \
--block-size REPLACE_ENV_LLMDBENCH_VLLM_COMMON_BLOCK_SIZE \
--max-model-len REPLACE_ENV_LLMDBENCH_VLLM_COMMON_MAX_MODEL_LEN \
--tensor-parallel-size REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM \
--gpu-memory-utilization REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_ACCELERATOR_MEM_UTIL \
--kv-transfer-config "{\"kv_connector\":\"NixlConnector\",\"kv_role\":\"kv_both\"}" \
--enforce-eager \
--disable-log-requests \
--disable-uvicorn-access-log
EOF

# Workload parameters
export LLMDBENCH_HARNESS_NAME=inference-perf
export LLMDBENCH_HARNESS_EXPERIMENT_PROFILE=shared_prefix_synthetic.yaml

# Local directory to copy benchmark runtime files and results
export LLMDBENCH_CONTROL_WORK_DIR=~/data/inference-scheduling
```

### Example Experiment: inference-scheduling.yaml

```yaml
setup:
  factors:
    - LLMDBENCH_VLLM_MODELSERVICE_GAIE_PLUGINS_CONFIGFILE
  levels:
    LLMDBENCH_VLLM_MODELSERVICE_GAIE_PLUGINS_CONFIGFILE: "inf-sche-none.yaml, inf-sche-prefix.yaml, inf-sche-kv.yaml, inf-sche-queue.yaml"
  treatments:
    - "inf-sche-none.yaml"
    - "inf-sche-prefix.yaml"
    - "inf-sche-kv.yaml"
    - "inf-sche-queue.yaml"
run:
# Harness and workload profile are defined on scenarios/guides
  factors:
    - question_len
    - output_len
  levels:
    question_len: "100,300,1000"
    output_len: "100,300,1000"
  treatments:
    - "100,100"
    - "100,300"
    - "100,1000"
    - "300,100"
    - "300,300"
    - "300,1000"
    - "1000,100"
    - "1000,300"
    - "1000,1000"
```

---

## Available Harnesses and Profiles

### Harnesses

| Harness | Description |
|---------|-------------|
| `inference-perf` | Default harness, comprehensive performance testing |
| `guidellm` | Alternative load generator |
| `vllm-benchmark` | vLLM native benchmarking |
| `nop` | No-op harness for testing |

### Profiles (inference-perf)

| Profile | Use Case |
|---------|----------|
| `sanity_random.yaml` | Quick validation tests |
| `chatbot_synthetic.yaml` | Chat workload simulation |
| `chatbot_sharegpt.yaml` | Real conversation patterns |
| `shared_prefix_synthetic.yaml` | Prefix caching tests |
| `shared_prefix_multi_turn_chat.yaml` | Multi-turn conversations |
| `summarization_synthetic.yaml` | Long context summarization |
| `code_completion_synthetic.yaml` | Code completion patterns |
| `random_concurrent.yaml` | Concurrent request stress |

### Profiles (guidellm)

| Profile | Use Case |
|---------|----------|
| `sanity_random.yaml` | Basic validation |
| `sanity_concurrent.yaml` | Concurrent validation |
| `chatbot_synthetic.yaml` | Chat simulation |
| `shared_prefix_synthetic.yaml` | Prefix caching |
| `summarization_synthetic.yaml` | Summarization |

### Profiles (vllm-benchmark)

| Profile | Use Case |
|---------|----------|
| `sanity_random.yaml` | Basic validation |
| `random_concurrent.yaml` | Concurrent stress test |
| `sharegpt.yaml` | ShareGPT dataset |

---

## Handling Complex Configurations

### Extra Args with Custom Commands

When the guide uses `modelCommand: custom`, use the heredoc pattern with `REPLACE_ENV_*` placeholders:

```bash
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_MODEL_COMMAND=custom

export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS
vllm serve /model-cache/models/REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--host 0.0.0.0 \
--port REPLACE_ENV_LLMDBENCH_VLLM_COMMON_METRICS_PORT \
--max-model-len REPLACE_ENV_LLMDBENCH_VLLM_COMMON_MAX_MODEL_LEN \
--tensor-parallel-size REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM
EOF
```

### Extra Volumes and Mounts

For additional volumes (like shared memory):

```bash
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES}
- name: dshm
  emptyDir:
    medium: Memory
    sizeLimit: REPLACE_ENV_LLMDBENCH_VLLM_COMMON_SHM_MEM
EOF

export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS}
- name: dshm
  mountPath: /dev/shm
EOF
```

### Environment Variables

For container environment variables:

```bash
export LLMDBENCH_VLLM_COMMON_ENVVARS_TO_YAML=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_COMMON_ENVVARS_TO_YAML
- name: VLLM_LOGGING_LEVEL
  value: INFO
- name: UCX_TLS
  value: "sm,cuda_ipc,cuda_copy,tcp"
EOF
```

---

## Common REPLACE_ENV Placeholders

| Placeholder | Description |
|------------|-------------|
| `REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL` | Current model being deployed |
| `REPLACE_ENV_LLMDBENCH_VLLM_COMMON_MAX_MODEL_LEN` | Maximum model length |
| `REPLACE_ENV_LLMDBENCH_VLLM_COMMON_BLOCK_SIZE` | KV cache block size |
| `REPLACE_ENV_LLMDBENCH_VLLM_COMMON_METRICS_PORT` | Metrics port |
| `REPLACE_ENV_LLMDBENCH_VLLM_COMMON_INFERENCE_PORT` | Inference port |
| `REPLACE_ENV_LLMDBENCH_VLLM_COMMON_NIXL_SIDE_CHANNEL_PORT` | NIXL side channel port |
| `REPLACE_ENV_LLMDBENCH_VLLM_COMMON_SHM_MEM` | Shared memory size |
| `REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM` | Decode tensor parallelism |
| `REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_ACCELERATOR_MEM_UTIL` | Decode GPU memory utilization |

---

## Deriving Guide Name

Extract the guide name from the source:

1. **From URL**: Use the last path segment before `values.yaml`
   - `https://github.com/llm-d/llm-d/tree/main/guides/inference-scheduling` -> `inference-scheduling`

2. **From Local Path**: Use the filename without extension, or parent directory name
   - `/path/to/my-guide.yaml` -> `my-guide`
   - `/path/to/inference-scheduling/values.yaml` -> `inference-scheduling`

3. **Sanitization**: Replace spaces and special characters with hyphens, lowercase

---

## Error Handling

1. **Invalid URL**: Report the error and suggest checking the URL
2. **Invalid YAML**: Show the error and problematic section
3. **Unknown Helm Values**: Log unmapped values as comments in the scenario file
4. **Missing Required Values**: Warn if critical values (like model name) are missing
