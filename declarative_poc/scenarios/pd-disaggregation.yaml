scenario:
  - name: "sut-1"
    prepare:
      user-overrides:
        secrets:
        - name: llm-d-hf-token
          secret: HF_TOKEN
          contents: TYLERS_TOKEN
    system:
      user-overrides:
        - name: "default"
          inference-engine:
            model:
              - name: meta-llama/Llama-3.1-8B-Instruct
                label: .auto
                maxlen: 16000
                blocksize: 128
            replicas:
              decode: 2
              prefill: 2
            parallelism:
              decode:
                tensor: 1
              prefill:
                tensor: 1
            resources:
              decode:
                memory: 128Gi
                cpu: 32
              prefill:
                memory: 128Gi
                cpu: 32
          volumes:
            - name: model-storage
              size: 1Ti
          command:
            decode:
              type: vllmServe
              args:
                - "--block-size"
                - "128"
                - "--kv-transfer-config"
                - '{"kv_connector":"NixlConnector","kv_role":"kv_both"}'
                - "--disable-log-requests"
                - "--disable-uvicorn-access-log"
                - "--max-model-len"
                - "16000"
            prefill:
              type: vllmServe
              args:
                - "--block-size"
                - "128"
                - "--kv-transfer-config"
                - '{"kv_connector":"NixlConnector","kv_role":"kv_both"}'
                - "--disable-log-requests"
                - "--disable-uvicorn-access-log"
                - "--max-model-len"
                - "16000"
    harness:
      user-overrides:
        - name: default
          namespace: harnessns
          harness:
            name: vllm-benchmark
            profile: random_concurrent.yaml