{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7099745a-82a5-494a-bac2-565fd9729eaf",
   "metadata": {},
   "source": [
    "# <font color=\"#000099\">llm-d-benchmarking Sweep Analysis</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3776a034-d0cd-40e4-b4f1-7d1cb5dcdd4d",
   "metadata": {},
   "source": [
    "This notebook imports data from configuration sweeps with [llm-d-benchmark](https://github.com/llm-d/llm-d-benchmark) using the [vLLM benchmark](https://github.com/vllm-project/vllm/tree/main/benchmarks) harness, and creates Pareto plots to compare configurations for a particular model and workload.\n",
    "\n",
    "The first cell contains function and class definitions to support basic functionality, while the second cell imports data from user-provided directories into [Pandas](https://pandas.pydata.org/) [DataFrames](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html). Two different DataFrames are created, one for prefill/decode disaggregated setups, and one for standalone (standard vLLM) setups.\n",
    "\n",
    "The cells following will look at the different scenarios (model, GPU, and workload input/output size) and create tables and Pareto plots for different configurations under these scenarios.\n",
    "\n",
    "While this basic functionality may be sufficient for many purposes, this notebook should be considered a starting point for more detailed analysis and customization by the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788d1ce3-22fc-402c-a9d5-7e31b1653a67",
   "metadata": {},
   "source": [
    "## Package imports and definitions (run once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f5c2a6-937b-49b1-8f65-99bb7d00e784",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Package imports\n",
    "################################################################################\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import yaml\n",
    "\n",
    "################################################################################\n",
    "# Function and class definitions\n",
    "################################################################################\n",
    "\n",
    "class Text:\n",
    "    \"\"\"ANSI SGR control codes for text formatting\"\"\"\n",
    "    DEFAULT = \"\\x1b[0m\"\n",
    "    BOLD = \"\\x1b[1m\"\n",
    "    BOLD_OFF = \"\\x1b[22m\"\n",
    "    UNDERLINE = \"\\x1b[4m\"\n",
    "    UNDERLINE_OFF = \"\\x1b[24m\"\n",
    "    DEFAULT_COLOR = \"\\x1b[39m\"\n",
    "    DEFAULT_BG_COLOR = \"\\x1b[49m\"\n",
    "    RED = \"\\x1b[31m\"\n",
    "    YELLOW = \"\\x1b[33m\"\n",
    "    GREEN = \"\\x1b[32m\"\n",
    "    CYAN = \"\\x1b[36m\"\n",
    "    BLUE = \"\\x1b[34m\"\n",
    "    MAGENTA = \"\\x1b[35m\"\n",
    "    BLACK = \"\\x1b[30m\"\n",
    "    WHITE = \"\\x1b[37m\"\n",
    "    BG_RED = \"\\x1b[41m\"\n",
    "    BG_YELLOW = \"\\x1b[43m\"\n",
    "    BG_GREEN = \"\\x1b[42m\"\n",
    "    BG_CYAN = \"\\x1b[46m\"\n",
    "    BG_BLUE = \"\\x1b[44m\"\n",
    "    BG_MAGENTA = \"\\x1b[45m\"\n",
    "    BG_BLACK = \"\\x1b[40m\"\n",
    "    BG_WHITE = \"\\x1b[47m\"\n",
    "\n",
    "\n",
    "def warn(mesg: str) -> None:\n",
    "    \"\"\"Print a warning message.\"\"\"\n",
    "    sys.stderr.write(f'{Text.YELLOW}{mesg}\\n{Text.DEFAULT}')\n",
    "\n",
    "\n",
    "def error(mesg: str, err_code: int = 1) -> None:\n",
    "    \"\"\"Print an error message and exit with an error code.\"\"\"\n",
    "    sys.stderr.write(f'{Text.RED}{mesg}\\n{Text.DEFAULT}')\n",
    "    sys.exit(err_code)\n",
    "\n",
    "\n",
    "def check_source_dir(source_dir: str) -> None:\n",
    "    \"\"\"Print an error if source directory does not exist.\"\"\"\n",
    "    if not os.path.isdir(source_dir):\n",
    "        error(f'Invalid path: {source_dir}')\n",
    "\n",
    "\n",
    "def _get_pd_sweep_dirs(source_dir: str) -> list[str]:\n",
    "    \"\"\"Get all immediate child directories within a source directory that\n",
    "    correspond to PD sweeps.\"\"\"\n",
    "    sweep_dirs = []\n",
    "    for file in os.listdir(source_dir):\n",
    "        if not os.path.isdir(os.path.join(source_dir, file)):\n",
    "            # Skip files that are not directories\n",
    "            continue\n",
    "        if not re.search('.+\\\\_\\\\_[0-9]+P-TP[0-9]+\\\\_[0-9]+D-TP[0-9]+$', file):\n",
    "            # Skip directories that do not match a swept run pattern\n",
    "            continue\n",
    "        sweep_dirs.append(os.path.join(source_dir, file))\n",
    "\n",
    "    sweep_dirs.sort()\n",
    "    return sweep_dirs\n",
    "\n",
    "\n",
    "def _get_sa_sweep_dirs(source_dir: str) -> list[str]:\n",
    "    \"\"\"Get all immediate child directories within a source directory that\n",
    "    correspond to standalone sweeps.\"\"\"\n",
    "    sweep_dirs = []\n",
    "    for file in os.listdir(source_dir):\n",
    "        if not os.path.isdir(os.path.join(source_dir, file)):\n",
    "            # Skip files that are not directories\n",
    "            continue\n",
    "        if not re.search('.+\\\\_\\\\_[0-9]+R-TP[0-9]+$', file):\n",
    "            # Skip directories that do not match a swept run pattern\n",
    "            continue\n",
    "        sweep_dirs.append(os.path.join(source_dir, file))\n",
    "\n",
    "    sweep_dirs.sort()\n",
    "    return sweep_dirs\n",
    "\n",
    "\n",
    "def get_sweep_dirs(source_dirs: list[str]) -> tuple[list[str], list[str]]:\n",
    "    \"\"\"Get all immediate child directories within a source directory that\n",
    "    correspond to sweeps, spit into P/D and standalone.\"\"\"\n",
    "    pd_sweep_dirs = []\n",
    "    sa_sweep_dirs = []\n",
    "    for s_dir in source_dirs:\n",
    "        check_source_dir(s_dir)\n",
    "        # Get P/D run directories\n",
    "        pd_sweep_dirs.extend(_get_pd_sweep_dirs(s_dir))\n",
    "        # Get standalone run directories\n",
    "        sa_sweep_dirs.extend(_get_sa_sweep_dirs(s_dir))\n",
    "    \n",
    "        if not pd_sweep_dirs and not sa_sweep_dirs:\n",
    "            error(f'No run directories found in source directory: {s_dir}')\n",
    "    return (pd_sweep_dirs, sa_sweep_dirs)\n",
    "\n",
    "\n",
    "def make_pd_df() -> pandas.core.frame.DataFrame:\n",
    "    \"\"\"Create DataFrame for PD benchmark run results.\"\"\"\n",
    "    return pandas.DataFrame(columns=[\n",
    "        'Name',\n",
    "        'Directory',\n",
    "        'Model',\n",
    "        'GPU',\n",
    "        'P_TP',\n",
    "        'P_Replicas',\n",
    "        'D_TP',\n",
    "        'D_Replicas',\n",
    "        'Concurrency',\n",
    "        'ISL',\n",
    "        'OSL',\n",
    "        'Date',\n",
    "        'Backend',\n",
    "        'Num_Prompts',\n",
    "        'Request_Rate',\n",
    "        'Burstiness',\n",
    "        'Duration',\n",
    "        'Completed',\n",
    "        'Total_Input_Tokens',\n",
    "        'Total_Output_Tokens',\n",
    "        'Request_Throughput',\n",
    "        'Request_Goodput',\n",
    "        'Output_Throughput',\n",
    "        'Total_Token_Throughput',\n",
    "        'Mean_TTFT_ms',\n",
    "        'Median_TTFT_ms',\n",
    "        'Std_TTFT_ms',\n",
    "        'P90_TTFT_ms',\n",
    "        'P95_TTFT_ms',\n",
    "        'P99_TTFT_ms',\n",
    "        'Mean_TPOT_ms',\n",
    "        'Median_TPOT_ms',\n",
    "        'Std_TPOT_ms',\n",
    "        'P90_TPOT_ms',\n",
    "        'P95_TPOT_ms',\n",
    "        'P99_TPOT_ms',\n",
    "        'Mean_ITL_ms',\n",
    "        'Median_ITL_ms',\n",
    "        'Std_ITL_ms',\n",
    "        'P90_ITL_ms',\n",
    "        'P95_ITL_ms',\n",
    "        'P99_ITL_ms',\n",
    "        'Mean_E2EL_ms',\n",
    "        'Median_E2EL_ms',\n",
    "        'Std_E2EL_ms',\n",
    "        'P90_E2EL_ms',\n",
    "        'P95_E2EL_ms',\n",
    "        'P99_E2EL_ms',\n",
    "    ])\n",
    "\n",
    "\n",
    "def make_sa_df() -> pandas.core.frame.DataFrame:\n",
    "    \"\"\"Create DataFrame for standalone benchmark run results.\"\"\"\n",
    "    return pandas.DataFrame(columns=[\n",
    "        'Name',\n",
    "        'Directory',\n",
    "        'Model',\n",
    "        'GPU',\n",
    "        'TP',\n",
    "        'Replicas',\n",
    "        'Concurrency',\n",
    "        'ISL',\n",
    "        'OSL',\n",
    "        'Date',\n",
    "        'Backend',\n",
    "        'Num_Prompts',\n",
    "        'Request_Rate',\n",
    "        'Burstiness',\n",
    "        'Duration',\n",
    "        'Completed',\n",
    "        'Total_Input_Tokens',\n",
    "        'Total_Output_Tokens',\n",
    "        'Request_Throughput',\n",
    "        'Request_Goodput',\n",
    "        'Output_Throughput',\n",
    "        'Total_Token_Throughput',\n",
    "        'Mean_TTFT_ms',\n",
    "        'Median_TTFT_ms',\n",
    "        'Std_TTFT_ms',\n",
    "        'P90_TTFT_ms',\n",
    "        'P95_TTFT_ms',\n",
    "        'P99_TTFT_ms',\n",
    "        'Mean_TPOT_ms',\n",
    "        'Median_TPOT_ms',\n",
    "        'Std_TPOT_ms',\n",
    "        'P90_TPOT_ms',\n",
    "        'P95_TPOT_ms',\n",
    "        'P99_TPOT_ms',\n",
    "        'Mean_ITL_ms',\n",
    "        'Median_ITL_ms',\n",
    "        'Std_ITL_ms',\n",
    "        'P90_ITL_ms',\n",
    "        'P95_ITL_ms',\n",
    "        'P99_ITL_ms',\n",
    "        'Mean_E2EL_ms',\n",
    "        'Median_E2EL_ms',\n",
    "        'Std_E2EL_ms',\n",
    "        'P90_E2EL_ms',\n",
    "        'P95_E2EL_ms',\n",
    "        'P99_E2EL_ms',\n",
    "    ])\n",
    "\n",
    "\n",
    "def import_yaml(file_path: str) -> dict[any, any]:\n",
    "    \"\"\"Import a JSON/YAML file as a dict.\"\"\"\n",
    "    if not os.path.isfile(file_path):\n",
    "        error(f'File does not exist: {file_path}')\n",
    "    with open(file_path, 'r', encoding='UTF-8') as file:\n",
    "        data = yaml.safe_load(file)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_results_files(run_dir: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Get list of results files from run.\n",
    "\n",
    "    If a particular workload has multiple results files, pick newest based on\n",
    "    filename's date.\n",
    "    \"\"\"\n",
    "    results_files = []\n",
    "    if not os.path.isdir(run_dir):\n",
    "        warn(f'Invalid run directory: {run_dir}')\n",
    "        return results_files\n",
    "    if not os.path.isdir(os.path.join(run_dir, 'results')):\n",
    "        warn(f'\"results\" directory missing in run: {run_dir}')\n",
    "        return results_files\n",
    "    # Within the results directory of a run, there can be several benchmarks\n",
    "    for benchmark in os.listdir(os.path.join(run_dir, 'results')):\n",
    "        if not os.path.isdir(os.path.join(run_dir, 'results', benchmark)):\n",
    "            continue\n",
    "        # Sort files by newest first\n",
    "        files_sorted = sorted(\n",
    "            os.listdir(os.path.join(run_dir, 'results', benchmark)),\n",
    "            # Sorting by modified time will not work if files are copied\n",
    "            # locally in arbitrary order\n",
    "            #key=lambda ff: os.path.getmtime(os.path.join(run_dir, \"results\", benchmark, ff)),\n",
    "            reverse=True)\n",
    "        for file in files_sorted:\n",
    "            if not os.path.isfile(os.path.join(run_dir, 'results', benchmark, file)):\n",
    "                continue\n",
    "            if not re.search('^vllm.+\\\\.json$', file):\n",
    "                # Skip files that do not match result data filename\n",
    "                continue\n",
    "            results_files.append(os.path.join(run_dir, 'results', benchmark, file))\n",
    "            break\n",
    "    return results_files\n",
    "\n",
    "\n",
    "def get_launcher_yaml(sweep_dir: str) -> dict[str, any]:\n",
    "    \"\"\"Get information on the pod_benchmark-launcher.yaml file.\"\"\"\n",
    "    launcher_yaml_path = os.path.join(sweep_dir, 'setup', 'yamls',\n",
    "                                    'pod_benchmark-launcher.yaml')\n",
    "    launcher = import_yaml(launcher_yaml_path)\n",
    "    # Check file contents before returning\n",
    "    try:\n",
    "        launcher['spec']['containers'][0]['env'][0]\n",
    "    except (KeyError, IndexError):\n",
    "        error(f'\"spec.containers[0].env[0]\" field missing: {launcher_yaml_path}')\n",
    "    return launcher\n",
    "\n",
    "\n",
    "def _get_workload_profile_v01(sweep_dir: str) -> dict[str, any]:\n",
    "    \"\"\"Get workload profile file from a sweep.\n",
    "\n",
    "    This works for datasets obtained prior to release v0.2.\n",
    "    Deprecated, to be removed in a future release.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(sweep_dir):\n",
    "        error(f'Invalid run directory: {sweep_dir}')\n",
    "    if not os.path.isdir(os.path.join(sweep_dir, 'workload', 'profiles')):\n",
    "        error(f'\"workload/profiles\" directory missing in sweep: {sweep_dir}')\n",
    "    # Get the workload file (there should be only one, and we will assume this)\n",
    "    for file in os.listdir(os.path.join(sweep_dir, 'workload', 'profiles')):\n",
    "        if os.path.isdir(os.path.join(sweep_dir, 'workload', 'profiles', file)):\n",
    "            continue\n",
    "        if not re.search('.+\\\\.yaml$', file):\n",
    "            # Skip files that do not match result data filename\n",
    "            continue\n",
    "        return import_yaml(os.path.join(sweep_dir, 'workload', 'profiles', file))\n",
    "\n",
    "\n",
    "def _get_workload_profile_v02(sweep_dir: str) -> dict[str, any]:\n",
    "    \"\"\"Get workload profile file from a sweep, using v0.2 structure.\"\"\"\n",
    "    launcher = get_launcher_yaml(sweep_dir)\n",
    "    profile_name = ''\n",
    "    harness_name = ''\n",
    "    for kv in launcher['spec']['containers'][0]['env']:\n",
    "        if 'name' not in kv or 'value' not in kv:\n",
    "            error(f'Invalid \"spec.containers[0].env\" entry in launcher: {sweep_dir}')\n",
    "        if kv['name'] == 'LLMDBENCH_RUN_EXPERIMENT_HARNESS_WORKLOAD_NAME':\n",
    "            profile_name = kv['value']\n",
    "        if kv['name'] == 'LLMDBENCH_HARNESS_NAME':\n",
    "            harness_name = kv['value']\n",
    "        if profile_name and harness_name:\n",
    "            break\n",
    "    if not profile_name:\n",
    "        error(f'Workload profile could not be found in in launcher: {sweep_dir}')\n",
    "    if not harness_name:\n",
    "        error(f'Harness name could not be found in in launcher: {sweep_dir}')\n",
    "    profile_dir_relative = os.path.join('workload', 'profiles', harness_name)\n",
    "    profile_dir = os.path.join(sweep_dir, profile_dir_relative)\n",
    "    if not os.path.isdir(profile_dir):\n",
    "        # Exception below is temporary, in order to support v0.1 imports, to be\n",
    "        # removed in future release\n",
    "        raise Exception(f'\"{profile_dir_relative}\" directory missing in sweep: {sweep_dir}')\n",
    "        error(f'\"{profile_dir_relative}\" directory missing in sweep: {sweep_dir}')\n",
    "    profile_path = os.path.join(profile_dir, profile_name + '.yaml')\n",
    "    if not os.path.isfile(profile_path):\n",
    "        error(f'Cannot find workload profile: {profile_path}')\n",
    "    return import_yaml(profile_path)\n",
    "\n",
    "\n",
    "def get_workload_profile(sweep_dir: str) -> dict[str, any]:\n",
    "    \"\"\"Get workload profile file from a sweep.\"\"\"\n",
    "    profile = {}\n",
    "    try:\n",
    "        profile = _get_workload_profile_v02(sweep_dir)\n",
    "    except:\n",
    "        warn(f'Sweep may not match v0.2 structure, trying v0.1: {sweep_dir}')\n",
    "        profile = _get_workload_profile_v01(sweep_dir)\n",
    "    return profile\n",
    "\n",
    "\n",
    "def get_envar(sweep_dir: str, envar: str) -> str:\n",
    "    \"\"\"Get value of environment variable in environment/variables file of sweep.\"\"\"\n",
    "    if not os.path.isdir(sweep_dir):\n",
    "        error(f'Invalid run directory: {sweep_dir}')\n",
    "    if not os.path.isdir(os.path.join(sweep_dir, \"environment\")):\n",
    "        error(f'\"environment\" directory missing in run: {sweep_dir}')\n",
    "    if not os.path.isfile(os.path.join(sweep_dir, \"environment\", \"variables\")):\n",
    "        error(f'\"variables\" file missing in run: {os.path.join(sweep_dir, \"environment\")}')\n",
    "    with open(os.path.join(sweep_dir, \"environment\", \"variables\"), \"r\", encoding=\"UTF-8\") as file:\n",
    "        for line in file:\n",
    "            if envar in line:\n",
    "                model = line.rsplit('=', 1)[-1].strip()\n",
    "                if not model:\n",
    "                    error(f'{envar} not defined: {sweep_dir}')\n",
    "                return model\n",
    "        error(f'{envar} missing from environment/variables: {sweep_dir}')\n",
    "\n",
    "\n",
    "def populate_pd_df(runs_df: pandas.core.frame.DataFrame, sweep_dirs: list[str]) -> None:\n",
    "    \"\"\"Populate PD dataframe with results from a list of PD sweeps.\"\"\"\n",
    "    for s_dir in sweep_dirs:\n",
    "        results_files = get_results_files(s_dir)\n",
    "        model = get_envar(s_dir, 'LLMDBENCH_DEPLOY_MODEL_LIST')\n",
    "        gpu = get_envar(s_dir, 'LLMDBENCH_VLLM_COMMON_AFFINITY').rsplit(':', 1)[-1]\n",
    "        name, config_str = s_dir.rsplit('__', 1)\n",
    "        name = name.rsplit('/', 1)[-1]\n",
    "        p_rep = int(config_str.split('P-TP', 1)[0])\n",
    "        p_tp = int(config_str.split('P-TP', 1)[1].split('_', 1)[0])\n",
    "        d_rep = int(config_str.rsplit('_', 1)[1].split('D-TP', 1)[0])\n",
    "        d_tp = int(config_str.split('D-TP', 1)[1])\n",
    "        workload_profile = get_workload_profile(s_dir)\n",
    "        for rf in results_files:\n",
    "            result_data = import_yaml(rf)\n",
    "            runs_df.loc[len(runs_df)] = {\n",
    "                'Name': name,\n",
    "                'Directory': s_dir,\n",
    "                'Model': model,\n",
    "                'GPU': gpu,\n",
    "                'P_TP': p_tp,\n",
    "                'P_Replicas': p_rep,\n",
    "                'D_TP': d_tp,\n",
    "                'D_Replicas': d_rep,\n",
    "                'Concurrency': result_data['max_concurrency'],\n",
    "                'ISL': workload_profile['random-input-len'],\n",
    "                'OSL': workload_profile['random-output-len'],\n",
    "                'Date': result_data['date'],\n",
    "                'Backend': result_data['backend'],\n",
    "                'Num_Prompts': result_data['num_prompts'],\n",
    "                'Request_Rate': result_data['request_rate'],\n",
    "                'Burstiness': result_data['burstiness'],\n",
    "                'Duration': result_data['duration'],\n",
    "                'Completed': result_data['completed'],\n",
    "                'Total_Input_Tokens': result_data['total_input_tokens'],\n",
    "                'Total_Output_Tokens': result_data['total_output_tokens'],\n",
    "                'Request_Throughput': result_data['request_throughput'],\n",
    "                'Request_Goodput': result_data['request_goodput'],\n",
    "                'Output_Throughput': result_data['output_throughput'],\n",
    "                'Total_Token_Throughput': result_data['total_token_throughput'],\n",
    "                'Mean_TTFT_ms': result_data['mean_ttft_ms'],\n",
    "                'Median_TTFT_ms': result_data['median_ttft_ms'],\n",
    "                'Std_TTFT_ms': result_data['std_ttft_ms'],\n",
    "                'P90_TTFT_ms': result_data['p90_ttft_ms'],\n",
    "                'P95_TTFT_ms': result_data['p95_ttft_ms'],\n",
    "                'P99_TTFT_ms': result_data['p99_ttft_ms'],\n",
    "                'Mean_TPOT_ms': result_data['mean_tpot_ms'],\n",
    "                'Median_TPOT_ms': result_data['median_tpot_ms'],\n",
    "                'Std_TPOT_ms': result_data['std_tpot_ms'],\n",
    "                'P90_TPOT_ms': result_data['p90_tpot_ms'],\n",
    "                'P95_TPOT_ms': result_data['p95_tpot_ms'],\n",
    "                'P99_TPOT_ms': result_data['p99_tpot_ms'],\n",
    "                'Mean_ITL_ms': result_data['mean_itl_ms'],\n",
    "                'Median_ITL_ms': result_data['median_itl_ms'],\n",
    "                'Std_ITL_ms': result_data['std_itl_ms'],\n",
    "                'P90_ITL_ms': result_data['p90_itl_ms'],\n",
    "                'P95_ITL_ms': result_data['p95_itl_ms'],\n",
    "                'P99_ITL_ms': result_data['p99_itl_ms'],\n",
    "                'Mean_E2EL_ms': result_data['mean_e2el_ms'],\n",
    "                'Median_E2EL_ms': result_data['median_e2el_ms'],\n",
    "                'Std_E2EL_ms': result_data['std_e2el_ms'],\n",
    "                'P90_E2EL_ms': result_data['p90_e2el_ms'],\n",
    "                'P95_E2EL_ms': result_data['p95_e2el_ms'],\n",
    "                'P99_E2EL_ms': result_data['p99_e2el_ms'],\n",
    "            }\n",
    "    # Add calculated columns\n",
    "    runs_df['Num_GPUs'] = runs_df['P_TP']*runs_df['P_Replicas'] + runs_df['D_TP']*runs_df['D_Replicas']\n",
    "    runs_df['Thpt_per_GPU'] = runs_df['Output_Throughput']/runs_df['Num_GPUs']\n",
    "    runs_df['Thpt_per_User'] = runs_df['Output_Throughput']/runs_df['Concurrency']\n",
    "\n",
    "\n",
    "def populate_sa_df(runs_df: pandas.core.frame.DataFrame, sweep_dirs: list[str]) -> None:\n",
    "    \"\"\"Populate standalone dataframe with results from a list of standalone sweeps.\"\"\"\n",
    "    for s_dir in sweep_dirs:\n",
    "        results_files = get_results_files(s_dir)\n",
    "        model = get_envar(s_dir, 'LLMDBENCH_DEPLOY_MODEL_LIST')\n",
    "        gpu = get_envar(s_dir, 'LLMDBENCH_VLLM_COMMON_AFFINITY').rsplit(':', 1)[-1]\n",
    "        name, config_str = s_dir.rsplit('__', 1)\n",
    "        name = name.rsplit('/', 1)[-1]\n",
    "        rep = int(config_str.split('R-TP', 1)[0])\n",
    "        tp = int(config_str.split('R-TP', 1)[-1])\n",
    "        workload_profile = get_workload_profile(s_dir)\n",
    "        for rf in results_files:\n",
    "            result_data = import_yaml(rf)\n",
    "            runs_df.loc[len(runs_df)] = {\n",
    "                'Name': name,\n",
    "                'Directory': s_dir,\n",
    "                'Model': model,\n",
    "                'GPU': gpu,\n",
    "                'TP': tp,\n",
    "                'Replicas': rep,\n",
    "                'Concurrency': result_data['max_concurrency'],\n",
    "                'ISL': workload_profile['random-input-len'],\n",
    "                'OSL': workload_profile['random-output-len'],\n",
    "                'Date': result_data['date'],\n",
    "                'Backend': result_data['backend'],\n",
    "                'Num_Prompts': result_data['num_prompts'],\n",
    "                'Request_Rate': result_data['request_rate'],\n",
    "                'Burstiness': result_data['burstiness'],\n",
    "                'Duration': result_data['duration'],\n",
    "                'Completed': result_data['completed'],\n",
    "                'Total_Input_Tokens': result_data['total_input_tokens'],\n",
    "                'Total_Output_Tokens': result_data['total_output_tokens'],\n",
    "                'Request_Throughput': result_data['request_throughput'],\n",
    "                'Request_Goodput': result_data['request_goodput'],\n",
    "                'Output_Throughput': result_data['output_throughput'],\n",
    "                'Total_Token_Throughput': result_data['total_token_throughput'],\n",
    "                'Mean_TTFT_ms': result_data['mean_ttft_ms'],\n",
    "                'Median_TTFT_ms': result_data['median_ttft_ms'],\n",
    "                'Std_TTFT_ms': result_data['std_ttft_ms'],\n",
    "                'P90_TTFT_ms': result_data['p90_ttft_ms'],\n",
    "                'P95_TTFT_ms': result_data['p95_ttft_ms'],\n",
    "                'P99_TTFT_ms': result_data['p99_ttft_ms'],\n",
    "                'Mean_TPOT_ms': result_data['mean_tpot_ms'],\n",
    "                'Median_TPOT_ms': result_data['median_tpot_ms'],\n",
    "                'Std_TPOT_ms': result_data['std_tpot_ms'],\n",
    "                'P90_TPOT_ms': result_data['p90_tpot_ms'],\n",
    "                'P95_TPOT_ms': result_data['p95_tpot_ms'],\n",
    "                'P99_TPOT_ms': result_data['p99_tpot_ms'],\n",
    "                'Mean_ITL_ms': result_data['mean_itl_ms'],\n",
    "                'Median_ITL_ms': result_data['median_itl_ms'],\n",
    "                'Std_ITL_ms': result_data['std_itl_ms'],\n",
    "                'P90_ITL_ms': result_data['p90_itl_ms'],\n",
    "                'P95_ITL_ms': result_data['p95_itl_ms'],\n",
    "                'P99_ITL_ms': result_data['p99_itl_ms'],\n",
    "                'Mean_E2EL_ms': result_data['mean_e2el_ms'],\n",
    "                'Median_E2EL_ms': result_data['median_e2el_ms'],\n",
    "                'Std_E2EL_ms': result_data['std_e2el_ms'],\n",
    "                'P90_E2EL_ms': result_data['p90_e2el_ms'],\n",
    "                'P95_E2EL_ms': result_data['p95_e2el_ms'],\n",
    "                'P99_E2EL_ms': result_data['p99_e2el_ms'],\n",
    "            }\n",
    "    # Add calculated columns\n",
    "    runs_df['Num_GPUs'] = runs_df['TP']*runs_df['Replicas']\n",
    "    runs_df['Thpt_per_GPU'] = runs_df['Output_Throughput']/runs_df['Num_GPUs']\n",
    "    runs_df['Thpt_per_User'] = runs_df['Output_Throughput']/runs_df['Concurrency']\n",
    "\n",
    "\n",
    "def get_scenarios(runs_df: pandas.core.frame.DataFrame) -> list[tuple[str]]:\n",
    "    \"\"\"Get a list of available scenarios from runs DataFrame, where\n",
    "    configurations and concurrency will be swept\"\"\"\n",
    "    columns = ['Model', 'GPU', 'ISL', 'OSL']\n",
    "    return list(set(runs_df.set_index(columns).index))\n",
    "\n",
    "\n",
    "def print_scenarios(scenarios: list[str]) -> None:\n",
    "    \"\"\"Print a formatted table of scenarios.\"\"\"\n",
    "    columns = ['Model', 'GPU', 'ISL', 'OSL']\n",
    "    # Get maximum text length for each column, including header\n",
    "    spans = list(map(len, columns))\n",
    "    for sc in scenarios:\n",
    "        for jj, item in enumerate(sc):\n",
    "            if spans[jj] < len(str(item)):\n",
    "                spans[jj] = len(str(item))\n",
    "    \n",
    "    header = f'{Text.BOLD}{Text.BLUE}IDX  {Text.DEFAULT}{Text.BOLD}'\n",
    "    for ii, col in enumerate(columns):\n",
    "        header += col + \" \" * (spans[ii] - len(col) + 2)\n",
    "    header += f'{Text.DEFAULT}'\n",
    "    print(header)\n",
    "    for ii, sc in enumerate(scenarios):\n",
    "        row = f'{Text.BLUE}{ii}{Text.DEFAULT}' + \" \" * (5 - len(str(ii)))\n",
    "        for jj, val in enumerate(sc):\n",
    "            row += f'{str(val)}' + \" \" * (spans[jj] - len(str(val)) + 2)\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64c4e28-fd50-4f2f-86a5-f9390d5f2968",
   "metadata": {},
   "source": [
    "## Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041839c2-0d6a-44b1-b7e1-de6cc8a575ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# User inputs\n",
    "################################################################################\n",
    "\n",
    "# List of directories containing sweep directories to import.\n",
    "# These can be a mix of PD and standalone sweeps.\n",
    "# Only sweep directories that are direct children of these directories will be\n",
    "# imported.\n",
    "source_dirs = [\n",
    "    \"/files/\",\n",
    "]\n",
    "\n",
    "################################################################################\n",
    "# Standard code\n",
    "################################################################################\n",
    "\n",
    "# Create blank DataFrames for prefill/decode and standalone configurations\n",
    "pd_runs = make_pd_df()\n",
    "sa_runs = make_sa_df()\n",
    "\n",
    "# Look through provided source directories for immediate child directories\n",
    "# containing sweep data\n",
    "pd_sweep_dirs, sa_sweep_dirs = get_sweep_dirs(source_dirs)\n",
    "# Populate DataFrames\n",
    "populate_pd_df(pd_runs, pd_sweep_dirs)\n",
    "populate_sa_df(sa_runs, sa_sweep_dirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1638a4e2-204e-45b7-a3ef-f19510421d60",
   "metadata": {},
   "source": [
    "# PD Disaggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e21648c-7286-4f48-ac57-4f61764ceb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenarios available, sweeping P and D replicas/TP configurations and concurrency\n",
    "pd_scenarios = get_scenarios(pd_runs)\n",
    "print_scenarios(pd_scenarios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955501bc-de64-435a-819b-dc04435827ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# User inputs\n",
    "################################################################################\n",
    "\n",
    "# Select scenario\n",
    "idx = 0\n",
    "\n",
    "# Segregate traces by directory (directories with identical scenarios, such as\n",
    "# repeated runs, will not be joined together in a single trace)\n",
    "seg_by_dir = True\n",
    "\n",
    "################################################################################\n",
    "# Standard code\n",
    "################################################################################\n",
    "\n",
    "# Get parameters of selected scenario\n",
    "model, gpu, isl, osl = pd_scenarios[idx]\n",
    "\n",
    "# Filter on column values\n",
    "pd_runs_selected = pd_runs[\n",
    "    (pd_runs['Model'] == model) &\n",
    "    (pd_runs['GPU'] == gpu) &\n",
    "    (pd_runs['ISL'] == isl) &\n",
    "    (pd_runs['OSL'] == osl)][[\n",
    "    'Model',\n",
    "    'GPU',\n",
    "    'P_TP',\n",
    "    'P_Replicas',\n",
    "    'D_TP',\n",
    "    'D_Replicas',\n",
    "    'Concurrency',\n",
    "    'ISL',\n",
    "    'OSL',\n",
    "    'Output_Throughput',\n",
    "    'Thpt_per_GPU',\n",
    "    'Thpt_per_User',\n",
    "    'Directory']].drop('Model', axis=1).drop('GPU', axis=1).drop('ISL', axis=1).drop('OSL', axis=1)#.sort_values(by='Output_Throughput')\n",
    "\n",
    "# Plot performance results\n",
    "colors = ['#FF0000', '#FFAA00', '#DDDD00', '#00DD00', '#00FFFF', '#0000FF',\n",
    "          '#FF00FF', '#666666', '#000000']\n",
    "\n",
    "# Unique configurations of replicas and TP\n",
    "if seg_by_dir:\n",
    "    columns = ['Model', 'GPU', 'ISL', 'OSL', 'Directory']\n",
    "    scenarios = list(set(pd_runs.set_index(columns).index))\n",
    "    configs = list(set(pd_runs_selected.set_index(['P_Replicas', 'P_TP', 'D_Replicas', 'D_TP', 'Directory']).index))\n",
    "else:\n",
    "    pd_runs_selected = pd_runs_selected.drop('Directory', axis=1)\n",
    "    configs = list(set(pd_runs_selected.set_index(['P_Replicas', 'P_TP', 'D_Replicas', 'D_TP']).index))\n",
    "configs.sort()\n",
    "# Sweep through configurations\n",
    "for ii, conf in enumerate(configs):\n",
    "    # Make a DataFrame for specific configuration\n",
    "    if seg_by_dir:\n",
    "        conf_df = pd_runs_selected[\n",
    "            (pd_runs_selected['P_Replicas'] == conf[0]) &\n",
    "            (pd_runs_selected['P_TP'] == conf[1]) &\n",
    "            (pd_runs_selected['D_Replicas'] == conf[2]) &\n",
    "            (pd_runs_selected['D_TP'] == conf[3]) &\n",
    "            (pd_runs_selected['Directory'] == conf[4])\n",
    "        ].sort_values(by='Concurrency')\n",
    "    else:\n",
    "        conf_df = pd_runs_selected[\n",
    "            (pd_runs_selected['P_Replicas'] == conf[0]) &\n",
    "            (pd_runs_selected['P_TP'] == conf[1]) &\n",
    "            (pd_runs_selected['D_Replicas'] == conf[2]) &\n",
    "            (pd_runs_selected['D_TP'] == conf[3])\n",
    "        ].sort_values(by='Concurrency')\n",
    "    display(conf_df)\n",
    "\n",
    "    # Plot throughputs for configuration\n",
    "    plt.plot(conf_df.Thpt_per_User, conf_df.Thpt_per_GPU,\n",
    "             label=f'{conf[0]}P-TP={conf[1]} {conf[2]}D-TP={conf[3]}',\n",
    "             marker='o', markersize=4,\n",
    "             color=colors[ii%len(colors)]\n",
    "            )\n",
    "    for jj, val in enumerate(conf_df.Concurrency):\n",
    "        plt.text(list(conf_df.Thpt_per_User)[jj],\n",
    "                 list(conf_df.Thpt_per_GPU)[jj]+pd_runs_selected['Thpt_per_GPU'].max()*0.02,\n",
    "                 str(val), ha='center', color=colors[ii%len(colors)])\n",
    "\n",
    "plt.title(f'GPU: {gpu}\\nModel: {model}\\nISL: {isl}  OSL: {osl}')\n",
    "plt.xlabel('Tok/s/User', fontsize='16')\n",
    "plt.ylabel('Tok/s/GPU', fontsize='16')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.grid(True, linewidth=1, ls='--', color='gray')\n",
    "plt.axis([0, None, 0, None])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb94b14-9bba-490d-9586-91b964d59480",
   "metadata": {},
   "source": [
    "# Standalone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb4e214-ef12-4276-9e9d-7bb22e21163a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenarios available, sweeping replicas/TP configurations and concurrency\n",
    "sa_scenarios = get_scenarios(sa_runs)\n",
    "print_scenarios(sa_scenarios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8906d3c-3052-45ea-8289-be4ad4e586f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# User inputs\n",
    "################################################################################\n",
    "\n",
    "# Select scenario\n",
    "idx = 0\n",
    "\n",
    "# Segregate traces by directory (directories with identical scenarios, such as\n",
    "# repeated runs, will not be joined together in a single trace)\n",
    "seg_by_dir = True\n",
    "\n",
    "################################################################################\n",
    "# Standard code\n",
    "################################################################################\n",
    "\n",
    "# Get parameters of selected scenario\n",
    "model, gpu, isl, osl = sa_scenarios[idx]\n",
    "\n",
    "# Filter on column values\n",
    "sa_runs_selected = sa_runs[\n",
    "    (sa_runs['Model'] == model) &\n",
    "    (sa_runs['GPU'] == gpu) &\n",
    "    (sa_runs['ISL'] == isl) &\n",
    "    (sa_runs['OSL'] == osl)][[\n",
    "    'Model',\n",
    "    'GPU',\n",
    "    'TP',\n",
    "    'Replicas',\n",
    "    'Concurrency',\n",
    "    'ISL',\n",
    "    'OSL',\n",
    "    'Output_Throughput',\n",
    "    'Thpt_per_GPU',\n",
    "    'Thpt_per_User',\n",
    "    'Directory']].drop('Model', axis=1).drop('GPU', axis=1).drop('ISL', axis=1).drop('OSL', axis=1)#.sort_values(by='Output_Throughput')\n",
    "\n",
    "# Plot performance results\n",
    "colors = ['#FF0000', '#FFAA00', '#DDDD00', '#00DD00', '#00FFFF', '#0000FF',\n",
    "          '#FF00FF', '#666666', '#000000']\n",
    "\n",
    "# Unique configurations of replicas and TP\n",
    "\n",
    "if seg_by_dir:\n",
    "    columns = ['Model', 'GPU', 'ISL', 'OSL', 'Directory']\n",
    "    scenarios = list(set(sa_runs.set_index(columns).index))\n",
    "    configs = list(set(sa_runs_selected.set_index(['Replicas', 'TP', 'Directory']).index))\n",
    "else:\n",
    "    sa_runs_selected = sa_runs_selected.drop('Directory', axis=1)\n",
    "    configs = list(set(sa_runs_selected.set_index(['Replicas', 'TP']).index))\n",
    "configs.sort()\n",
    "# Sweep through configurations\n",
    "for ii, conf in enumerate(configs):\n",
    "    # Make a DataFrame for specific configuration\n",
    "    if seg_by_dir:\n",
    "        conf_df = sa_runs_selected[\n",
    "            (sa_runs_selected['Replicas'] == conf[0]) &\n",
    "            (sa_runs_selected['TP'] == conf[1]) &\n",
    "            (sa_runs_selected['Directory'] == conf[2])\n",
    "        ].sort_values(by='Concurrency')\n",
    "    else:\n",
    "        conf_df = sa_runs_selected[\n",
    "            (sa_runs_selected['Replicas'] == conf[0]) &\n",
    "            (sa_runs_selected['TP'] == conf[1])\n",
    "        ].sort_values(by='Concurrency')\n",
    "    display(conf_df)\n",
    "\n",
    "    # Plot throughputs for configuration\n",
    "    plt.plot(conf_df.Thpt_per_User, conf_df.Thpt_per_GPU,\n",
    "             label=f'Replicas: {conf[0]}  TP={conf[1]}',\n",
    "             marker='o', markersize=4,\n",
    "             color=colors[ii%len(colors)]\n",
    "            )\n",
    "    for jj, val in enumerate(conf_df.Concurrency):\n",
    "        plt.text(list(conf_df.Thpt_per_User)[jj],\n",
    "                 list(conf_df.Thpt_per_GPU)[jj]+sa_runs_selected['Thpt_per_GPU'].max()*0.02,\n",
    "                 str(val), ha='center', color=colors[ii%len(colors)])\n",
    "\n",
    "plt.title(f'GPU: {gpu}\\nModel: {model}\\nISL: {isl}  OSL: {osl}')\n",
    "plt.xlabel('Tok/s/User', fontsize='16')\n",
    "plt.ylabel('Tok/s/GPU', fontsize='16')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.grid(True, linewidth=1, ls='--', color='gray')\n",
    "plt.axis([0, None, 0, None])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb99ff4c-37cb-4ef0-a234-427bc1754deb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
