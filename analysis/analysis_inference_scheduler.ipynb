{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7099745a-82a5-494a-bac2-565fd9729eaf",
   "metadata": {},
   "source": [
    "# <font color=\"#000099\">llm-d-benchmarking Inference Scheduler Analysis</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3776a034-d0cd-40e4-b4f1-7d1cb5dcdd4d",
   "metadata": {},
   "source": [
    "This notebook imports benchmark report data from configuration sweeps from [llm-d-benchmark](https://github.com/llm-d/llm-d-benchmark), and investigates inference scheduler performance for a single scheduling profile.\n",
    "\n",
    "The first cell contains function and class definitions to support basic functionality, while the second cell imports data from user-provided directories into [Pandas](https://pandas.pydata.org/) [DataFrames](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html). The cells following plot results from the imported data, grouping them into \"scenarios\" with certain common features.\n",
    "\n",
    "While the basic functionality here may be sufficient for many purposes, this notebook should be considered a starting point for more detailed analysis and customization by the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788d1ce3-22fc-402c-a9d5-7e31b1653a67",
   "metadata": {},
   "source": [
    "## Package imports and definitions (run once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f5c2a6-937b-49b1-8f65-99bb7d00e784",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Package imports\n",
    "################################################################################\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "\n",
    "#sys.path.insert(0, '../workload/report/')\n",
    "import convert\n",
    "import schema\n",
    "\n",
    "\n",
    "class Text:\n",
    "    \"\"\"ANSI SGR control codes for text formatting\"\"\"\n",
    "    DEFAULT = \"\\x1b[0m\"\n",
    "    BOLD = \"\\x1b[1m\"\n",
    "    BOLD_OFF = \"\\x1b[22m\"\n",
    "    UNDERLINE = \"\\x1b[4m\"\n",
    "    UNDERLINE_OFF = \"\\x1b[24m\"\n",
    "    DEFAULT_COLOR = \"\\x1b[39m\"\n",
    "    DEFAULT_BG_COLOR = \"\\x1b[49m\"\n",
    "    RED = \"\\x1b[31m\"\n",
    "    YELLOW = \"\\x1b[33m\"\n",
    "    GREEN = \"\\x1b[32m\"\n",
    "    CYAN = \"\\x1b[36m\"\n",
    "    BLUE = \"\\x1b[34m\"\n",
    "    MAGENTA = \"\\x1b[35m\"\n",
    "    BLACK = \"\\x1b[30m\"\n",
    "    WHITE = \"\\x1b[37m\"\n",
    "    BG_RED = \"\\x1b[41m\"\n",
    "    BG_YELLOW = \"\\x1b[43m\"\n",
    "    BG_GREEN = \"\\x1b[42m\"\n",
    "    BG_CYAN = \"\\x1b[46m\"\n",
    "    BG_BLUE = \"\\x1b[44m\"\n",
    "    BG_MAGENTA = \"\\x1b[45m\"\n",
    "    BG_BLACK = \"\\x1b[40m\"\n",
    "    BG_WHITE = \"\\x1b[47m\"\n",
    "\n",
    "\n",
    "def info(mesg: str) -> None:\n",
    "    \"\"\"Print information message.\n",
    "\n",
    "    Args:\n",
    "        mesg (str): Information message.\n",
    "    \"\"\"\n",
    "    sys.stderr.write(f'{Text.GREEN}{mesg}\\n{Text.DEFAULT}')\n",
    "\n",
    "\n",
    "def warn(mesg: str) -> None:\n",
    "    \"\"\"Print a warning message.\n",
    "\n",
    "    Args:\n",
    "        mesg (str): Warming message.\n",
    "    \"\"\"\n",
    "    sys.stderr.write(f'{Text.YELLOW}{mesg}\\n{Text.DEFAULT}')\n",
    "\n",
    "\n",
    "def error(mesg: str, err_code: int = 1) -> None:\n",
    "    \"\"\"Print an error message and exit with an error code.\n",
    "\n",
    "    Args:\n",
    "        mesg (str): Error message.\n",
    "        err_code (int): Error code.\n",
    "    \"\"\"\n",
    "    sys.stderr.write(f'{Text.RED}{mesg}\\n{Text.DEFAULT}')\n",
    "    sys.exit(err_code)\n",
    "\n",
    "\n",
    "def check_dir(dir: str) -> None:\n",
    "    \"\"\"Print an error if directory does not exist.\n",
    "\n",
    "    Args:\n",
    "        dir (str): Directory to check existence of.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(dir):\n",
    "        error(f'Invalid path: {dir}')\n",
    "\n",
    "\n",
    "def check_file(file: str) -> None:\n",
    "    \"\"\"Print an error if file does not exist.\n",
    "\n",
    "    Args:\n",
    "        file (str): File to check existence of.\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(file):\n",
    "        error(f'Invalid file: {file}')\n",
    "\n",
    "\n",
    "def get_benchmark_report_files(source_dir: str) -> list[str]:\n",
    "    \"\"\"Get a list of benchmark report files within provided path (recursive).\n",
    "\n",
    "    Args:\n",
    "        source_dir (str): Directory to recursively search for results files.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of paths to benchmark report files.\n",
    "    \"\"\"\n",
    "    rb_files = []\n",
    "    check_dir(source_dir)\n",
    "    path = Path(source_dir)\n",
    "    for file in path.rglob(\"benchmark_report,_*.yaml\"):\n",
    "        rb_files.append(str(file))\n",
    "    return rb_files\n",
    "\n",
    "\n",
    "def make_benchmark_runs_df() -> pandas.core.frame.DataFrame:\n",
    "    \"\"\"Create DataFrame for benchmark run results.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Empty DataFrame for benchmark runs.\n",
    "    \"\"\"\n",
    "    return pandas.DataFrame(columns=[\n",
    "        'Name',\n",
    "        'Directory',\n",
    "        'Model',\n",
    "        'GPU',\n",
    "        'ISL',\n",
    "        'OSL',\n",
    "        'Duration',\n",
    "        'Total_Requests',\n",
    "        'Failures',\n",
    "        'Request_Throughput',\n",
    "        'Output_Token_Throughput',\n",
    "        'Total_Token_Throughput',\n",
    "        'Mean_TTFT_ms',\n",
    "        'Mean_TPOT_ms',\n",
    "        'Mean_ITL_ms',\n",
    "        'Mean_E2EL_ms',\n",
    "        'KV_Cache_Scorer_Weight',\n",
    "        'Queue_Scorer_Weight',\n",
    "        'Prefix_Cache_Scorer_Weight',\n",
    "        'Prefix_Cache_Scorer_Block_Size',\n",
    "        'Prefix_Cache_Scorer_LRU_Capacity_Per_Server',\n",
    "        'Prefix_Cache_Scorer_Max_Blocks_To_Match',\n",
    "        'Prefix_Cache_Scorer_Tracking',\n",
    "        'System_Prompt_Length',\n",
    "        'Question_Length',\n",
    "        'Approx_OSL',\n",
    "        'Groups',\n",
    "        'Prompts_Per_Group',\n",
    "        'QPS',\n",
    "    ])\n",
    "\n",
    "\n",
    "def _make_name(report: schema.BenchmarkReport) -> str:\n",
    "    \"\"\"Create a name based on the benchmark run's configuration.\n",
    "\n",
    "    Args:\n",
    "        report (BenchmarkReport): Benchmark report to create a name for.\n",
    "\n",
    "    Returns:\n",
    "        str: Name of benchmark run.\n",
    "    \"\"\"\n",
    "    return 'name'\n",
    "\n",
    "\n",
    "def add_benchmark_report_to_df(\n",
    "    runs_df: pandas.core.frame.DataFrame,\n",
    "    br_file: str) -> None:\n",
    "    \"\"\"Load a results file and add it to the DataFrame of benchmark runs.\n",
    "\n",
    "    Args:\n",
    "        runs_df (DataFrame): DataFrame to add a row to for the provided run.\n",
    "        br_file (str): Benchmark report file to import.\n",
    "    \"\"\"\n",
    "    report = convert.import_benchmark_report(br_file)\n",
    "    if not report.scenario.platform.metadata:\n",
    "        warn(f'Missing scenario.platform.metadata, skipping: {br_file}')\n",
    "        return\n",
    "    if report.metrics.requests.failures > 0:\n",
    "        warn(f'Report has {report.metrics.requests.failures} request failures: {br_file}')\n",
    "\n",
    "    # Get plugin parameters\n",
    "    prefix_cache_scorer_block_size = None\n",
    "    prefix_cache_scorer_lur_capacity_per_server = None\n",
    "    prefix_cache_scorer_max_blocks_to_match = None\n",
    "    prefix_cache_scorer_tracking = None\n",
    "    for plugin in report.scenario.platform.metadata['inferenceScheduler']['plugins']:\n",
    "        if plugin['type'] == 'prefix-cache-scorer':\n",
    "            if 'parameters' not in plugin:\n",
    "                continue\n",
    "            prefix_cache_scorer_block_size = plugin['parameters'].get('blockSize', 16)\n",
    "            prefix_cache_scorer_lur_capacity_per_server = plugin['parameters'].get('lruCapacityPerServer', 31250)\n",
    "            prefix_cache_scorer_max_blocks_to_match = plugin['parameters'].get('maxPrefixBlocksToMatch', 256)\n",
    "            # If mode is 'cache_tracking', then precise prefix scoring is used\n",
    "            prefix_cache_scorer_tracking = plugin['parameters'].get('mode', '') == 'cache_tracking'\n",
    "        \n",
    "    # Set default weights to zero (disabled)\n",
    "    # TODO: capture other settings for prefix cache scorer\n",
    "    # https://gateway-api-inference-extension.sigs.k8s.io/guides/epp-configuration/prefix-aware/\n",
    "    prefix_cache_scorer_weight = 0\n",
    "    kv_cache_scorer_weight = 0\n",
    "    queue_scorer_weight = 0\n",
    "    # TODO: this analysis assumes only a single scheduling profile.\n",
    "    # In addition we assume the plugins have not been renamed, and the pluginRef\n",
    "    # is the same as the plugin type.\n",
    "    # https://gateway-api-inference-extension.sigs.k8s.io/guides/epp-configuration/config-text/\n",
    "    for plugin in report.scenario.platform.metadata['inferenceScheduler']['schedulingProfiles'][0]['plugins']:\n",
    "    # is the same as the plugin type.\n",
    "        if plugin['pluginRef'] == 'prefix-cache-scorer':\n",
    "            prefix_cache_scorer_weight = plugin.get('weight', 1)\n",
    "        if plugin['pluginRef'] == 'kv-cache-scorer':\n",
    "            kv_cache_scorer_weight = plugin.get('weight', 1)\n",
    "        if plugin['pluginRef'] == 'queue-scorer':\n",
    "            queue_scorer_weight = plugin.get('weight', 1)\n",
    "\n",
    "    # TODO get this from within benchmark report file\n",
    "    stage = report.scenario.load.metadata['stage']\n",
    "    #stage = int(br_file.rsplit('benchmark_report,_stage_')[-1].split('_', 1)[0])\n",
    "\n",
    "    # Add row to DataFrame\n",
    "    runs_df.loc[len(runs_df)] = {\n",
    "        'Name': _make_name(report),\n",
    "        # We want the base directory for the sweep, which is two levels up\n",
    "        'Directory': os.path.abspath(br_file).rsplit(os.sep, 1)[0],\n",
    "        'Model': report.scenario.model.name,\n",
    "        # Assume heterogeneous\n",
    "        'GPU': report.scenario.host.accelerator[0].model,\n",
    "        # TODO this may need to be configurable...\n",
    "        'ISL': int(round(report.metrics.requests.input_length.mean)),\n",
    "        'OSL': int(report.metrics.requests.output_length.mean),\n",
    "        'Duration': report.metrics.time.duration,\n",
    "        'Total_Requests': report.metrics.requests.total,\n",
    "        'Failures': report.metrics.requests.failures,\n",
    "        'Request_Throughput': report.metrics.throughput.requests_per_sec,\n",
    "        'Output_Token_Throughput': report.metrics.throughput.output_tokens_per_sec,\n",
    "        'Total_Token_Throughput': report.metrics.throughput.total_tokens_per_sec,\n",
    "        'Mean_TTFT_ms': report.metrics.latency.time_to_first_token.mean * (1000 if report.metrics.latency.time_to_first_token.units == schema.Units.S else 1),\n",
    "        'Mean_TPOT_ms': report.metrics.latency.time_per_output_token.mean * (1000 if report.metrics.latency.time_per_output_token.units == schema.Units.S_PER_TOKEN else 1),\n",
    "        'Mean_ITL_ms': report.metrics.latency.inter_token_latency.mean * (1000 if report.metrics.latency.inter_token_latency.units == schema.Units.S_PER_TOKEN else 1),\n",
    "        'Mean_E2EL_ms': report.metrics.latency.request_latency.mean * (1000 if report.metrics.latency.request_latency.units == schema.Units.S else 1),\n",
    "        'KV_Cache_Scorer_Weight': kv_cache_scorer_weight,\n",
    "        'Queue_Scorer_Weight': queue_scorer_weight,\n",
    "        'Prefix_Cache_Scorer_Weight': prefix_cache_scorer_weight,\n",
    "        'Prefix_Cache_Scorer_Block_Size': prefix_cache_scorer_block_size,\n",
    "        'Prefix_Cache_Scorer_LRU_Capacity_Per_Server': prefix_cache_scorer_lur_capacity_per_server,\n",
    "        'Prefix_Cache_Scorer_Max_Blocks_To_Match': prefix_cache_scorer_max_blocks_to_match,\n",
    "        'Prefix_Cache_Scorer_Tracking': prefix_cache_scorer_tracking,\n",
    "        'System_Prompt_Length': report.scenario.load.args['data']['shared_prefix']['system_prompt_len'],\n",
    "        'Question_Length': report.scenario.load.args['data']['shared_prefix']['question_len'],\n",
    "        'Approx_OSL': report.scenario.load.args['data']['shared_prefix']['output_len'],\n",
    "        'Groups': report.scenario.load.args['data']['shared_prefix']['num_groups'],\n",
    "        'Prompts_Per_Group': report.scenario.load.args['data']['shared_prefix']['num_prompts_per_group'],\n",
    "        'QPS': report.scenario.load.args['load']['stages'][stage]['rate'],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64c4e28-fd50-4f2f-86a5-f9390d5f2968",
   "metadata": {},
   "source": [
    "## Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041839c2-0d6a-44b1-b7e1-de6cc8a575ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# User inputs\n",
    "################################################################################\n",
    "\n",
    "# List of directories containing benchmark sweeps to import.\n",
    "search_dirs = [\n",
    "    \"/path/to/data/\",\n",
    "]\n",
    "\n",
    "################################################################################\n",
    "# Standard code\n",
    "################################################################################\n",
    "\n",
    "# Create blank DataFrames for benchmarking runs\n",
    "runs = make_benchmark_runs_df()\n",
    "\n",
    "# Populate the runs DataFrame\n",
    "for sdir in search_dirs:\n",
    "    info(f'Searching for benchmark report files within {sdir}')\n",
    "    # Find all benchmark report files in the directory\n",
    "    for br_file in get_benchmark_report_files(sdir):\n",
    "        #info(f'Importing {br_file}')\n",
    "        # Import the results and add to the runs DataFrame\n",
    "        add_benchmark_report_to_df(runs, br_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cd38ea-42b5-40bf-b505-35f67541838f",
   "metadata": {},
   "source": [
    "## Get available scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1a4c39-1f5a-4fd4-843f-87abd6f416a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Definitions\n",
    "################################################################################\n",
    "\n",
    "SCENARIO_COLUMNS = ['Model', 'GPU', 'System_Prompt_Length', 'Question_Length', 'Approx_OSL', 'Groups', 'Prompts_Per_Group']\n",
    "\n",
    "def get_scenarios(runs_df: pandas.core.frame.DataFrame) -> list[tuple[str]]:\n",
    "    \"\"\"Get a list of available scenarios from runs DataFrame.\n",
    "\n",
    "    Args:\n",
    "        runs_df (DataFrame): Benchmark runs to find the scenarios for.\n",
    "\n",
    "    Returns:\n",
    "        list[tuple[str]]: List of scenarios, consisting of unique groups of\n",
    "            values from SCENARIO_COLUMNS.\n",
    "    \"\"\"\n",
    "    return list(set(runs_df.set_index(SCENARIO_COLUMNS).index))\n",
    "\n",
    "\n",
    "def print_scenarios(scenarios: list[tuple[str]]) -> None:\n",
    "    \"\"\"Print a formatted table of scenarios.\n",
    "\n",
    "    Args:\n",
    "        scenarios (list[tuple[str]]): Scenario groups to print.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get maximum text length for each column, including header\n",
    "    spans = list(map(len, SCENARIO_COLUMNS))\n",
    "    for sc in scenarios:\n",
    "        for jj, item in enumerate(sc):\n",
    "            if spans[jj] < len(str(item)):\n",
    "                spans[jj] = len(str(item))\n",
    "\n",
    "    # Create header, starting with scenario index\n",
    "    header = f'{Text.BOLD}{Text.BLUE}IDX  {Text.DEFAULT}{Text.BOLD}'\n",
    "    # Add each column name to header\n",
    "    for ii, col in enumerate(SCENARIO_COLUMNS):\n",
    "        header += col + \" \" * (spans[ii] - len(col) + 2)\n",
    "    header += f'{Text.DEFAULT}'\n",
    "    print(header)\n",
    "\n",
    "    # Print details of each scenario\n",
    "    for ii, sc in enumerate(scenarios):\n",
    "        row = f'{Text.BLUE}{ii}{Text.DEFAULT}' + \" \" * (5 - len(str(ii)))\n",
    "        for jj, val in enumerate(sc):\n",
    "            row += f'{str(val)}' + \" \" * (spans[jj] - len(str(val)) + 2)\n",
    "        print(row)\n",
    "\n",
    "################################################################################\n",
    "# Execute code\n",
    "################################################################################\n",
    "\n",
    "scenarios = get_scenarios(runs)\n",
    "print_scenarios(scenarios)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12abbb2a-e109-438c-9331-95daaf35c01b",
   "metadata": {},
   "source": [
    "## Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8267142f-81fd-41e9-b593-a3c3857f7eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Definitions\n",
    "################################################################################\n",
    "\n",
    "def plot_scenario(\n",
    "        runs: pandas.core.frame.DataFrame,\n",
    "        scenarios: list[tuple[str]],\n",
    "        idx: int,\n",
    "        print_tables: bool = False,\n",
    "        seg_by_dir: bool = True) -> None:\n",
    "    \"\"\"\n",
    "    Plot inference scheduler scenario as TTFT vs throughput for different\n",
    "    request rates (in queries per second).\n",
    "\n",
    "    Args:\n",
    "        runs (DataFrame): Collection of benchmark run data.\n",
    "        scenarios (list[tuple[str]]): Scenarios in dataset.\n",
    "        idx (int): Index of scenario to plot.\n",
    "        print_tables (bool): Print tabular data (default: False).\n",
    "        seg_by_dir (bool): Group points with matching scorer weights only\n",
    "            if they come from benchmark reports in the same directory\n",
    "            (default: true). This is helpful when repeated runs of the same\n",
    "            experiment are viewed.\n",
    "    \"\"\"\n",
    "    # Get parameters of selected scenario\n",
    "    model, gpu, prompt_len, q_len, osl, groups, prompts_per_grp = scenarios[idx]\n",
    "    \n",
    "    # Filter on column values\n",
    "    runs_selected = runs[\n",
    "        (runs['Model'] == model) &\n",
    "        (runs['GPU'] == gpu) &\n",
    "        (runs['System_Prompt_Length'] == prompt_len) &\n",
    "        (runs['Question_Length'] == q_len) &\n",
    "        (runs['Approx_OSL'] == osl) &\n",
    "        (runs['Groups'] == groups) &\n",
    "        (runs['Prompts_Per_Group'] == prompts_per_grp)\n",
    "        ][[\n",
    "        'KV_Cache_Scorer_Weight',\n",
    "        'Queue_Scorer_Weight',\n",
    "        'Prefix_Cache_Scorer_Weight',\n",
    "        'Prefix_Cache_Scorer_Tracking',\n",
    "        'Total_Token_Throughput',\n",
    "        'Mean_TTFT_ms',\n",
    "        'Mean_TPOT_ms',\n",
    "        'QPS',\n",
    "        'Total_Requests',\n",
    "        'Failures',\n",
    "        'Directory']].sort_values(by='Mean_TTFT_ms')\n",
    "    \n",
    "    # Unique configurations of scorer weights\n",
    "    # NOTE: We are assuming plugin parameters in this analysis!\n",
    "    if seg_by_dir:\n",
    "        config_sets = list(set(runs_selected.set_index(['KV_Cache_Scorer_Weight', 'Queue_Scorer_Weight', 'Prefix_Cache_Scorer_Weight', 'Directory']).index))\n",
    "    else:\n",
    "        config_sets = list(set(runs_selected.set_index(['KV_Cache_Scorer_Weight', 'Queue_Scorer_Weight', 'Prefix_Cache_Scorer_Weight']).index))\n",
    "    config_sets.sort()\n",
    "    # Convert the list of sets to a list of dicts, to make code following clearer\n",
    "    configs = []\n",
    "    for conf in config_sets:\n",
    "        configs.append({\n",
    "            'kv': conf[0],\n",
    "            'queue': conf[1],\n",
    "            'prefix': conf[2],\n",
    "            'dir': conf[3] if seg_by_dir else '',\n",
    "        })\n",
    "    \n",
    "    # Plot performance results\n",
    "    colors = ['#FF0000', '#FFAA00', '#DDDD00', '#00DD00', '#00FFFF', '#0000FF',\n",
    "              '#FF00FF', '#666666', '#000000',\n",
    "             '#990000', '#777700', '#007700', '#009999', '#000099']\n",
    "\n",
    "    ###\n",
    "\n",
    "    # Print tables\n",
    "    if print_tables:\n",
    "        for ii, conf in enumerate(configs):\n",
    "            # Make a DataFrame for specific configuration\n",
    "            if seg_by_dir:\n",
    "                conf_df = runs_selected[\n",
    "                    (runs_selected['KV_Cache_Scorer_Weight'] == conf['kv']) &\n",
    "                    (runs_selected['Queue_Scorer_Weight'] == conf['queue']) &\n",
    "                    (runs_selected['Prefix_Cache_Scorer_Weight'] == conf['prefix']) &\n",
    "                    (runs_selected['Directory'] == conf['dir'])\n",
    "                ].sort_values(by='QPS')\n",
    "            else:\n",
    "                conf_df = runs_selected[\n",
    "                    (runs_selected['KV_Cache_Scorer_Weight'] == conf['kv']) &\n",
    "                    (runs_selected['Queue_Scorer_Weight'] == conf['queue']) &\n",
    "                    (runs_selected['Prefix_Cache_Scorer_Weight'] == conf['prefix'])\n",
    "                ].sort_values(by='QPS')\n",
    "    \n",
    "            if seg_by_dir:\n",
    "                # Print source data directory\n",
    "                print(runs_selected.iloc[0]['Directory'])\n",
    "            # Print table\n",
    "            display(conf_df)\n",
    "\n",
    "    ###\n",
    "\n",
    "    # Plot TTFT vs throughput across rates for each configuration\n",
    "    for ii, conf in enumerate(configs):\n",
    "        # Make a DataFrame for specific configuration\n",
    "        if seg_by_dir:\n",
    "            conf_df = runs_selected[\n",
    "                (runs_selected['KV_Cache_Scorer_Weight'] == conf['kv']) &\n",
    "                (runs_selected['Queue_Scorer_Weight'] == conf['queue']) &\n",
    "                (runs_selected['Prefix_Cache_Scorer_Weight'] == conf['prefix']) &\n",
    "                (runs_selected['Directory'] == conf['dir'])\n",
    "            ].sort_values(by='QPS')\n",
    "        else:\n",
    "            conf_df = runs_selected[\n",
    "                (runs_selected['KV_Cache_Scorer_Weight'] == conf['kv']) &\n",
    "                (runs_selected['Queue_Scorer_Weight'] == conf['queue']) &\n",
    "                (runs_selected['Prefix_Cache_Scorer_Weight'] == conf['prefix'])\n",
    "            ].sort_values(by='QPS')\n",
    "\n",
    "        # Plot throughputs for configuration\n",
    "        plt.plot(conf_df.Total_Token_Throughput, conf_df.Mean_TTFT_ms,\n",
    "                 label=f'KV:{conf['kv']} Queue:{conf['queue']} Prefix:{conf['prefix']}' + (' Precise' if conf_df.iloc[0].Prefix_Cache_Scorer_Tracking else ''),\n",
    "                 marker='o', markersize=4,\n",
    "                 color=colors[ii%len(colors)]\n",
    "                )\n",
    "        for jj, val in enumerate(conf_df.QPS):\n",
    "            plt.text(list(conf_df.Total_Token_Throughput)[jj],\n",
    "                     list(conf_df.Mean_TTFT_ms)[jj]+runs_selected['Mean_TTFT_ms'].max()*0.02,\n",
    "                     str(val), ha='center', color=colors[ii%len(colors)])\n",
    "    plt.title(f'GPU: {gpu}\\nModel: {model}\\nPrompt Len: {prompt_len}   Query Len: {q_len}   OSL: {osl}\\nGroups: {groups}   Prompts per Group: {prompts_per_grp}')\n",
    "    plt.xlabel('Total Throughput (Tok/s)', fontsize='16')\n",
    "    plt.ylabel('Mean TTFT (ms)', fontsize='16')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.grid(True, linewidth=1, ls='--', color='gray')\n",
    "    plt.axis([0, None, 0, None])\n",
    "    plt.show()\n",
    "\n",
    "    ###\n",
    "\n",
    "    # Plot Throuput vs QPS\n",
    "    for ii, conf in enumerate(configs):\n",
    "        # Make a DataFrame for specific configuration\n",
    "        if seg_by_dir:\n",
    "            conf_df = runs_selected[\n",
    "                (runs_selected['KV_Cache_Scorer_Weight'] == conf['kv']) &\n",
    "                (runs_selected['Queue_Scorer_Weight'] == conf['queue']) &\n",
    "                (runs_selected['Prefix_Cache_Scorer_Weight'] == conf['prefix']) &\n",
    "                (runs_selected['Directory'] == conf['dir'])\n",
    "            ].sort_values(by='QPS')\n",
    "        else:\n",
    "            conf_df = runs_selected[\n",
    "                (runs_selected['KV_Cache_Scorer_Weight'] == conf['kv']) &\n",
    "                (runs_selected['Queue_Scorer_Weight'] == conf['queue']) &\n",
    "                (runs_selected['Prefix_Cache_Scorer_Weight'] == conf['prefix'])\n",
    "            ].sort_values(by='QPS')\n",
    "\n",
    "        plt.plot(conf_df.QPS, conf_df.Total_Token_Throughput,\n",
    "                 label=f'KV:{conf['kv']} Queue:{conf['queue']} Prefix:{conf['prefix']}' + (' Precise' if conf_df.iloc[0].Prefix_Cache_Scorer_Tracking else ''),\n",
    "                 marker='o', markersize=4,\n",
    "                 color=colors[ii%len(colors)]\n",
    "                )\n",
    "    plt.title(f'GPU: {gpu}\\nModel: {model}\\nPrompt Len: {prompt_len}   Query Len: {q_len}   OSL: {osl}\\nGroups: {groups}   Prompts per Group: {prompts_per_grp}')\n",
    "    plt.xlabel('Request Rate (queries/s)', fontsize='16')\n",
    "    plt.ylabel('Total Throughput (Tok/s)', fontsize='16')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.grid(True, linewidth=1, ls='--', color='gray')\n",
    "    plt.axis([0, None, 0, None])\n",
    "    plt.show()\n",
    "\n",
    "    ###\n",
    "\n",
    "    # Plot TTFT vs QPS\n",
    "    for ii, conf in enumerate(configs):\n",
    "        # Make a DataFrame for specific configuration\n",
    "        if seg_by_dir:\n",
    "            conf_df = runs_selected[\n",
    "                (runs_selected['KV_Cache_Scorer_Weight'] == conf['kv']) &\n",
    "                (runs_selected['Queue_Scorer_Weight'] == conf['queue']) &\n",
    "                (runs_selected['Prefix_Cache_Scorer_Weight'] == conf['prefix']) &\n",
    "                (runs_selected['Directory'] == conf['dir'])\n",
    "            ].sort_values(by='QPS')\n",
    "        else:\n",
    "            conf_df = runs_selected[\n",
    "                (runs_selected['KV_Cache_Scorer_Weight'] == conf['kv']) &\n",
    "                (runs_selected['Queue_Scorer_Weight'] == conf['queue']) &\n",
    "                (runs_selected['Prefix_Cache_Scorer_Weight'] == conf['prefix'])\n",
    "            ].sort_values(by='QPS')\n",
    "\n",
    "        plt.plot(conf_df.QPS, conf_df.Mean_TTFT_ms,\n",
    "                 label=f'KV:{conf['kv']} Queue:{conf['queue']} Prefix:{conf['prefix']}' + (' Precise' if conf_df.iloc[0].Prefix_Cache_Scorer_Tracking else ''),\n",
    "                 marker='o', markersize=4,\n",
    "                 color=colors[ii%len(colors)]\n",
    "                )\n",
    "    plt.title(f'GPU: {gpu}\\nModel: {model}\\nPrompt Len: {prompt_len}   Query Len: {q_len}   OSL: {osl}\\nGroups: {groups}   Prompts per Group: {prompts_per_grp}')\n",
    "    plt.xlabel('Request Rate (queries/s)', fontsize='16')\n",
    "    plt.ylabel('TTFT (ms)', fontsize='16')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.grid(True, linewidth=1, ls='--', color='gray')\n",
    "    plt.axis([0, None, 0, None])\n",
    "    plt.show()\n",
    "\n",
    "    ###\n",
    "\n",
    "    # Plot TPOT vs QPS\n",
    "    for ii, conf in enumerate(configs):\n",
    "        # Make a DataFrame for specific configuration\n",
    "        if seg_by_dir:\n",
    "            conf_df = runs_selected[\n",
    "                (runs_selected['KV_Cache_Scorer_Weight'] == conf['kv']) &\n",
    "                (runs_selected['Queue_Scorer_Weight'] == conf['queue']) &\n",
    "                (runs_selected['Prefix_Cache_Scorer_Weight'] == conf['prefix']) &\n",
    "                (runs_selected['Directory'] == conf['dir'])\n",
    "            ].sort_values(by='QPS')\n",
    "        else:\n",
    "            conf_df = runs_selected[\n",
    "                (runs_selected['KV_Cache_Scorer_Weight'] == conf['kv']) &\n",
    "                (runs_selected['Queue_Scorer_Weight'] == conf['queue']) &\n",
    "                (runs_selected['Prefix_Cache_Scorer_Weight'] == conf['prefix'])\n",
    "            ].sort_values(by='QPS')\n",
    "\n",
    "        plt.plot(conf_df.QPS, conf_df.Mean_TPOT_ms,\n",
    "                 label=f'KV:{conf['kv']} Queue:{conf['queue']} Prefix:{conf['prefix']}' + (' Precise' if conf_df.iloc[0].Prefix_Cache_Scorer_Tracking else ''),\n",
    "                 marker='o', markersize=4,\n",
    "                 color=colors[ii%len(colors)]\n",
    "                )\n",
    "    plt.title(f'GPU: {gpu}\\nModel: {model}\\nPrompt Len: {prompt_len}   Query Len: {q_len}   OSL: {osl}\\nGroups: {groups}   Prompts per Group: {prompts_per_grp}')\n",
    "    plt.xlabel('Request Rate (queries/s)', fontsize='16')\n",
    "    plt.ylabel('TPOT (ms)', fontsize='16')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.grid(True, linewidth=1, ls='--', color='gray')\n",
    "    plt.axis([0, None, 0, None])\n",
    "    plt.show()\n",
    "\n",
    "    ###\n",
    "\n",
    "    # Plot Failures vs QPS\n",
    "    for ii, conf in enumerate(configs):\n",
    "        # Make a DataFrame for specific configuration\n",
    "        if seg_by_dir:\n",
    "            conf_df = runs_selected[\n",
    "                (runs_selected['KV_Cache_Scorer_Weight'] == conf['kv']) &\n",
    "                (runs_selected['Queue_Scorer_Weight'] == conf['queue']) &\n",
    "                (runs_selected['Prefix_Cache_Scorer_Weight'] == conf['prefix']) &\n",
    "                (runs_selected['Directory'] == conf['dir'])\n",
    "            ].sort_values(by='QPS')\n",
    "        else:\n",
    "            conf_df = runs_selected[\n",
    "                (runs_selected['KV_Cache_Scorer_Weight'] == conf['kv']) &\n",
    "                (runs_selected['Queue_Scorer_Weight'] == conf['queue']) &\n",
    "                (runs_selected['Prefix_Cache_Scorer_Weight'] == conf['prefix'])\n",
    "            ].sort_values(by='QPS')\n",
    "\n",
    "        plt.plot(conf_df.QPS, conf_df.Failures/conf_df.Total_Requests,\n",
    "                 label=f'KV:{conf['kv']} Queue:{conf['queue']} Prefix:{conf['prefix']}' + (' Precise' if conf_df.iloc[0].Prefix_Cache_Scorer_Tracking else ''),\n",
    "                 marker='o', markersize=4,\n",
    "                 color=colors[ii%len(colors)]\n",
    "                )\n",
    "    plt.title(f'GPU: {gpu}\\nModel: {model}\\nPrompt Len: {prompt_len}   Query Len: {q_len}   OSL: {osl}\\nGroups: {groups}   Prompts per Group: {prompts_per_grp}')\n",
    "    plt.xlabel('Request Rate (queries/s)', fontsize='16')\n",
    "    plt.ylabel('Failure Fraction', fontsize='16')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.grid(True, linewidth=1, ls='--', color='gray')\n",
    "    plt.axis([0, None, 0, None])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbf4a7e-4b58-4072-a316-e553063daa5d",
   "metadata": {},
   "source": [
    "### Plot a specific scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ce65fe-6763-430c-a1a3-b6b5cd302223",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# User inputs\n",
    "################################################################################\n",
    "\n",
    "# Select scenario\n",
    "idx = 0\n",
    "\n",
    "# Print tables\n",
    "print_tables = False\n",
    "\n",
    "# Segregate traces by directory (data with matching scorer weights will only\n",
    "# be joined if they originate from benchmark reports in the same directory).\n",
    "# This is helpful if repeated runs are performed. If an experimental run is\n",
    "# extended through a follow-on run, setting this to \"False\" will allow the\n",
    "# data in the plot to be joined.\n",
    "seg_by_dir = True\n",
    "\n",
    "################################################################################\n",
    "# Standard code\n",
    "################################################################################\n",
    "\n",
    "plot_scenario(runs, scenarios, idx, print_tables, seg_by_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4506dd07-f871-4c74-9afe-08ec9775e080",
   "metadata": {},
   "source": [
    "### Plot all scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06a911b-a4b7-4a0f-bb3a-a6f32d3f76d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# User inputs\n",
    "################################################################################\n",
    "\n",
    "# Print tables\n",
    "print_tables = False\n",
    "\n",
    "# Segregate traces by directory (data with matching scorer weights will only\n",
    "# be joined if they originate from benchmark reports in the same directory).\n",
    "# This is helpful if repeated runs are performed. If an experimental run is\n",
    "# extended through a follow-on run, setting this to \"False\" will allow the\n",
    "# data in the plot to be joined.\n",
    "seg_by_dir = True\n",
    "\n",
    "################################################################################\n",
    "# Standard code\n",
    "################################################################################\n",
    "\n",
    "for idx in range(len(scenarios)):\n",
    "    plot_scenario(runs, scenarios, idx, print_tables, seg_by_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1984b4-d754-4429-bd56-bc084b19d2d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
