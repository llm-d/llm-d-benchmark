{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7099745a-82a5-494a-bac2-565fd9729eaf",
   "metadata": {},
   "source": [
    "# <font color=\"#000099\">llm-d-benchmarking Sweep Analysis</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3776a034-d0cd-40e4-b4f1-7d1cb5dcdd4d",
   "metadata": {},
   "source": [
    "This notebook imports benchmark report data from configuration sweeps from [llm-d-benchmark](https://github.com/llm-d/llm-d-benchmark), and creates Pareto plots to compare configurations for a particular model and workload.\n",
    "\n",
    "The first cell contains function and class definitions to support basic functionality, while the second cell imports data from user-provided directories into [Pandas](https://pandas.pydata.org/) [DataFrames](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html).\n",
    "\n",
    "The cells following will look at the different scenarios (a particular selection of model, GPU, and workload input/output size) and create tables and Pareto plots for different configurations under these scenarios.\n",
    "\n",
    "While this basic functionality may be sufficient for many purposes, this notebook should be considered a starting point for more detailed analysis and customization by the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788d1ce3-22fc-402c-a9d5-7e31b1653a67",
   "metadata": {},
   "source": [
    "## Package imports and definitions (run once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f5c2a6-937b-49b1-8f65-99bb7d00e784",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Package imports\n",
    "################################################################################\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "\n",
    "#sys.path.insert(0, '../workload/report/')\n",
    "import convert\n",
    "import schema\n",
    "\n",
    "\n",
    "class Text:\n",
    "    \"\"\"ANSI SGR control codes for text formatting\"\"\"\n",
    "    DEFAULT = \"\\x1b[0m\"\n",
    "    BOLD = \"\\x1b[1m\"\n",
    "    BOLD_OFF = \"\\x1b[22m\"\n",
    "    UNDERLINE = \"\\x1b[4m\"\n",
    "    UNDERLINE_OFF = \"\\x1b[24m\"\n",
    "    DEFAULT_COLOR = \"\\x1b[39m\"\n",
    "    DEFAULT_BG_COLOR = \"\\x1b[49m\"\n",
    "    RED = \"\\x1b[31m\"\n",
    "    YELLOW = \"\\x1b[33m\"\n",
    "    GREEN = \"\\x1b[32m\"\n",
    "    CYAN = \"\\x1b[36m\"\n",
    "    BLUE = \"\\x1b[34m\"\n",
    "    MAGENTA = \"\\x1b[35m\"\n",
    "    BLACK = \"\\x1b[30m\"\n",
    "    WHITE = \"\\x1b[37m\"\n",
    "    BG_RED = \"\\x1b[41m\"\n",
    "    BG_YELLOW = \"\\x1b[43m\"\n",
    "    BG_GREEN = \"\\x1b[42m\"\n",
    "    BG_CYAN = \"\\x1b[46m\"\n",
    "    BG_BLUE = \"\\x1b[44m\"\n",
    "    BG_MAGENTA = \"\\x1b[45m\"\n",
    "    BG_BLACK = \"\\x1b[40m\"\n",
    "    BG_WHITE = \"\\x1b[47m\"\n",
    "\n",
    "\n",
    "def info(mesg: str) -> None:\n",
    "    \"\"\"Print information message.\n",
    "\n",
    "    Args:\n",
    "        mesg (str): Information message.\n",
    "    \"\"\"\n",
    "    sys.stderr.write(f'{Text.GREEN}{mesg}\\n{Text.DEFAULT}')\n",
    "\n",
    "\n",
    "def warn(mesg: str) -> None:\n",
    "    \"\"\"Print a warning message.\n",
    "\n",
    "    Args:\n",
    "        mesg (str): Warming message.\n",
    "    \"\"\"\n",
    "    sys.stderr.write(f'{Text.YELLOW}{mesg}\\n{Text.DEFAULT}')\n",
    "\n",
    "\n",
    "def error(mesg: str, err_code: int = 1) -> None:\n",
    "    \"\"\"Print an error message and exit with an error code.\n",
    "\n",
    "    Args:\n",
    "        mesg (str): Error message.\n",
    "        err_code (int): Error code.\n",
    "    \"\"\"\n",
    "    sys.stderr.write(f'{Text.RED}{mesg}\\n{Text.DEFAULT}')\n",
    "    sys.exit(err_code)\n",
    "\n",
    "\n",
    "def check_dir(dir: str) -> None:\n",
    "    \"\"\"Print an error if directory does not exist.\n",
    "\n",
    "    Args:\n",
    "        dir (str): Directory to check existence of.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(dir):\n",
    "        error(f'Invalid path: {dir}')\n",
    "\n",
    "\n",
    "def check_file(file: str) -> None:\n",
    "    \"\"\"Print an error if file does not exist.\n",
    "\n",
    "    Args:\n",
    "        file (str): File to check existence of.\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(file):\n",
    "        error(f'Invalid file: {file}')\n",
    "\n",
    "\n",
    "def get_benchmark_report_files(source_dir: str) -> list[str]:\n",
    "    \"\"\"Get a list of benchmark report files within provided path (recursive).\n",
    "\n",
    "    Args:\n",
    "        source_dir (str): Directory to recursively search for results files.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of paths to benchmark report files.\n",
    "    \"\"\"\n",
    "    rb_files = []\n",
    "    check_dir(source_dir)\n",
    "    path = Path(source_dir)\n",
    "    for file in path.rglob(\"benchmark_report,_*.yaml\"):\n",
    "        rb_files.append(str(file))\n",
    "    return rb_files\n",
    "\n",
    "\n",
    "def make_benchmark_runs_df() -> pandas.core.frame.DataFrame:\n",
    "    \"\"\"Create DataFrame for benchmark run results.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Empty DataFrame for benchmark runs.\n",
    "    \"\"\"\n",
    "    return pandas.DataFrame(columns=[\n",
    "        'Name',\n",
    "        'Directory',\n",
    "        'Model',\n",
    "        'GPU',\n",
    "        'DP',\n",
    "        'TP',\n",
    "        'PP',\n",
    "        'EP',\n",
    "        'Replicas',\n",
    "        'P_DP',\n",
    "        'P_TP',\n",
    "        'P_PP',\n",
    "        'P_EP',\n",
    "        'P_Replicas',\n",
    "        'D_DP',\n",
    "        'D_TP',\n",
    "        'D_PP',\n",
    "        'D_EP',\n",
    "        'D_Replicas',\n",
    "        'Concurrency',\n",
    "        'ISL',\n",
    "        'OSL',\n",
    "        'Backend',\n",
    "        'Duration',\n",
    "        'Completed',\n",
    "        'Request_Throughput',\n",
    "        'Output_Token_Throughput',\n",
    "        'Total_Token_Throughput',\n",
    "        'Mean_TTFT_ms',\n",
    "        'Mean_TPOT_ms',\n",
    "        'Mean_ITL_ms',\n",
    "        'Mean_E2EL_ms',\n",
    "    ])\n",
    "\n",
    "\n",
    "def _get_replicas_and_parallelism(report: schema.BenchmarkReport) -> dict[str, int | None]:\n",
    "    \"\"\"Get the number of replicas and parallelisms.\n",
    "\n",
    "    Args:\n",
    "        report (BenchmarkReport): Benchmark run to evaluate.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, int | None]: Replicas and parallelisms for standalone or\n",
    "            prefill/decode configuration. Irrelevant fields will have a value\n",
    "            of None.\n",
    "    \"\"\"\n",
    "    rp = {}\n",
    "    rp['replicas'] = report.scenario.host.type.count(schema.HostType.REPLICA)\n",
    "    rp['p_replicas'] = report.scenario.host.type.count(schema.HostType.PREFILL)\n",
    "    rp['d_replicas'] = report.scenario.host.type.count(schema.HostType.DECODE)\n",
    "    if rp['replicas'] == 0:\n",
    "        rp['replicas'] = None\n",
    "    if rp['p_replicas'] == 0:\n",
    "        rp['p_replicas'] = None\n",
    "    if rp['d_replicas'] == 0:\n",
    "        rp['d_replicas'] = None\n",
    "    rp['tp'] = None\n",
    "    rp['dp'] = None\n",
    "    rp['pp'] = None\n",
    "    rp['ep'] = None\n",
    "    rp['p_tp'] = None\n",
    "    rp['p_dp'] = None\n",
    "    rp['p_pp'] = None\n",
    "    rp['p_ep'] = None\n",
    "    rp['d_tp'] = None\n",
    "    rp['d_dp'] = None\n",
    "    rp['d_pp'] = None\n",
    "    rp['d_ep'] = None\n",
    "    if rp['replicas']:\n",
    "        # We have a standalone setup\n",
    "        rp['tp'] = report.scenario.host.accelerator[0].parallelism.tp\n",
    "        rp['dp'] = report.scenario.host.accelerator[0].parallelism.dp\n",
    "        rp['pp'] = report.scenario.host.accelerator[0].parallelism.pp\n",
    "        rp['ep'] = report.scenario.host.accelerator[0].parallelism.ep\n",
    "        return rp\n",
    "    # We have a P/D setup\n",
    "    for ii, accel in enumerate(report.scenario.host.accelerator):\n",
    "        if report.scenario.host.type[ii] is schema.HostType.PREFILL and not rp['p_tp']:\n",
    "            rp['p_tp'] = accel.parallelism.tp\n",
    "            rp['p_dp'] = accel.parallelism.dp\n",
    "            rp['p_pp'] = accel.parallelism.pp\n",
    "            rp['p_ep'] = accel.parallelism.ep\n",
    "        if report.scenario.host.type[ii] is schema.HostType.DECODE and not rp['d_tp']:\n",
    "            rp['d_tp'] = accel.parallelism.tp\n",
    "            rp['d_dp'] = accel.parallelism.dp\n",
    "            rp['d_pp'] = accel.parallelism.pp\n",
    "            rp['d_ep'] = accel.parallelism.ep\n",
    "        if rp['p_tp'] and rp['d_tp']:\n",
    "            break\n",
    "    return rp\n",
    "\n",
    "\n",
    "def _make_name(report: schema.BenchmarkReport) -> str:\n",
    "    \"\"\"Create a name based on the benchmark run's configuration.\n",
    "\n",
    "    Args:\n",
    "        report (BenchmarkReport): Benchmark report to create a name for.\n",
    "\n",
    "    Returns:\n",
    "        str: Name of benchmark run, providing replica and parallelism details.\n",
    "    \"\"\"\n",
    "    rp = _get_replicas_and_parallelism(report)\n",
    "    if rp['replicas']:\n",
    "        # We have a standalone setup\n",
    "        return f'{rp['replicas']}R TP{rp['tp']}'\n",
    "    # We have a P/D setup\n",
    "    # TODO we currently assume the only type of parallelism is TP\n",
    "    return f'{rp['p_replicas']}P TP{rp['p_tp']}, {rp['d_replicas']}D TP{rp['d_tp']}'\n",
    "\n",
    "\n",
    "def add_benchmark_report_to_df(\n",
    "    runs_df: pandas.core.frame.DataFrame,\n",
    "    br_file: str) -> None:\n",
    "    \"\"\"Load a results file and add it to the DataFrame of benchmark runs.\n",
    "\n",
    "    Args:\n",
    "        runs_df (DataFrame): DataFrame to add a row to for the provided run.\n",
    "        br_file (str): Benchmark report file to import.\n",
    "    \"\"\"\n",
    "    report = convert.import_benchmark_report(br_file)\n",
    "    rp = _get_replicas_and_parallelism(report)\n",
    "    # TODO getting concurrency is speciffic to each harness, will need\n",
    "    # a way to capture this universally in the report so we don't have to do\n",
    "    # extractions like this\n",
    "    if report.scenario.load.args and 'max_concurrency' in report.scenario.load.args:\n",
    "        # vLLM Benchmark\n",
    "        concurrency = report.scenario.load.args['max_concurrency']\n",
    "    elif report.scenario.load.args and 'profile' in report.scenario.load.args \\\n",
    "    and 'measured_concurrencies' in report.scenario.load.args['profile']:\n",
    "        # GuideLLM\n",
    "        concurrency = report.scenario.load.args['profile']['measured_concurrencies'][0]\n",
    "    else:\n",
    "        warn('\"Concurrency\" is not defined, setting to 1, \"Thpt_per_User\" and Pareto plots will also be invalid.')\n",
    "        concurrency = 1\n",
    "\n",
    "    runs_df.loc[len(runs_df)] = {\n",
    "        'Name': _make_name(report),\n",
    "        # We want the base directory for the sweep, which is two levels up\n",
    "        'Directory': os.path.abspath(br_file).rsplit(os.sep, 2)[0],\n",
    "        'Model': report.scenario.model.name,\n",
    "        # Assume heterogeneous across P and D\n",
    "        'GPU': report.scenario.host.accelerator[0].model,\n",
    "        'DP': rp['dp'],\n",
    "        'TP': rp['tp'],\n",
    "        'PP': rp['pp'],\n",
    "        'EP': rp['ep'],\n",
    "        'Replicas': rp['replicas'],\n",
    "        'P_DP': rp['p_dp'],\n",
    "        'P_TP': rp['p_tp'],\n",
    "        'P_PP': rp['p_pp'],\n",
    "        'P_EP': rp['p_ep'],\n",
    "        'P_Replicas': rp['p_replicas'],\n",
    "        'D_DP': rp['d_dp'],\n",
    "        'D_TP': rp['d_tp'],\n",
    "        'D_PP': rp['d_pp'],\n",
    "        'D_EP': rp['d_ep'],\n",
    "        'D_Replicas': rp['d_replicas'],\n",
    "        'Concurrency': concurrency,\n",
    "        # TODO this may need to be configurable...\n",
    "        # We need to group by ISL/OSL exactly, so round and convert to int.\n",
    "        # Round ISL to nearest 10's\n",
    "        'ISL': int(round(report.metrics.requests.input_length.mean, -1)),\n",
    "        'OSL': int(round(report.metrics.requests.output_length.mean)),\n",
    "        'Duration': report.metrics.time.duration,\n",
    "        'Completed': report.metrics.requests.total,\n",
    "        'Request_Throughput': report.metrics.throughput.requests_per_sec,\n",
    "        'Output_Token_Throughput': report.metrics.throughput.output_tokens_per_sec,\n",
    "        'Total_Token_Throughput': report.metrics.throughput.total_tokens_per_sec,\n",
    "        'Mean_TTFT_ms': report.metrics.latency.time_to_first_token,\n",
    "        'Mean_TPOT_ms': report.metrics.latency,\n",
    "        'Mean_ITL_ms': report.metrics.latency,\n",
    "        'Mean_E2EL_ms': report.metrics.latency,\n",
    "    }\n",
    "    # Add calculated columns\n",
    "    if rp['tp']:\n",
    "        runs_df['Is_PD'] = False\n",
    "        runs_df['Num_GPUs'] = runs_df['TP']*runs_df['Replicas']\n",
    "    else:\n",
    "        runs_df['Is_PD'] = True\n",
    "        runs_df['Num_GPUs'] = runs_df['P_TP']*runs_df['P_Replicas'] + runs_df['D_TP']*runs_df['D_Replicas']\n",
    "    runs_df['Thpt_per_GPU'] = runs_df['Output_Token_Throughput']/runs_df['Num_GPUs']\n",
    "    runs_df['Thpt_per_User'] = runs_df['Output_Token_Throughput']/runs_df['Concurrency']\n",
    "\n",
    "\n",
    "def get_scenarios(runs_df: pandas.core.frame.DataFrame) -> list[tuple[str]]:\n",
    "    \"\"\"Get a list of available scenarios from runs DataFrame, where\n",
    "    configurations and concurrency will be swept.\n",
    "\n",
    "    Args:\n",
    "        runs_df (DataFrame): Benchmark runs to find the scenarios for.\n",
    "\n",
    "    Returns:\n",
    "        list[tuple[str]]: List of scenarios, consisting of unique groups of\n",
    "            model, GPU type, ISL, and OSL.\n",
    "    \"\"\"\n",
    "    columns = ['Model', 'GPU', 'ISL', 'OSL']\n",
    "    return list(set(runs_df.set_index(columns).index))\n",
    "\n",
    "\n",
    "def print_scenarios(scenarios: list[tuple[str]]) -> None:\n",
    "    \"\"\"Print a formatted table of scenarios.\n",
    "\n",
    "    Args:\n",
    "        scenarios (list[tuple[str]]): Scenario groups to print.\n",
    "    \"\"\"\n",
    "    columns = ['Model', 'GPU', 'ISL', 'OSL']\n",
    "    # Get maximum text length for each column, including header\n",
    "    spans = list(map(len, columns))\n",
    "    for sc in scenarios:\n",
    "        for jj, item in enumerate(sc):\n",
    "            if spans[jj] < len(str(item)):\n",
    "                spans[jj] = len(str(item))\n",
    "\n",
    "    # Create header, starting with scenario index\n",
    "    header = f'{Text.BOLD}{Text.BLUE}IDX  {Text.DEFAULT}{Text.BOLD}'\n",
    "    # Add each column name to header\n",
    "    for ii, col in enumerate(columns):\n",
    "        header += col + \" \" * (spans[ii] - len(col) + 2)\n",
    "    header += f'{Text.DEFAULT}'\n",
    "    print(header)\n",
    "\n",
    "    # Print details of each scenario\n",
    "    for ii, sc in enumerate(scenarios):\n",
    "        row = f'{Text.BLUE}{ii}{Text.DEFAULT}' + \" \" * (5 - len(str(ii)))\n",
    "        for jj, val in enumerate(sc):\n",
    "            row += f'{str(val)}' + \" \" * (spans[jj] - len(str(val)) + 2)\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64c4e28-fd50-4f2f-86a5-f9390d5f2968",
   "metadata": {},
   "source": [
    "## Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041839c2-0d6a-44b1-b7e1-de6cc8a575ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# User inputs\n",
    "################################################################################\n",
    "\n",
    "# List of directories containing benchmark sweeps to import.\n",
    "search_dirs = [\n",
    "    '/files/',\n",
    "]\n",
    "\n",
    "################################################################################\n",
    "# Standard code\n",
    "################################################################################\n",
    "\n",
    "# Create blank DataFrames for benchmarking runs\n",
    "runs = make_benchmark_runs_df()\n",
    "\n",
    "# Populate the runs DataFrame\n",
    "for sdir in search_dirs:\n",
    "    info(f'Searching for benchmark report files within {sdir}')\n",
    "    # Find all benchmark report files in the directory\n",
    "    for br_file in get_benchmark_report_files(sdir):\n",
    "        info(f'Importing {br_file}')\n",
    "        # Import the results and add to the runs DataFrame\n",
    "        add_benchmark_report_to_df(runs, br_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1638a4e2-204e-45b7-a3ef-f19510421d60",
   "metadata": {},
   "source": [
    "## Scenarios available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e21648c-7286-4f48-ac57-4f61764ceb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenarios available, where model, GPU type, ISL and OSL are constant.\n",
    "# Configurations (seplicas and parallelism) are swept within a scenario.\n",
    "\n",
    "scenarios = get_scenarios(runs)\n",
    "print_scenarios(scenarios)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cd38ea-42b5-40bf-b505-35f67541838f",
   "metadata": {},
   "source": [
    "## Pareto plotting and tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955501bc-de64-435a-819b-dc04435827ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# User inputs\n",
    "################################################################################\n",
    "\n",
    "# Select scenario\n",
    "idx = 0\n",
    "\n",
    "# Show P/D disaggregated scenarios\n",
    "show_pd = True\n",
    "# Show standalone scenarios\n",
    "show_sa = True\n",
    "\n",
    "# Segregate traces by directory (directories with identical scenarios, such as\n",
    "# repeated runs, will not be joined together in a single trace)\n",
    "seg_by_dir = True\n",
    "\n",
    "################################################################################\n",
    "# Standard code\n",
    "################################################################################\n",
    "\n",
    "# Get parameters of selected scenario\n",
    "model, gpu, isl, osl = scenarios[idx]\n",
    "\n",
    "# Filter on column values\n",
    "pd_runs_selected = runs[\n",
    "    (runs['Model'] == model) &\n",
    "    (runs['GPU'] == gpu) &\n",
    "    (runs['ISL'] == isl) &\n",
    "    (runs['OSL'] == osl)][[\n",
    "    'Model',\n",
    "    'GPU',\n",
    "    'P_TP',\n",
    "    'P_Replicas',\n",
    "    'D_TP',\n",
    "    'D_Replicas',\n",
    "    'Concurrency',\n",
    "    'ISL',\n",
    "    'OSL',\n",
    "    'Output_Token_Throughput',\n",
    "    'Thpt_per_GPU',\n",
    "    'Thpt_per_User',\n",
    "    'Directory']].drop('Model', axis=1).drop('GPU', axis=1).drop('ISL', axis=1).drop('OSL', axis=1)#.sort_values(by='Output_Token_Throughput')\n",
    "\n",
    "sa_runs_selected = runs[\n",
    "    (runs['Model'] == model) &\n",
    "    (runs['GPU'] == gpu) &\n",
    "    (runs['ISL'] == isl) &\n",
    "    (runs['OSL'] == osl) &\n",
    "    (runs['Is_PD']) == False][[\n",
    "    'Model',\n",
    "    'GPU',\n",
    "    'TP',\n",
    "    'Replicas',\n",
    "    'Concurrency',\n",
    "    'ISL',\n",
    "    'OSL',\n",
    "    'Output_Token_Throughput',\n",
    "    'Thpt_per_GPU',\n",
    "    'Thpt_per_User',\n",
    "    'Directory']].drop('Model', axis=1).drop('GPU', axis=1).drop('ISL', axis=1).drop('OSL', axis=1)#.sort_values(by='Output_Token_Throughput')\n",
    "\n",
    "# Plot performance results\n",
    "colors = ['#FF0000', '#FFAA00', '#DDDD00', '#00DD00', '#00FFFF', '#0000FF',\n",
    "          '#FF00FF', '#666666', '#000000']\n",
    "\n",
    "# Unique configurations of replicas and TP, described as a tuple\n",
    "# Tuple format is (rep, tp, p_rep, p_tp, d_rep, d_tp, dir, is_pd)\n",
    "config_sets = []\n",
    "if seg_by_dir:\n",
    "    configs_pd = list(set(pd_runs_selected.set_index(['P_Replicas', 'P_TP', 'D_Replicas', 'D_TP', 'Directory']).index))\n",
    "    configs_sa = list(set(sa_runs_selected.set_index(['Replicas', 'TP', 'Directory']).index))\n",
    "    for conf in configs_pd:\n",
    "        config_sets.append((\n",
    "            None,    # Replicas\n",
    "            None,    # TP\n",
    "            conf[0], # P replicas\n",
    "            conf[1], # P TP\n",
    "            conf[2], # D replicas\n",
    "            conf[3], # D TP\n",
    "            conf[4], # Directory\n",
    "            True     # Is PD\n",
    "        ))\n",
    "    for conf in configs_sa:\n",
    "        config_sets.append((\n",
    "            conf[0], # Replicas\n",
    "            conf[0], # TP\n",
    "            None,    # P replicas\n",
    "            None,    # P TP\n",
    "            None,    # D replicas\n",
    "            None,    # D TP\n",
    "            conf[2], # Directory\n",
    "            False    # Is PD\n",
    "        ))\n",
    "else:\n",
    "    pd_runs_selected = pd_runs_selected.drop('Directory', axis=1)\n",
    "    sa_runs_selected = sa_runs_selected.drop('Directory', axis=1)\n",
    "    configs_pd = list(set(pd_runs_selected.set_index(['P_Replicas', 'P_TP', 'D_Replicas', 'D_TP']).index))\n",
    "    configs_sa = list(set(sa_runs_selected.set_index(['Replicas', 'TP']).index))\n",
    "    for conf in configs_pd:\n",
    "        config_sets.append((\n",
    "            None,    # Replicas\n",
    "            None,    # TP\n",
    "            conf[0], # P replicas\n",
    "            conf[1], # P TP\n",
    "            conf[2], # D replicas\n",
    "            conf[3], # D TP\n",
    "            None,    # Directory\n",
    "            True     # Is PD\n",
    "        ))\n",
    "    for conf in configs_sa:\n",
    "        config_sets.append((\n",
    "            conf[0], # Replicas\n",
    "            conf[0], # TP\n",
    "            None,    # P replicas\n",
    "            None,    # P TP\n",
    "            None,    # D replicas\n",
    "            None,    # D TP\n",
    "            None,    # Directory\n",
    "            False    # Is PD\n",
    "        ))\n",
    "\n",
    "# Sort so prinouts/plots are organized\n",
    "config_sets.sort()\n",
    "\n",
    "# Convert the list of sets to a list of dicts, to make code following clearer\n",
    "configs = []\n",
    "for conf in config_sets:\n",
    "    configs.append({\n",
    "        'rep': conf[0],\n",
    "        'tp': conf[1],\n",
    "        'p_rep': conf[2],\n",
    "        'p_tp': conf[3],\n",
    "        'd_rep': conf[4],\n",
    "        'd_tp': conf[5],\n",
    "        'dir': conf[6],\n",
    "        'is_pd': conf[7],\n",
    "    })\n",
    "\n",
    "# Sweep through configurations\n",
    "for ii, conf in enumerate(configs):\n",
    "    is_pd = 'P_TP' in conf\n",
    "    # Make a DataFrame for specific configuration\n",
    "    if conf['is_pd']:\n",
    "        # This configuration is PD\n",
    "        if seg_by_dir:\n",
    "            conf_df = pd_runs_selected[\n",
    "                (pd_runs_selected['P_Replicas'] == conf['p_rep']) &\n",
    "                (pd_runs_selected['P_TP'] == conf['p_tp']) &\n",
    "                (pd_runs_selected['D_Replicas'] == conf['d_rep']) &\n",
    "                (pd_runs_selected['D_TP'] == conf['d_tp']) &\n",
    "                (pd_runs_selected['Directory'] == conf['dir'])\n",
    "            ].sort_values(by='Concurrency')\n",
    "        else:\n",
    "            conf_df = pd_runs_selected[\n",
    "                (pd_runs_selected['P_Replicas'] == conf['p_rep']) &\n",
    "                (pd_runs_selected['P_TP'] == conf['p_tp']) &\n",
    "                (pd_runs_selected['D_Replicas'] == conf['d_rep']) &\n",
    "                (pd_runs_selected['D_TP'] == conf['d_tp'])\n",
    "            ].sort_values(by='Concurrency')\n",
    "\n",
    "        # Print table\n",
    "        display(conf_df)\n",
    "    \n",
    "        # Plot throughputs for configuration\n",
    "        plt.plot(conf_df.Thpt_per_User, conf_df.Thpt_per_GPU,\n",
    "                 label=f'{conf['p_rep']}P-TP{conf['p_tp']} {conf['d_rep']}D-TP{conf['d_tp']}',\n",
    "                 marker='o', markersize=4,\n",
    "                 color=colors[ii%len(colors)]\n",
    "                )\n",
    "        for jj, val in enumerate(conf_df.Concurrency):\n",
    "            plt.text(list(conf_df.Thpt_per_User)[jj],\n",
    "                     list(conf_df.Thpt_per_GPU)[jj]+pd_runs_selected['Thpt_per_GPU'].max()*0.02,\n",
    "                     str(val), ha='center', color=colors[ii%len(colors)])\n",
    "    else:\n",
    "        # This configuration is standalone\n",
    "        if seg_by_dir:\n",
    "            conf_df = sa_runs_selected[\n",
    "                (sa_runs_selected['Replicas'] == conf['rep']) &\n",
    "                (sa_runs_selected['TP'] == conf['tp']) &\n",
    "                (sa_runs_selected['Directory'] == conf['dir'])\n",
    "            ].sort_values(by='Concurrency')\n",
    "        else:\n",
    "            conf_df = sa_runs_selected[\n",
    "                (sa_runs_selected['Replicas'] == conf['rep']) &\n",
    "                (sa_runs_selected['TP'] == conf['tp'])\n",
    "            ].sort_values(by='Concurrency')\n",
    "\n",
    "        # Print table\n",
    "        display(conf_df)\n",
    "    \n",
    "        # Plot throughputs for configuration\n",
    "        plt.plot(conf_df.Thpt_per_User, conf_df.Thpt_per_GPU,\n",
    "                 label=f'Replicas: {conf['rep']}  TP{conf['tp']}',\n",
    "                 marker='o', markersize=4,\n",
    "                 color=colors[ii%len(colors)]\n",
    "                )\n",
    "        for jj, val in enumerate(conf_df.Concurrency):\n",
    "            plt.text(list(conf_df.Thpt_per_User)[jj],\n",
    "                     list(conf_df.Thpt_per_GPU)[jj]+sa_runs_selected['Thpt_per_GPU'].max()*0.02,\n",
    "                     str(val), ha='center', color=colors[ii%len(colors)])\n",
    "\n",
    "plt.title(f'GPU: {gpu}\\nModel: {model}\\nISL: {isl}  OSL: {osl}')\n",
    "plt.xlabel('Tok/s/User', fontsize='16')\n",
    "plt.ylabel('Tok/s/GPU', fontsize='16')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.grid(True, linewidth=1, ls='--', color='gray')\n",
    "plt.axis([0, None, 0, None])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5299c3c-65cb-48e3-b381-6fe8b89a26a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
