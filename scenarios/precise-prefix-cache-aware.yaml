# ============================================================================
# PRECISE PREFIX CACHE AWARE ROUTING WELL LIT PATH
# Converted from bash environment variables to YAML format
# Based on https://github.com/llm-d/llm-d/tree/main/guides/precise-prefix-cache-aware/README.md
#
# Notes:
# - Removed pod monitoring; can be added using decode.extraContainerConfig
# - Removed extra volumes metrics-volume and torch-compile-volume (not needed for this model/hardware)
# - Use decode.additionalVolumeMounts and decode.additionalVolumes to add them if needed
# ============================================================================

scenario:
  - name: "precise-prefix-cache-aware"
    
    # -------------------------------------------------------------------------
    # Model Configuration
    # Corresponds to: LLMDBENCH_DEPLOY_MODEL_LIST="meta-llama/Llama-3.1-8B-Instruct"
    # -------------------------------------------------------------------------
    model:
      name: meta-llama/Llama-3.1-8B-Instruct
      shortName: meta-lla-8b-instruct
      path: models/meta-llama/Llama-3.1-8B-Instruct
      huggingfaceId: meta-llama/Llama-3.1-8B-Instruct
      # Corresponds to: LLMDBENCH_VLLM_COMMON_PVC_MODEL_CACHE_SIZE=1Ti
      size: 1Ti
      # Corresponds to: LLMDBENCH_VLLM_COMMON_MAX_MODEL_LEN=16000
      maxModelLen: 16000
      # Corresponds to: LLMDBENCH_VLLM_COMMON_BLOCK_SIZE=64
      blockSize: 64
      tensorParallelSize: 2
      gpuMemoryUtilization: 0.95
    
    # -------------------------------------------------------------------------
    # Routing / GAIE Configuration
    # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_GAIE_PLUGINS_CONFIGFILE="precise-prefix-cache-aware"
    # -------------------------------------------------------------------------
    inferenceExtension:
      pluginsConfigFile: "precise-prefix-cache-aware"
    
    # -------------------------------------------------------------------------
    # Routing Configuration
    # Corresponds to: LLMDBENCH_LLMD_ROUTINGSIDECAR_CONNECTOR=nixlv2 (default)
    # -------------------------------------------------------------------------
    routing:
      connector: nixlv2
    
    # -------------------------------------------------------------------------
    # Affinity Configuration
    # Corresponds to: LLMDBENCH_VLLM_COMMON_AFFINITY (left uncommented for auto-detect)
    # -------------------------------------------------------------------------
    # Uncomment and modify to select specific GPU types:
    # decode:
    #   acceleratorType:
    #     labelKey: nvidia.com/gpu.product
    #     labelValue: NVIDIA-H100-80GB-HBM3        # OpenShift
    #     # labelValue: H200                        # Kubernetes (gpu.nvidia.com/model)
    #     # labelValue: nvidia-tesla-a100           # GKE (cloud.google.com/gke-accelerator)
    #     # labelValue: nvidia-h100-80gb            # GKE
    #     # labelValue: NVIDIA-L40S                 # OpenShift
    #     # labelValue: NVIDIA-A100-SXM4-80GB       # OpenShift
    
    # -------------------------------------------------------------------------
    # Prefill Configuration
    # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_PREFILL_REPLICAS=0
    # -------------------------------------------------------------------------
    prefill:
      enabled: false
      replicas: 0
    
    # -------------------------------------------------------------------------
    # Decode Configuration
    # -------------------------------------------------------------------------
    decode:
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_DECODE_REPLICAS=2
      replicas: 2
      
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM=2
      parallelism:
        tensor: 2
        data: 1
        dataLocal: 1
        workers: 2
      
      # Resource configuration
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_NR=16
      #                 LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_MEM=64Gi
      resources:
        limits:
          memory: 64Gi
          cpu: "16"
        requests:
          memory: 64Gi
          cpu: "16"
      
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_DECODE_MODEL_COMMAND=custom
      vllm:
        useTemplatedArgs: true
        # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS
        # Using custom command to include prefix-caching-hash-algo and kv-events-config
        # 
        # kv-events-config and kv-transfer-config are autogenerated. To override their
        # values please adjust the below vllmCommon.kvEvents and vllmCommon.kvTransfer below
        # 
        customCommand: |
          vllm serve /model-cache/models/meta-llama/Llama-3.1-8B-Instruct \
          --host 0.0.0.0 \
          --served-model-name meta-llama/Llama-3.1-8B-Instruct \
          --port 8200 \
          --block-size 64 \
          --max-model-len 16000 \
          --tensor-parallel-size 2 \
          --gpu-memory-utilization 0.95 \
          --prefix-caching-hash-algo sha256_cbor \
          --enforce-eager
        # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS
        customPreprocessCommand: null  # Uses vllmCommon.preprocessScript
      
      # -----------------------------------------------------------------------
      # Extra Environment Variables
      # Corresponds to: LLMDBENCH_VLLM_COMMON_ENVVARS_TO_YAML
      # -----------------------------------------------------------------------
      extraEnvVars:
        - name: PYTHONHASHSEED
          value: "42"
        - name: POD_IP
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.podIP
        - name: UCX_TLS
          value: "rc,sm,cuda_ipc,cuda_copy,tcp"
        - name: UCX_SOCKADDR_TLS_PRIORITY
          value: "tcp"
        - name: VLLM_NIXL_SIDE_CHANNEL_HOST
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: VLLM_NIXL_SIDE_CHANNEL_PORT
          value: "5557"
        - name: VLLM_LOGGING_LEVEL
          value: "INFO"
        - name: VLLM_ALLOW_LONG_MAX_MODEL_LEN
          value: "1"
      
      # -----------------------------------------------------------------------
      # Extra Container Configuration
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_EXTRA_CONTAINER_CONFIG
      # -----------------------------------------------------------------------
      extraContainerConfig:
        ports:
          - containerPort: 5557  # NIXL side channel port
            protocol: TCP
          - containerPort: 8000  # Metrics port
            name: metrics
            protocol: TCP
      
      # -----------------------------------------------------------------------
      # Additional Volume Mounts
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS
      # Note: dshm and preprocesses are already in vllmCommon.volumeMounts
      # -----------------------------------------------------------------------
      additionalVolumeMounts: []
      
      # -----------------------------------------------------------------------
      # Additional Volumes
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES
      # Note: dshm and preprocesses are already in vllmCommon.volumes
      # -----------------------------------------------------------------------
      additionalVolumes: []
      
      # -----------------------------------------------------------------------
      # Multi-NIC Configuration (uncomment to enable)
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_DECODE_PODANNOTATIONS
      # -----------------------------------------------------------------------
      # extraPodLabels: {}
      # To enable multi-nic, add to annotations.decode.pod:
      # annotations:
      #   decode:
      #     pod:
      #       k8s.v1.cni.cncf.io/networks: multi-nic-compute
      
      # -----------------------------------------------------------------------
      # ROCE/GDR Configuration (uncomment to enable)
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_DECODE_NETWORK_RESOURCE
      #                 LLMDBENCH_VLLM_MODELSERVICE_DECODE_NETWORK_NR
      # -----------------------------------------------------------------------
      # To enable roce/gdr, add to resources:
      # resources:
      #   limits:
      #     rdma/roce_gdr: "16"
      #   requests:
      #     rdma/roce_gdr: "16"
    
    # -------------------------------------------------------------------------
    # vLLM Common Configuration
    # -------------------------------------------------------------------------
    vllmCommon:
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS
      preprocessScript: "python3 /setup/preprocess/set_llmdbench_environment.py; source $HOME/llmdbench_env.sh"
      
      kvTransfer:
        enabled: true
        connector: NixlConnector
        role: kv_both

      kvEvents:
        enabled: true
        publisher: zmq
        port: 5557
        topicPrefix: kv
        # serviceName defaults to model.shortName if not specified
        # serviceName: custom-service-name
      
      flags:
        enforceEager: true
        disableLogRequests: true
        disableUvicornAccessLog: true
        allowLongMaxModelLen: "1"
        serverDevMode: "1"
      
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES
      volumes:
        - name: preprocesses
          type: configMap
          configMap:
            name: llm-d-benchmark-preprocesses
            defaultMode: 320
        - name: dshm
          type: emptyDir
          emptyDir:
            medium: Memory
            # Corresponds to: LLMDBENCH_VLLM_COMMON_SHM_MEM
            sizeLimit: 16Gi
      
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS
      volumeMounts:
        - name: dshm
          mountPath: /dev/shm
        - name: preprocesses
          mountPath: /setup/preprocess
    
    # -------------------------------------------------------------------------
    # Storage Configuration
    # Corresponds to: LLMDBENCH_VLLM_COMMON_PVC_MODEL_CACHE_SIZE=1Ti
    #                 LLMDBENCH_VLLM_COMMON_PVC_STORAGE_CLASS (auto-detect if not set)
    # -------------------------------------------------------------------------
    storage:
      modelPvc:
        size: 1Ti
        # Uncomment to specify storage class:
        # storageClassName: standard-rwx
        # storageClassName: shared-vast
        # storageClassName: ocs-storagecluster-cephfs
    
    # -------------------------------------------------------------------------
    # Workload/Harness Configuration
    # Corresponds to: LLMDBENCH_HARNESS_NAME=inference-perf
    #                 LLMDBENCH_HARNESS_EXPERIMENT_PROFILE=shared_prefix_synthetic.yaml
    #                 LLMDBENCH_CONTROL_WORK_DIR=~/data/precise_prefix_cache_aware
    # -------------------------------------------------------------------------
    harness:
      name: inference-perf
      experimentProfile: shared_prefix_synthetic.yaml
      workDir: "~/data/precise_prefix_cache_aware"