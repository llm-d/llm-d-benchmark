# P/D DISAGGREGATION WELL LIT PATH
# Based on https://github.com/llm-d/llm-d/tree/main/guides/pd-disaggregation
# Migrated from bash environment variables to YAML format
#
# Notes:
# - Removed pod monitoring; can be added using decode.extraContainerConfig / prefill.extraContainerConfig
# - Removed extra volumes metrics-volume and torch-compile-volume (not needed for this model/hardware)
# - Use decode.additionalVolumeMounts and decode.additionalVolumes to add them if needed
# - Use prefill.additionalVolumeMounts and prefill.additionalVolumes to add them if needed

scenario:
  # ============================================================================
  # P/D Disaggregation - Llama 3.1 8B Instruct with 2 prefill and 2 decode pods
  # ============================================================================
  - name: "pd-disaggregation"
    
    # -------------------------------------------------------------------------
    # Model Configuration
    # Corresponds to: LLMDBENCH_DEPLOY_MODEL_LIST="meta-llama/Llama-3.1-8B-Instruct"
    # Alternative models (commented out in original):
    #   - Qwen/Qwen3-0.6B
    #   - facebook/opt-125m
    #   - meta-llama/Llama-3.1-70B-Instruct
    #   - deepseek-ai/DeepSeek-R1-0528
    # -------------------------------------------------------------------------
    model:
      name: meta-llama/Llama-3.1-8B-Instruct
      shortName: meta-lla-8b-instruct
      path: models/meta-llama/Llama-3.1-8B-Instruct
      huggingfaceId: meta-llama/Llama-3.1-8B-Instruct
      # Corresponds to: LLMDBENCH_VLLM_COMMON_PVC_MODEL_CACHE_SIZE=1Ti
      size: 1Ti
      # Corresponds to: LLMDBENCH_VLLM_COMMON_MAX_MODEL_LEN=16000
      maxModelLen: 16000
      # Corresponds to: LLMDBENCH_VLLM_COMMON_BLOCK_SIZE=128
      blockSize: 128
      tensorParallelSize: 1
      gpuMemoryUtilization: 0.95
      cacheBase: /model-cache
    
    # -------------------------------------------------------------------------
    # Routing Configuration
    # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_INFERENCE_MODEL=true
    # Corresponds to: LLMDBENCH_LLMD_ROUTINGSIDECAR_CONNECTOR=nixlv2 (default)
    # -------------------------------------------------------------------------
    routing:
      connector: nixlv2
    
    # -------------------------------------------------------------------------
    # Affinity Configuration
    # Corresponds to: LLMDBENCH_VLLM_COMMON_AFFINITY (left uncommented for auto-detect)
    # -------------------------------------------------------------------------
    # Uncomment and modify to select specific GPU types:
    # affinity:
    #   enabled: true
    #   nodeSelector:
    #     nvidia.com/gpu.product: NVIDIA-H100-80GB-HBM3        # OpenShift
    #     # gpu.nvidia.com/model: H200                         # Kubernetes
    #     # cloud.google.com/gke-accelerator: nvidia-tesla-a100  # GKE
    #     # cloud.google.com/gke-accelerator: nvidia-h100-80gb   # GKE
    #     # nvidia.com/gpu.product: NVIDIA-L40S                  # OpenShift
    #     # nvidia.com/gpu.product: NVIDIA-A100-SXM4-80GB        # OpenShift
    #     # nvidia.com/gpu                                       # ANY GPU (useful for Minikube)
    
    # -------------------------------------------------------------------------
    # Routing / GAIE Configuration
    # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_GAIE_PLUGINS_CONFIGFILE="default-plugins.yaml"
    # -------------------------------------------------------------------------
    inferenceExtension:
      pluginsConfigFile: "default-plugins.yaml"
    
    # -------------------------------------------------------------------------
    # Storage Configuration
    # Corresponds to: LLMDBENCH_VLLM_COMMON_PVC_MODEL_CACHE_SIZE=1Ti
    #                 LLMDBENCH_VLLM_COMMON_PVC_STORAGE_CLASS (auto-detect if not set)
    # -------------------------------------------------------------------------
    storage:
      modelPvc:
        size: 1Ti
        # Uncomment to specify storage class:
        # storageClassName: standard-rwx
        # storageClassName: shared-vast
        # storageClassName: ocs-storagecluster-cephfs
    
    # -------------------------------------------------------------------------
    # Prefill Configuration
    # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_PREFILL_*
    # -------------------------------------------------------------------------
    prefill:
      enabled: true
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_PREFILL_REPLICAS=2
      replicas: 2
      
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_PREFILL_TENSOR_PARALLELISM=1
      parallelism:
        tensor: 1
        data: 1
        dataLocal: 1
        workers: 1
      
      # Resource configuration
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_PREFILL_CPU_NR=32
      #                 LLMDBENCH_VLLM_MODELSERVICE_PREFILL_CPU_MEM=128Gi
      resources:
        limits:
          memory: 128Gi
          cpu: "32"
        requests:
          memory: 128Gi
          cpu: "32"
      
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_PREFILL_MODEL_COMMAND=custom
      #                 LLMDBENCH_VLLM_MODELSERVICE_PREFILL_PREPROCESS
      #                 LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_ARGS
      vllm:
        customCommand: |
          vllm serve /model-cache/models/meta-llama/Llama-3.1-8B-Instruct \
          --host 0.0.0.0 \
          --served-model-name meta-llama/Llama-3.1-8B-Instruct \
          --port 8200 \
          --block-size 128 \
          --max-model-len 16000 \
          --tensor-parallel-size 1 \
          --gpu-memory-utilization 0.95 \
          --disable-log-requests \
          --disable-uvicorn-access-log \
          --no-enable-prefix-caching
        # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_PREFILL_PREPROCESS
        customPreprocessCommand: null  # Uses vllmCommon.preprocessScript
      
      # -----------------------------------------------------------------------
      # Extra Environment Variables
      # Corresponds to: LLMDBENCH_VLLM_COMMON_ENVVARS_TO_YAML
      # -----------------------------------------------------------------------
      extraEnvVars:
        - name: UCX_TLS
          value: "rc,sm,cuda_ipc,cuda_copy,tcp"
        - name: UCX_SOCKADDR_TLS_PRIORITY
          value: "tcp"
        - name: VLLM_NIXL_SIDE_CHANNEL_PORT
          value: "5557"
        - name: VLLM_NIXL_SIDE_CHANNEL_HOST
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: VLLM_LOGGING_LEVEL
          value: "INFO"
        - name: VLLM_ALLOW_LONG_MAX_MODEL_LEN
          value: "1"
      
      # -----------------------------------------------------------------------
      # Extra Container Configuration
      # -----------------------------------------------------------------------
      extraContainerConfig:
        ports:
          - containerPort: 5557  # NIXL side channel port
            protocol: TCP
          - containerPort: 8000  # Metrics port
            name: metrics
            protocol: TCP
      
      # -----------------------------------------------------------------------
      # Additional Volume Mounts
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUME_MOUNTS
      # Note: dshm and preprocesses are already in vllmCommon.volumeMounts
      # -----------------------------------------------------------------------
      additionalVolumeMounts: []
      
      # -----------------------------------------------------------------------
      # Additional Volumes
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUMES
      # Note: dshm and preprocesses are already in vllmCommon.volumes
      # -----------------------------------------------------------------------
      additionalVolumes: []
      
      # -----------------------------------------------------------------------
      # Multi-NIC Configuration (uncomment to enable)
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_PREFILL_PODANNOTATIONS
      # -----------------------------------------------------------------------
      # To enable multi-nic, add to annotations.prefill.pod:
      # annotations:
      #   prefill:
      #     pod:
      #       k8s.v1.cni.cncf.io/networks: multi-nic-compute
      
      # -----------------------------------------------------------------------
      # ROCE/GDR Configuration (uncomment to enable)
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_PREFILL_NETWORK_RESOURCE
      #                 LLMDBENCH_VLLM_MODELSERVICE_PREFILL_NETWORK_NR
      # -----------------------------------------------------------------------
      # To enable roce/gdr, add to resources:
      # resources:
      #   limits:
      #     rdma/roce_gdr: "1"
      #   requests:
      #     rdma/roce_gdr: "1"
    
    # -------------------------------------------------------------------------
    # Decode Configuration
    # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_DECODE_*
    # -------------------------------------------------------------------------
    decode:
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_DECODE_REPLICAS=2
      replicas: 2
      
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM=1
      parallelism:
        tensor: 1
        data: 1
        dataLocal: 1
        workers: 1
      
      # Resource configuration
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_NR=32
      #                 LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_MEM=128Gi
      resources:
        limits:
          memory: 128Gi
          cpu: "32"
        requests:
          memory: 128Gi
          cpu: "32"
      
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_DECODE_MODEL_COMMAND=custom
      #                 LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS
      #                 LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS
      vllm:
        customCommand: |
          vllm serve /model-cache/models/meta-llama/Llama-3.1-8B-Instruct \
          --host 0.0.0.0 \
          --served-model-name meta-llama/Llama-3.1-8B-Instruct \
          --port 8200 \
          --block-size 128 \
          --max-model-len 16000 \
          --tensor-parallel-size 1 \
          --gpu-memory-utilization 0.95 \
          --disable-log-requests \
          --disable-uvicorn-access-log \
          --no-enable-prefix-caching
        # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS
        customPreprocessCommand: null  # Uses vllmCommon.preprocessScript
      
      # -----------------------------------------------------------------------
      # Extra Environment Variables
      # Corresponds to: LLMDBENCH_VLLM_COMMON_ENVVARS_TO_YAML
      # -----------------------------------------------------------------------
      extraEnvVars:
        - name: UCX_TLS
          value: "rc,sm,cuda_ipc,cuda_copy,tcp"
        - name: UCX_SOCKADDR_TLS_PRIORITY
          value: "tcp"
        - name: VLLM_NIXL_SIDE_CHANNEL_PORT
          value: "5557"
        - name: VLLM_NIXL_SIDE_CHANNEL_HOST
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: VLLM_LOGGING_LEVEL
          value: "INFO"
        - name: VLLM_ALLOW_LONG_MAX_MODEL_LEN
          value: "1"
      
      # -----------------------------------------------------------------------
      # Extra Container Configuration
      # -----------------------------------------------------------------------
      extraContainerConfig:
        ports:
          - containerPort: 5557  # NIXL side channel port
            protocol: TCP
          - containerPort: 8000  # Metrics port
            name: metrics
            protocol: TCP
      
      # -----------------------------------------------------------------------
      # Additional Volume Mounts
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS
      # Note: dshm and preprocesses are already in vllmCommon.volumeMounts
      # -----------------------------------------------------------------------
      additionalVolumeMounts: []
      
      # -----------------------------------------------------------------------
      # Additional Volumes
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES
      # Note: dshm and preprocesses are already in vllmCommon.volumes
      # -----------------------------------------------------------------------
      additionalVolumes: []
      
      # -----------------------------------------------------------------------
      # Multi-NIC Configuration (uncomment to enable)
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_DECODE_PODANNOTATIONS
      # -----------------------------------------------------------------------
      # To enable multi-nic, add to annotations.decode.pod:
      # annotations:
      #   decode:
      #     pod:
      #       k8s.v1.cni.cncf.io/networks: multi-nic-compute
      
      # -----------------------------------------------------------------------
      # ROCE/GDR Configuration (uncomment to enable)
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_DECODE_NETWORK_RESOURCE
      #                 LLMDBENCH_VLLM_MODELSERVICE_DECODE_NETWORK_NR
      # -----------------------------------------------------------------------
      # To enable roce/gdr, add to resources:
      # resources:
      #   limits:
      #     rdma/roce_gdr: "1"
      #   requests:
      #     rdma/roce_gdr: "1"
    
    # -------------------------------------------------------------------------
    # vLLM Common Configuration
    # -------------------------------------------------------------------------
    vllmCommon:
      host: "0.0.0.0"
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS / PREFILL_PREPROCESS
      preprocessScript: "python3 /setup/preprocess/set_llmdbench_environment.py; source $HOME/llmdbench_env.sh"
      
      # KV Transfer Configuration
      # Corresponds to: --kv-transfer-config in LLMDBENCH_VLLM_MODELSERVICE_*_EXTRA_ARGS
      kvTransfer:
        enabled: true
        connector: NixlConnector
        role: kv_both
      
      # KV Events not enabled in this scenario (used in precise-prefix-cache-aware)
      kvEvents:
        enabled: false
      
      flags:
        enforceEager: false  # Not in original bash script
        disableLogRequests: true
        disableUvicornAccessLog: true
        disablePrefixCaching: true  # Corresponds to: --no-enable-prefix-caching
        allowLongMaxModelLen: "1"
        serverDevMode: "1"
      
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_*_EXTRA_VOLUMES
      volumes:
        - name: preprocesses
          type: configMap
          configMap:
            name: llm-d-benchmark-preprocesses
            defaultMode: 320
        - name: dshm
          type: emptyDir
          emptyDir:
            medium: Memory
            # Corresponds to: LLMDBENCH_VLLM_COMMON_SHM_MEM (using default 16Gi)
            sizeLimit: 16Gi
      
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_*_EXTRA_VOLUME_MOUNTS
      volumeMounts:
        - name: dshm
          mountPath: /dev/shm
        - name: preprocesses
          mountPath: /setup/preprocess
    
    # -------------------------------------------------------------------------
    # Timeout Configuration
    # Corresponds to: LLMDBENCH_CONTROL_WAIT_TIMEOUT=900000
    #                 LLMDBENCH_HARNESS_WAIT_TIMEOUT=900000
    # -------------------------------------------------------------------------
    timeouts:
      controlWait: 900000
      harnessWait: 900000
    
    # -------------------------------------------------------------------------
    # Workload/Harness Configuration
    # Corresponds to: LLMDBENCH_HARNESS_NAME=vllm-benchmark
    #                 LLMDBENCH_HARNESS_EXPERIMENT_PROFILE=random_concurrent.yaml
    #                 LLMDBENCH_CONTROL_WORK_DIR=~/data/pd-disaggregation
    # -------------------------------------------------------------------------
    harness:
      name: vllm-benchmark
      experimentProfile: random_concurrent.yaml
      workDir: "~/data/pd-disaggregation"