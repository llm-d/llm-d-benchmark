# TIERED PREFIX CACHE / PREFIX CACHE OFFLOADING WELL LIT PATH
# Based on https://github.com/llm-d/llm-d/tree/main/guides/tiered-prefix-cache/README.md
# Migrated from bash environment variables to YAML format
#
# Notes:
# - Removed pod monitoring; can be added using decode.extraContainerConfig
# - Removed extra volumes metrics-volume and torch-compile-volume (not needed for this model/hardware)
# - Use decode.additionalVolumeMounts and decode.additionalVolumes to add them if needed

scenario:
  # ============================================================================
  # Tiered Prefix Cache - Llama 3.1 8B Instruct with 2 decode pods (no prefill)
  # ============================================================================
  - name: "tiered-prefix-cache"
    
    # -------------------------------------------------------------------------
    # Model Configuration
    # Corresponds to: LLMDBENCH_DEPLOY_MODEL_LIST="meta-llama/Llama-3.1-8B-Instruct"
    # Alternative models (commented out in original):
    #   - Qwen/Qwen3-0.6B
    #   - facebook/opt-125m
    #   - meta-llama/Llama-3.1-70B-Instruct
    #   - deepseek-ai/DeepSeek-R1-0528
    # -------------------------------------------------------------------------
    model:
      name: meta-llama/Llama-3.1-8B-Instruct
      shortName: meta-lla-8b-instruct
      path: models/meta-llama/Llama-3.1-8B-Instruct
      huggingfaceId: meta-llama/Llama-3.1-8B-Instruct
      # Corresponds to: LLMDBENCH_VLLM_COMMON_PVC_MODEL_CACHE_SIZE=1Ti
      size: 1Ti
      # Corresponds to: LLMDBENCH_VLLM_COMMON_MAX_MODEL_LEN=16000
      maxModelLen: 16000
      # Corresponds to: LLMDBENCH_VLLM_COMMON_BLOCK_SIZE=64
      blockSize: 64
      tensorParallelSize: 1
      gpuMemoryUtilization: 0.95
      cacheBase: /model-cache
    
    # -------------------------------------------------------------------------
    # Routing Configuration
    # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_INFERENCE_MODEL=true
    # -------------------------------------------------------------------------
    routing:
      connector: nixlv2
    
    # -------------------------------------------------------------------------
    # Affinity Configuration
    # Corresponds to: LLMDBENCH_VLLM_COMMON_AFFINITY (left uncommented for auto-detect)
    # -------------------------------------------------------------------------
    # Uncomment and modify to select specific GPU types:
    # affinity:
    #   enabled: true
    #   nodeSelector:
    #     nvidia.com/gpu.product: NVIDIA-H100-80GB-HBM3        # OpenShift
    #     # gpu.nvidia.com/model: H200                         # Kubernetes
    #     # cloud.google.com/gke-accelerator: nvidia-tesla-a100  # GKE
    #     # cloud.google.com/gke-accelerator: nvidia-h100-80gb   # GKE
    #     # nvidia.com/gpu.product: NVIDIA-L40S                  # OpenShift
    #     # nvidia.com/gpu.product: NVIDIA-A100-SXM4-80GB        # OpenShift
    #     # nvidia.com/gpu                                       # ANY GPU (useful for Minikube)
    
    # -------------------------------------------------------------------------
    # Routing / GAIE Configuration
    # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_GAIE_PLUGINS_CONFIGFILE="default-plugins.yaml"
    # -------------------------------------------------------------------------
    inferenceExtension:
      pluginsConfigFile: "default-plugins.yaml"
    
    # -------------------------------------------------------------------------
    # Storage Configuration
    # Corresponds to: LLMDBENCH_VLLM_COMMON_PVC_MODEL_CACHE_SIZE=1Ti
    #                 LLMDBENCH_VLLM_COMMON_PVC_STORAGE_CLASS (auto-detect if not set)
    # -------------------------------------------------------------------------
    storage:
      modelPvc:
        size: 1Ti
        # Uncomment to specify storage class:
        # storageClassName: standard-rwx
        # storageClassName: shared-vast
        # storageClassName: ocs-storagecluster-cephfs
    
    # -------------------------------------------------------------------------
    # Prefill Configuration
    # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_PREFILL_REPLICAS=0
    # -------------------------------------------------------------------------
    prefill:
      enabled: false
      replicas: 0
    
    # -------------------------------------------------------------------------
    # Decode Configuration
    # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_DECODE_*
    # -------------------------------------------------------------------------
    decode:
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_DECODE_REPLICAS=2
      replicas: 2
      
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM=1
      parallelism:
        tensor: 1
        data: 1
        dataLocal: 1
        workers: 1
      
      # Resource configuration
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_NR=48
      #                 LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_MEM=400Gi
      resources:
        limits:
          memory: 400Gi
          cpu: "48"
        requests:
          memory: 400Gi
          cpu: "48"
      
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_DECODE_MODEL_COMMAND=custom
      #                 LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS
      #                 LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS
      vllm:
        # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS
        # Note: Uses OffloadingConnector for tiered prefix cache
        # Note: kv-transfer-config is included in customCommand since it has extra config
        customCommand: |
          vllm serve /model-cache/models/meta-llama/Llama-3.1-8B-Instruct \
          --host 0.0.0.0 \
          --served-model-name meta-llama/Llama-3.1-8B-Instruct \
          --port 8200 \
          --block-size 64 \
          --max-model-len 16000 \
          --max-num-seq 256 \
          --tensor-parallel-size 1 \
          --gpu-memory-utilization 0.95 \
          --kv-transfer-config '{"kv_connector":"OffloadingConnector","kv_role":"kv_both","kv_connector_extra_config":{"num_cpu_blocks":10000}}' \
          --enforce-eager \
          --disable-log-requests \
          --disable-uvicorn-access-log \
          --enable-prefix-caching
        # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS
        customPreprocessCommand: "python3 /setup/preprocess/set_llmdbench_environment.py; source $HOME/llmdbench_env.sh"
      
      # -----------------------------------------------------------------------
      # Extra Environment Variables
      # Corresponds to: LLMDBENCH_VLLM_COMMON_ENVVARS_TO_YAML
      # -----------------------------------------------------------------------
      extraEnvVars:
        - name: PYTHONHASHSEED
          value: "123"
        - name: LMCACHE_MAX_LOCAL_CPU_SIZE
          value: "200.0"
        - name: UCX_TLS
          value: "rc,sm,cuda_ipc,cuda_copy,tcp"
        - name: UCX_SOCKADDR_TLS_PRIORITY
          value: "tcp"
        - name: VLLM_NIXL_SIDE_CHANNEL_PORT
          value: "5557"
        - name: VLLM_NIXL_SIDE_CHANNEL_HOST
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: VLLM_LOGGING_LEVEL
          value: "INFO"
        - name: VLLM_ALLOW_LONG_MAX_MODEL_LEN
          value: "1"
      
      # -----------------------------------------------------------------------
      # Extra Container Configuration
      # Corresponds to: LLMDBENCH_VLLM_COMMON_EXTRA_CONTAINER_CONFIG
      # -----------------------------------------------------------------------
      extraContainerConfig:
        ports:
          - containerPort: 5557  # NIXL side channel port
            protocol: TCP
          - containerPort: 8000  # Metrics port
            name: metrics
            protocol: TCP
      
      # -----------------------------------------------------------------------
      # Additional Volume Mounts
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS
      # Note: dshm and preprocesses are already in vllmCommon.volumeMounts
      # -----------------------------------------------------------------------
      additionalVolumeMounts: []
      
      # -----------------------------------------------------------------------
      # Additional Volumes
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES
      # Note: dshm and preprocesses are already in vllmCommon.volumes
      # -----------------------------------------------------------------------
      additionalVolumes: []
      
      # -----------------------------------------------------------------------
      # Multi-NIC Configuration (uncomment to enable)
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_DECODE_PODANNOTATIONS
      # -----------------------------------------------------------------------
      # To enable multi-nic, add to annotations.decode.pod:
      # annotations:
      #   decode:
      #     pod:
      #       k8s.v1.cni.cncf.io/networks: multi-nic-compute
      
      # -----------------------------------------------------------------------
      # ROCE/GDR Configuration (uncomment to enable)
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_DECODE_NETWORK_RESOURCE
      #                 LLMDBENCH_VLLM_MODELSERVICE_DECODE_NETWORK_NR
      # -----------------------------------------------------------------------
      # To enable roce/gdr, add to resources:
      # resources:
      #   limits:
      #     rdma/roce_gdr: "16"
      #   requests:
      #     rdma/roce_gdr: "16"
    
    # -------------------------------------------------------------------------
    # vLLM Common Configuration
    # -------------------------------------------------------------------------
    vllmCommon:
      host: "0.0.0.0"
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS
      preprocessScript: "python3 /setup/preprocess/set_llmdbench_environment.py; source $HOME/llmdbench_env.sh"
      
      # KV Transfer Configuration
      # Note: For tiered-prefix-cache, kv-transfer-config is in customCommand
      # because it includes kv_connector_extra_config
      kvTransfer:
        enabled: false  # Handled in customCommand for this scenario
      
      # KV Events not enabled in this scenario
      kvEvents:
        enabled: false 
      
      flags:
        enforceEager: true
        disableLogRequests: true
        disableUvicornAccessLog: true
        enablePrefixCaching: true  # Corresponds to: --enable-prefix-caching
        allowLongMaxModelLen: "1"
        serverDevMode: "1"
      
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES
      volumes:
        - name: preprocesses
          type: configMap
          configMap:
            name: llm-d-benchmark-preprocesses
            defaultMode: 320
        - name: dshm
          type: emptyDir
          emptyDir:
            medium: Memory
            # Corresponds to: LLMDBENCH_VLLM_COMMON_SHM_MEM (using default 16Gi)
            sizeLimit: 16Gi
      
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS
      volumeMounts:
        - name: dshm
          mountPath: /dev/shm
        - name: preprocesses
          mountPath: /setup/preprocess
    
    # -------------------------------------------------------------------------
    # Custom Image Configuration (optional)
    # Corresponds to commented out LLMDBENCH_LLMD_IMAGE_* variables
    # -------------------------------------------------------------------------
    # Uncomment to use lmcache image:
    # images:
    #   vllm:
    #     repository: docker.io/lmcache/vllm-openai
    #     tag: v0.3.7
    
    # -------------------------------------------------------------------------
    # Workload/Harness Configuration
    # Corresponds to: LLMDBENCH_HARNESS_NAME=inference-perf
    #                 LLMDBENCH_HARNESS_EXPERIMENT_PROFILE=shared_prefix_synthetic.yaml
    #                 LLMDBENCH_CONTROL_WORK_DIR=~/data/tiered-prefix-cache
    # -------------------------------------------------------------------------
    harness:
      name: inference-perf
      experimentProfile: shared_prefix_synthetic.yaml
      workDir: "~/data/tiered-prefix-cache"
