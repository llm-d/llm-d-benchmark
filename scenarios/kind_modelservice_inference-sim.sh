# A scenario to capture running inference-sim on a Kind cluster without requiring GPUs

# Skip model download step
export LLMDBENCH_CLIOVERRIDE_STEP_LIST=00_ensure_llm-d-infra,01_ensure_local_conda,02_ensure_gateway_provider,07_deploy_setup,08_deploy_gaie,09_deploy_via_modelservice
export LLMDBENCH_DEPLOY_METHODS=modelservice
export LLMDBENCH_VLLM_COMMON_REPLICAS=1
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_ACCELERATOR_NR=0
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_ACCELERATOR_NR=0
export LLMDBENCH_VLLM_COMMON_AFFINITY=kubernetes.io/os:linux
export LLMDBENCH_LLMD_IMAGE_NAME="llm-d-inference-sim"
export LLMDBENCH_LLMD_IMAGE_TAG="v0.3.0"
export LLMDBENCH_LLMD_ROUTINGSIDECAR_IMAGE_TAG="v0.2.0@sha256:a623a0752af0a71b7b05ebf95517848b5dbc3d8d235c1897035905632d5b7d80"
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_MODEL_COMMAND=imageDefault
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_MODEL_COMMAND=imageDefault
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS="[]"
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_ARGS="[]"
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_NR=0
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_CPU_NR=0
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_MEM=100Mi
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_CPU_MEM=100Mi
export LLMDBENCH_VLLM_MODELSERVICE_URI="hf://meta-llama/Llama-3.2-1B-Instruct"
export LLMDBENCH_DEPLOY_MODEL_LIST="meta-llama/Llama-3.2-1B-Instruct:meta-llama/Llama-3.2-1B-Instruct"
export LLMDBENCH_VLLM_COMMON_PVC_MODEL_CACHE_SIZE=100Mi
export LLMDBENCH_HARNESS_PVC_SIZE=3Gi
export LLMDBENCH_VLLM_COMMON_PVC_NAME=model-pvc
export LLMDBENCH_CONTROL_DEPLOY_IS_OPENSHIFT=0
export LLMDBENCH_CONTROL_DRY_RUN=0
export LLMDBENCH_CONTROL_RESOURCE_LIST=deployment,httproute,service,gateway,gatewayparameters,inferencepool,inferencemodel,cm,ing,pod,job

