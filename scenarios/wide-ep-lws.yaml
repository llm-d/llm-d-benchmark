# WIDE EP/DP WITH LWS WELL LIT PATH
# Based on https://github.com/llm-d/llm-d/tree/main/guides/wide-ep-lws/README.md
# Migrated from bash environment variables to YAML format
#
# Notes:
# - Removed pod monitoring; can be added using decode.extraContainerConfig / prefill.extraContainerConfig
# - Removed extra volumes metrics-volume and torch-compile-volume (not needed for this model/hardware)
# - Use decode.additionalVolumeMounts and decode.additionalVolumes to add them if needed
# - Use prefill.additionalVolumeMounts and prefill.additionalVolumes to add them if needed

scenario:
  # ============================================================================
  # Wide EP/DP with LWS - DeepSeek R1 with multinode support
  # ============================================================================
  - name: "wide-ep-lws"
    
    # -------------------------------------------------------------------------
    # Model Configuration
    # Corresponds to: LLMDBENCH_DEPLOY_MODEL_LIST="deepseek-ai/DeepSeek-R1-0528"
    # Alternative models (commented out in original):
    #   - Qwen/Qwen3-0.6B
    #   - facebook/opt-125m
    #   - meta-llama/Llama-3.1-8B-Instruct
    #   - meta-llama/Llama-3.1-70B-Instruct
    # -------------------------------------------------------------------------
    model:
      name: deepseek-ai/DeepSeek-R1-0528
      shortName: deepseek-r1-0528
      path: models/deepseek-ai/DeepSeek-R1-0528
      huggingfaceId: deepseek-ai/DeepSeek-R1-0528
      # Corresponds to: LLMDBENCH_VLLM_COMMON_PVC_MODEL_CACHE_SIZE=1Ti
      size: 1Ti
      cacheBase: /model-cache
    
    # -------------------------------------------------------------------------
    # Multinode Configuration (LeaderWorkerSet)
    # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_MULTINODE=true
    # -------------------------------------------------------------------------
    multinode:
      enabled: true
    
    # -------------------------------------------------------------------------
    # Scheduler Configuration
    # Corresponds to: LLMDBENCH_VLLM_COMMON_POD_SCHEDULER=custom-binpack-scheduler
    # -------------------------------------------------------------------------
    schedulerName: custom-binpack-scheduler
    
    # -------------------------------------------------------------------------
    # Routing Configuration
    # Corresponds to: LLMDBENCH_LLMD_ROUTINGSIDECAR_CONNECTOR=nixlv2 (default)
    # -------------------------------------------------------------------------
    routing:
      connector: nixlv2
    
    # -------------------------------------------------------------------------
    # Affinity Configuration
    # Corresponds to: LLMDBENCH_VLLM_COMMON_AFFINITY (left uncommented for auto-detect)
    # -------------------------------------------------------------------------
    # Uncomment and modify to select specific GPU types:
    # affinity:
    #   enabled: true
    #   nodeSelector:
    #     nvidia.com/gpu.product: NVIDIA-H100-80GB-HBM3        # OpenShift
    #     # gpu.nvidia.com/model: H200                         # Kubernetes
    #     # cloud.google.com/gke-accelerator: nvidia-tesla-a100  # GKE
    #     # cloud.google.com/gke-accelerator: nvidia-h100-80gb   # GKE
    #     # nvidia.com/gpu.product: NVIDIA-L40S                  # OpenShift
    #     # nvidia.com/gpu.product: NVIDIA-A100-SXM4-80GB        # OpenShift
    #     # nvidia.com/gpu                                       # ANY GPU (useful for Minikube)
    
    # -------------------------------------------------------------------------
    # Routing / GAIE Configuration
    # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_GAIE_PLUGINS_CONFIGFILE="custom-plugins.yaml"
    #                 LLMDBENCH_VLLM_MODELSERVICE_GAIE_CUSTOM_PLUGINS
    # -------------------------------------------------------------------------
    inferenceExtension:
      pluginsConfigFile: "custom-plugins.yaml"
      customPlugins:
        custom-plugins.yaml: |
          apiVersion: inference.networking.x-k8s.io/v1alpha1
          kind: EndpointPickerConfig
          plugins:
          - type: prefill-header-handler
          - type: prefill-filter
          - type: decode-filter
          - type: random-picker
            parameters:
              maxNumOfEndpoints: 1
          - type: pd-profile-handler
            parameters:
              threshold: 0
              hashBlockSize: 5
          schedulingProfiles:
          - name: prefill
            plugins:
            - pluginRef: prefill-filter
            - pluginRef: random-picker
          - name: decode
            plugins:
            - pluginRef: decode-filter
            - pluginRef: random-picker
    
    # -------------------------------------------------------------------------
    # Inference Pool Provider Configuration
    # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_INFERENCE_POOL_PROVIDER_CONFIG
    # -------------------------------------------------------------------------
    inferencePool:
      providerConfig:
        destinationRule:
          host: deepseek-r1-0528-gaie-epp
          trafficPolicy:
            connectionPool:
              http:
                http1MaxPendingRequests: 256000
                maxRequestsPerConnection: 256000
                http2MaxRequests: 256000
                idleTimeout: "900s"
              tcp:
                maxConnections: 256000
                maxConnectionDuration: "1800s"
                connectTimeout: "900s"
    
    # -------------------------------------------------------------------------
    # Storage Configuration
    # Corresponds to: LLMDBENCH_VLLM_COMMON_PVC_MODEL_CACHE_SIZE=1Ti
    #                 LLMDBENCH_VLLM_COMMON_PVC_STORAGE_CLASS (auto-detect if not set)
    # -------------------------------------------------------------------------
    storage:
      modelPvc:
        size: 1Ti
        # Uncomment to specify storage class:
        # storageClassName: standard-rwx
        # storageClassName: shared-vast
        # storageClassName: ocs-storagecluster-cephfs
    
    # -------------------------------------------------------------------------
    # Prefill Configuration
    # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_PREFILL_*
    # -------------------------------------------------------------------------
    prefill:
      enabled: true
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_PREFILL_REPLICAS=1
      replicas: 1
      
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_PREFILL_*_PARALLELISM
      parallelism:
        tensor: 1      # LLMDBENCH_VLLM_MODELSERVICE_PREFILL_TENSOR_PARALLELISM=1
        data: 1        # LLMDBENCH_VLLM_MODELSERVICE_PREFILL_DATA_PARALLELISM=1
        dataLocal: 8   # LLMDBENCH_VLLM_MODELSERVICE_PREFILL_DATA_LOCAL_PARALLELISM=8
        workers: 2     # LLMDBENCH_VLLM_MODELSERVICE_PREFILL_NUM_WORKERS_PARALLELISM=2
      
      # Resource configuration
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_PREFILL_CPU_NR=32
      #                 LLMDBENCH_VLLM_MODELSERVICE_PREFILL_CPU_MEM=512Gi
      #                 LLMDBENCH_VLLM_MODELSERVICE_PREFILL_NETWORK_RESOURCE=rdma/ib
      #                 LLMDBENCH_VLLM_MODELSERVICE_PREFILL_NETWORK_NR=1
      #                 LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EPHEMERAL_STORAGE=1Ti
      resources:
        limits:
          memory: 512Gi
          cpu: "32"
          rdma/ib: "1"
          ephemeral-storage: 1Ti
        requests:
          memory: 512Gi
          cpu: "32"
          rdma/ib: "1"
          ephemeral-storage: 1Ti
      
      # Accelerator configuration
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_PREFILL_ACCELERATOR_RESOURCE=nvidia
      #                 LLMDBENCH_VLLM_MODELSERVICE_PREFILL_ACCELERATOR_MEM_UTIL=0.75
      acceleratorType:
        labelKey: nvidia.com/gpu.product
        labelValue: NVIDIA-H100-80GB-HBM3
      
      # Mount model volume override
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_MOUNT_MODEL_VOLUME_OVERRIDE=true
      mountModelVolume: true
      
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_PREFILL_MODEL_COMMAND=custom
      #                 LLMDBENCH_VLLM_MODELSERVICE_PREFILL_PREPROCESS
      #                 LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_ARGS
      vllm:
        useTemplatedArgs: false
        imagePullPolicy: Always
        # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_ARGS
        customCommand: |
          exec vllm serve \
            deepseek-ai/DeepSeek-R1-0528 \
            --served-model-name deepseek-ai/DeepSeek-R1-0528 \
            --port 8200 \
            --trust-remote-code \
            --disable-uvicorn-access-log \
            --data-parallel-hybrid-lb \
            --enable-expert-parallel \
            --tensor-parallel-size $TP_SIZE \
            --data-parallel-size $((LWS_GROUP_SIZE * DP_SIZE_LOCAL)) \
            --data-parallel-size-local $DP_SIZE_LOCAL \
            --data-parallel-address ${LWS_LEADER_ADDRESS} \
            --data-parallel-rpc-port 5555 \
            --data-parallel-start-rank $START_RANK \
            --kv-transfer-config '{"kv_connector":"NixlConnector","kv_role":"kv_both","kv_load_failure_policy":"fail"}' \
            --async-scheduling \
            --enable-dbo \
            --dbo-prefill-token-threshold 32 \
            --enable-eplb \
            --eplb-config '{"window_size":"1000","step_interval":"3000","num_redundant_experts":"32","log_balancedness":"False"}' \
            --gpu-memory-utilization 0.75
        # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_PREFILL_PREPROCESS
        customPreprocessCommand: "python3 /setup/preprocess/set_llmdbench_environment.py; source $HOME/llmdbench_env.sh"
      
      # -----------------------------------------------------------------------
      # Extra Environment Variables
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_PREFILL_ENVVARS_TO_YAML
      # -----------------------------------------------------------------------
      extraEnvVars:
        - name: TRITON_LIBCUDA_PATH
          value: "/usr/lib64"
        - name: VLLM_SKIP_P2P_CHECK
          value: "1"
        - name: VLLM_RANDOMIZE_DP_DUMMY_INPUTS
          value: "1"
        - name: VLLM_USE_DEEP_GEMM
          value: "1"
        - name: VLLM_ALL2ALL_BACKEND
          value: "deepep_high_throughput"
        - name: NVIDIA_GDRCOPY
          value: "enabled"
        - name: NVSHMEM_REMOTE_TRANSPORT
          value: "ibgda"
        - name: NVSHMEM_IB_ENABLE_IBGDA
          value: "true"
        - name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
          value: "eth0"
        - name: GLOO_SOCKET_IFNAME
          value: "eth0"
        - name: NCCL_SOCKET_IFNAME
          value: "eth0"
        - name: VLLM_LOGGING_LEVEL
          value: "INFO"
        - name: CUDA_CACHE_PATH
          value: "/var/cache/vllm/cuda"
        - name: CCACHE_DIR
          value: "/var/cache/vllm/ccache"
        - name: VLLM_CACHE_ROOT
          value: "/var/cache/vllm/vllm"
        - name: FLASHINFER_WORKSPACE_BASE
          value: "/var/cache/vllm/flashinfer"
        - name: HF_HUB_CACHE
          value: "/var/cache/huggingface"
        - name: HF_HUB_DISABLE_XET
          value: "1"
        - name: NCCL_IB_HCA
          value: "ibp"
        - name: NVSHMEM_HCA_PREFIX
          value: "ibp"
      
      # -----------------------------------------------------------------------
      # Extra Container Configuration
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_CONTAINER_CONFIG
      # -----------------------------------------------------------------------
      extraContainerConfig:
        securityContext:
          capabilities:
            add:
              - IPC_LOCK
              - SYS_RAWIO
          runAsGroup: 0
          runAsUser: 0
      
      # -----------------------------------------------------------------------
      # Additional Volume Mounts
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUME_MOUNTS
      # -----------------------------------------------------------------------
      additionalVolumeMounts:
        - name: hf-cache
          mountPath: /var/cache/huggingface
        - name: jit-cache
          mountPath: /var/cache/vllm
      
      # -----------------------------------------------------------------------
      # Additional Volumes
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUMES
      # -----------------------------------------------------------------------
      additionalVolumes:
        - name: hf-cache
          type: hostPath
          hostPath:
            path: /mnt/local/hf-cache
            type: DirectoryOrCreate
        - name: jit-cache
          type: hostPath
          hostPath:
            path: /mnt/local/jit-cache
            type: DirectoryOrCreate
    
    # -------------------------------------------------------------------------
    # Decode Configuration
    # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_DECODE_*
    # -------------------------------------------------------------------------
    decode:
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_DECODE_REPLICAS=1
      replicas: 1
      
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_DECODE_*_PARALLELISM
      parallelism:
        tensor: 1      # LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM=1
        data: 1        # LLMDBENCH_VLLM_MODELSERVICE_DECODE_DATA_PARALLELISM=1
        dataLocal: 8   # LLMDBENCH_VLLM_MODELSERVICE_DECODE_DATA_LOCAL_PARALLELISM=8
        workers: 2     # LLMDBENCH_VLLM_MODELSERVICE_DECODE_NUM_WORKERS_PARALLELISM=2
      
      # Resource configuration
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_NR=32
      #                 LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_MEM=512Gi
      #                 LLMDBENCH_VLLM_MODELSERVICE_DECODE_NETWORK_RESOURCE=rdma/ib
      #                 LLMDBENCH_VLLM_MODELSERVICE_DECODE_NETWORK_NR=1
      #                 LLMDBENCH_VLLM_MODELSERVICE_DECODE_EPHEMERAL_STORAGE=1Ti
      resources:
        limits:
          memory: 512Gi
          cpu: "32"
          rdma/ib: "1"
          ephemeral-storage: 1Ti
        requests:
          memory: 512Gi
          cpu: "32"
          rdma/ib: "1"
          ephemeral-storage: 1Ti
      
      # Accelerator configuration
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_DECODE_ACCELERATOR_RESOURCE=nvidia
      acceleratorType:
        labelKey: nvidia.com/gpu.product
        labelValue: NVIDIA-H100-80GB-HBM3
      
      # Mount model volume override
      mountModelVolume: true
      
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_DECODE_MODEL_COMMAND=custom
      #                 LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS
      #                 LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS
      vllm:
        useTemplatedArgs: false
        imagePullPolicy: Always
        # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS
        customCommand: |
          exec vllm serve \
            deepseek-ai/DeepSeek-R1-0528 \
            --served-model-name deepseek-ai/DeepSeek-R1-0528 \
            --port 8200 \
            --trust-remote-code \
            --disable-uvicorn-access-log \
            --data-parallel-hybrid-lb \
            --enable-expert-parallel \
            --tensor-parallel-size $TP_SIZE \
            --data-parallel-size $((LWS_GROUP_SIZE * DP_SIZE_LOCAL)) \
            --data-parallel-size-local $DP_SIZE_LOCAL \
            --data-parallel-address ${LWS_LEADER_ADDRESS} \
            --data-parallel-rpc-port 5555 \
            --data-parallel-start-rank $START_RANK \
            --kv-transfer-config '{"kv_connector":"NixlConnector","kv_role":"kv_both","kv_load_failure_policy":"fail"}' \
            --async-scheduling \
            --enable-dbo \
            --dbo-decode-token-threshold 32 \
            --enable-eplb \
            --eplb-config '{"window_size":"1000","step_interval":"3000","num_redundant_experts":"32","log_balancedness":"False"}' \
            --compilation_config '{"cudagraph_mode": "FULL_DECODE_ONLY"}' \
            --kv-cache-memory-bytes=${KV_CACHE_MEMORY_BYTES-}
        # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS
        customPreprocessCommand: "python3 /setup/preprocess/set_llmdbench_environment.py; source $HOME/llmdbench_env.sh"
      
      # -----------------------------------------------------------------------
      # Extra Environment Variables
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_TO_YAML
      # Note: Decode has VLLM_MOE_DP_CHUNK_SIZE and deepep_low_latency (vs high_throughput)
      # -----------------------------------------------------------------------
      extraEnvVars:
        - name: VLLM_MOE_DP_CHUNK_SIZE
          value: "384"
        - name: TRITON_LIBCUDA_PATH
          value: "/usr/lib64"
        - name: VLLM_SKIP_P2P_CHECK
          value: "1"
        - name: VLLM_RANDOMIZE_DP_DUMMY_INPUTS
          value: "1"
        - name: VLLM_USE_DEEP_GEMM
          value: "1"
        - name: VLLM_ALL2ALL_BACKEND
          value: "deepep_low_latency"
        - name: NVIDIA_GDRCOPY
          value: "enabled"
        - name: NVSHMEM_REMOTE_TRANSPORT
          value: "ibgda"
        - name: NVSHMEM_IB_ENABLE_IBGDA
          value: "true"
        - name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
          value: "eth0"
        - name: GLOO_SOCKET_IFNAME
          value: "eth0"
        - name: NCCL_SOCKET_IFNAME
          value: "eth0"
        - name: VLLM_LOGGING_LEVEL
          value: "INFO"
        - name: CUDA_CACHE_PATH
          value: "/var/cache/vllm/cuda"
        - name: CCACHE_DIR
          value: "/var/cache/vllm/ccache"
        - name: VLLM_CACHE_ROOT
          value: "/var/cache/vllm/vllm"
        - name: FLASHINFER_WORKSPACE_BASE
          value: "/var/cache/vllm/flashinfer"
        - name: HF_HUB_CACHE
          value: "/var/cache/huggingface"
        - name: HF_HUB_DISABLE_XET
          value: "1"
        - name: NCCL_IB_HCA
          value: "ibp"
        - name: NVSHMEM_HCA_PREFIX
          value: "ibp"
      
      # -----------------------------------------------------------------------
      # Extra Container Configuration
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_CONTAINER_CONFIG
      # -----------------------------------------------------------------------
      extraContainerConfig:
        securityContext:
          capabilities:
            add:
              - IPC_LOCK
              - SYS_RAWIO
          runAsGroup: 0
          runAsUser: 0
      
      # -----------------------------------------------------------------------
      # Additional Volume Mounts
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS
      # -----------------------------------------------------------------------
      additionalVolumeMounts:
        - name: hf-cache
          mountPath: /var/cache/huggingface
        - name: jit-cache
          mountPath: /var/cache/vllm
      
      # -----------------------------------------------------------------------
      # Additional Volumes
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES
      # -----------------------------------------------------------------------
      additionalVolumes:
        - name: hf-cache
          type: hostPath
          hostPath:
            path: /mnt/local/hf-cache
            type: DirectoryOrCreate
        - name: jit-cache
          type: hostPath
          hostPath:
            path: /mnt/local/jit-cache
            type: DirectoryOrCreate
    
    # -------------------------------------------------------------------------
    # vLLM Common Configuration
    # -------------------------------------------------------------------------
    vllmCommon:
      host: "0.0.0.0"
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_*_PREPROCESS
      preprocessScript: "python3 /setup/preprocess/set_llmdbench_environment.py; source $HOME/llmdbench_env.sh"
      
      # KV Transfer Configuration
      # Note: kv-transfer-config is in customCommand for this scenario due to extra options
      kvTransfer:
        enabled: false  # Handled in customCommand
      
      # KV Events not enabled in this scenario
      kvEvents:
        enabled: false
      
      flags:
        enforceEager: false
        disableLogRequests: false
        disableUvicornAccessLog: true
        allowLongMaxModelLen: "1"
        serverDevMode: "1"
      
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_*_EXTRA_VOLUMES
      volumes:
        - name: preprocesses
          type: configMap
          configMap:
            name: llm-d-benchmark-preprocesses
            defaultMode: 320
        - name: dshm
          type: emptyDir
          emptyDir:
            medium: Memory
            # Corresponds to: LLMDBENCH_VLLM_COMMON_SHM_MEM
            # roughly 32MB per local DP plus scratch space
            sizeLimit: 16Gi
      
      # Corresponds to: LLMDBENCH_VLLM_MODELSERVICE_*_EXTRA_VOLUME_MOUNTS
      volumeMounts:
        - name: dshm
          mountPath: /dev/shm
        - name: preprocesses
          mountPath: /setup/preprocess
    
    # -------------------------------------------------------------------------
    # Workload/Harness Configuration
    # Corresponds to: LLMDBENCH_HARNESS_NAME=vllm-benchmark
    #                 LLMDBENCH_HARNESS_EXPERIMENT_PROFILE=random_concurrent.yaml
    #                 LLMDBENCH_CONTROL_WORK_DIR=~/data/wide_ep_lws
    # -------------------------------------------------------------------------
    harness:
      name: vllm-benchmark
      experimentProfile: random_concurrent.yaml
      workDir: "~/data/wide_ep_lws"