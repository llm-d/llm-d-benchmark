#!/bin/bash
# TIERED PREFIX CACHE - STORAGE OFFLOADING (llm-d-fs-connector) WELL LIT PATH
# Based on https://github.com/llm-d/llm-d/tree/main/guides/tiered-prefix-cache/storage
# Variant: llm-d-fs-connector (shared storage offloading)
# Generated by llm-d-benchmark guide converter
#
# This guide demonstrates KV cache offloading to shared storage using vLLM's
# native OffloadingConnector with the llm-d FS backend. This enables prefix
# cache reuse across multiple vLLM instances and nodes.
#
# Kustomize Hierarchy:
# guides/tiered-prefix-cache/storage/manifests/vllm/llm-d-fs-connector/kustomization.yaml
#     └── ../../../../../recipes/vllm/standard (referenced but not found in repo)
#
# NOTE: This guide uses a standalone vLLM image (vllm/vllm-openai:v0.14.1)
# and installs the llm-d FS connector at runtime, not the standard llm-d-cuda image.

# =============================================================================
# SOURCE: guides/tiered-prefix-cache/storage/manifests/vllm/llm-d-fs-connector/kustomization.yaml
# Lines 18-36 (args):
#   exec vllm serve \
#     Qwen/Qwen3-32B \
#     --tensor-parallel-size 2 \
#     --port 8000 \
#     --max-num-seq 1024 \
#     --kv-transfer-config '{...}'
# =============================================================================
export LLMDBENCH_DEPLOY_MODEL_LIST="Qwen/Qwen3-32B"

# =============================================================================
# SOURCE: guides/tiered-prefix-cache/storage/manifests/vllm/llm-d-fs-connector/kustomization.yaml
# Lines 10-12:
#   - op: replace
#     path: /spec/template/spec/containers/0/image
#     value: vllm/vllm-openai:v0.14.1
#
# NOTE: This guide uses the upstream vLLM image, not llm-d-cuda.
# The llm-d FS connector is installed at runtime via pip.
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_IMAGE="vllm/vllm-openai:v0.14.1"

# =============================================================================
# SOURCE: Benchmark framework default (guide requires 300Gi for Qwen3-32B)
# =============================================================================
export LLMDBENCH_VLLM_COMMON_PVC_MODEL_CACHE_SIZE="300Gi"

# =============================================================================
# SOURCE: guides/tiered-prefix-cache/storage/manifests/vllm/llm-d-fs-connector/kustomization.yaml
# Lines 41-47:
#   - op: add
#     path: /spec/template/spec/containers/0/resources
#     value:
#       limits:
#         nvidia.com/gpu: 2
#       requests:
#         nvidia.com/gpu: 2
#         memory: 400G
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM=2
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_MEM="400Gi"

# =============================================================================
# SOURCE: guides/tiered-prefix-cache/storage/manifests/vllm/llm-d-fs-connector/kustomization.yaml
# Lines 19-21:
#   --tensor-parallel-size 2 \
#   --port 8000 \
#   --max-num-seq 1024 \
# =============================================================================
export LLMDBENCH_VLLM_COMMON_MAX_NUM_SEQ=1024

# =============================================================================
# SOURCE: Benchmark framework convention (not in guide)
# Using custom command to inject preprocessing and exact vLLM args
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_MODEL_COMMAND=custom
export LLMDBENCH_VLLM_COMMON_PREPROCESS="python3 /setup/preprocess/set_llmdbench_environment.py; source \$HOME/llmdbench_env.sh"
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS=$LLMDBENCH_VLLM_COMMON_PREPROCESS

# =============================================================================
# SOURCE: guides/tiered-prefix-cache/storage/manifests/vllm/llm-d-fs-connector/kustomization.yaml
# Lines 15-36 (full command with llm-d FS connector installation and vLLM args):
#   pip install https://raw.githubusercontent.com/llm-d/llm-d-kv-cache/main/kv_connectors/llmd_fs_backend/wheels/llmd_fs_connector-0.1.0-cp312-cp312-linux_x86_64.whl
#   exec vllm serve \
#     Qwen/Qwen3-32B \
#     --tensor-parallel-size 2 \
#     --port 8000 \
#     --max-num-seq 1024 \
#     --kv-transfer-config '{
#       "kv_connector": "OffloadingConnector",
#       "kv_role": "kv_both",
#       "kv_connector_extra_config": {
#         "spec_name": "SharedStorageOffloadingSpec",
#         "shared_storage_path": "/mnt/files-storage/kv-cache/",
#         "block_size": 256,
#         "threads_per_gpu": 64,
#         "spec_module_path": "llmd_fs_backend.spec"
#       }
#     }'
#
# NOTE: Port mapping - Guide uses port 8000 but benchmark uses proxy pattern:
#   - INFERENCE_PORT (8000): proxy/sidecar listens here
#   - METRICS_PORT (8200): vLLM actually listens here
# The command is adapted to use METRICS_PORT while maintaining guide semantics.
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS
REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS; \
pip install https://raw.githubusercontent.com/llm-d/llm-d-kv-cache/main/kv_connectors/llmd_fs_backend/wheels/llmd_fs_connector-0.1.0-cp312-cp312-linux_x86_64.whl && \
exec vllm serve /model-cache/models/REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--host 0.0.0.0 \
--served-model-name REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--port REPLACE_ENV_LLMDBENCH_VLLM_COMMON_METRICS_PORT \
--tensor-parallel-size REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM \
--max-num-seq REPLACE_ENV_LLMDBENCH_VLLM_COMMON_MAX_NUM_SEQ \
--kv-transfer-config '{"kv_connector":"OffloadingConnector","kv_role":"kv_both","kv_connector_extra_config":{"spec_name":"SharedStorageOffloadingSpec","shared_storage_path":"/mnt/files-storage/kv-cache/","block_size":256,"threads_per_gpu":64,"spec_module_path":"llmd_fs_backend.spec"}}'
EOF

# =============================================================================
# SOURCE: guides/tiered-prefix-cache/storage/manifests/vllm/llm-d-fs-connector/kustomization.yaml
# Lines 70-77:
#   - op: add
#     path: /spec/template/spec/containers/0/env/-
#     value:
#       name: PYTHONHASHSEED
#       value: "42"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_TO_YAML=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_TO_YAML
- name: PYTHONHASHSEED
  value: "42"
EOF

# =============================================================================
# SOURCE: guides/tiered-prefix-cache/storage/manifests/vllm/llm-d-fs-connector/kustomization.yaml
# Lines 50-57 (PVC volume):
#   - op: add
#     path: /spec/template/spec/volumes/-
#     value:
#       name: files-storage
#       persistentVolumeClaim:
#         claimName: pvc-storage-files
#
# NOTE: The guide creates PVC separately via manifests/pvc.yaml.
# In llm-d-benchmark, you must create this PVC manually before deployment:
#   kubectl apply -f guides/tiered-prefix-cache/storage/manifests/pvc.yaml -n <namespace>
#
# PLUS benchmark framework standard volumes (preprocesses, dshm, etc.)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES=$(mktemp)
cat << 'EOF' > ${LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES}
- name: preprocesses
  configMap:
    defaultMode: 0755
    name: llm-d-benchmark-preprocesses
- name: dshm
  emptyDir:
    medium: Memory
    sizeLimit: REPLACE_ENV_LLMDBENCH_VLLM_COMMON_SHM_MEM
- name: files-storage
  persistentVolumeClaim:
    claimName: pvc-storage-files
EOF

# =============================================================================
# SOURCE: guides/tiered-prefix-cache/storage/manifests/vllm/llm-d-fs-connector/kustomization.yaml
# Lines 60-67 (PVC volume mount):
#   - op: add
#     path: /spec/template/spec/containers/0/volumeMounts/-
#     value:
#       name: files-storage
#       mountPath: /mnt/files-storage
#
# PLUS benchmark framework standard mounts (preprocesses, dshm, etc.)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS=$(mktemp)
cat << 'EOF' > ${LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS}
- name: dshm
  mountPath: /dev/shm
- name: preprocesses
  mountPath: /setup/preprocess
- name: files-storage
  mountPath: /mnt/files-storage
EOF

# =============================================================================
# SOURCE: Guide does not use P/D disaggregation
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_REPLICAS=0

# =============================================================================
# SOURCE: Guide mentions InferencePool deployment but doesn't specify GAIE config inline
# The guide references: "This guide uses an InferencePool recipe with HBM cache only"
# See: https://github.com/llm-d/llm-d/tree/main/guides/tiered-prefix-cache/storage#deploy-inferencepool
#
# NOTE: The guide indicates using default InferencePool config for HBM cache only.
# Storage offloading is handled by vLLM's OffloadingConnector, not GAIE plugins.
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_PLUGINS_CONFIGFILE="default-plugins.yaml"

# =============================================================================
# SOURCE: Benchmark framework defaults
# =============================================================================
export LLMDBENCH_HARNESS_NAME=inference-perf
export LLMDBENCH_HARNESS_EXPERIMENT_PROFILE=sanity_random.yaml
export LLMDBENCH_CONTROL_WORK_DIR=~/data/tiered-prefix-cache-storage

# =============================================================================
# IMPORTANT PREREQUISITES:
#
# 1. Create shared storage PVC BEFORE deploying:
#    kubectl apply -f https://raw.githubusercontent.com/llm-d/llm-d/main/guides/tiered-prefix-cache/storage/manifests/pvc.yaml -n <namespace>
#
# 2. Ensure your cluster has a storage class that supports ReadWriteMany (RWX):
#    - Default storage class with RWX support, OR
#    - GCP Lustre (see guide for setup), OR
#    - IBM Storage Scale, CephFS, AWS Lustre, etc.
#
# 3. The PVC name in manifests/pvc.yaml is "pvc-storage-files" and must match
#    the claimName in EXTRA_VOLUMES above.
#
# 4. Gateway and InferencePool must be deployed separately (not handled by this scenario):
#    - Gateway: Follow https://github.com/llm-d/llm-d/tree/main/recipes/gateway
#    - InferencePool: Follow https://github.com/llm-d/llm-d/tree/main/recipes/inferencepool
#
# =============================================================================
