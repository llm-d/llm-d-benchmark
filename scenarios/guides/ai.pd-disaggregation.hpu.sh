# PD-DISAGGREGATION (HPU) WELL LIT PATH
# Based on https://github.com/llm-d/llm-d/tree/main/guides/pd-disaggregation
# Variant: hpu (Intel Gaudi HPU accelerator)
# Generated by llm-d-benchmark guide converter

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_hpu.yaml
# Lines 9-18:
#   modelArtifacts:
#     uri: "hf://Qwen/Qwen3-8B"
#     size: 100Gi
#     authSecretName: "llm-d-hf-token"
#     name: "Qwen/Qwen3-8B"
#     labels:
#       llm-d.ai/inference-serving: "true"
#       llm-d.ai/guide: "pd-disaggregation"
#       llm-d.ai/hardware-variant: "hpu"
#       llm-d.ai/hardware-vendor: "intel"
#       llm-d.ai/model: "Qwen3-8B"
# =============================================================================
export LLMDBENCH_DEPLOY_MODEL_LIST="Qwen/Qwen3-8B"

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_hpu.yaml
# Lines 9-10:
#   uri: "hf://Qwen/Qwen3-8B"
#   size: 100Gi
# =============================================================================
export LLMDBENCH_VLLM_COMMON_PVC_MODEL_CACHE_SIZE=100Gi

# =============================================================================
# SOURCE: guides/pd-disaggregation/helmfile.yaml.gotmpl
# Lines 41-44:
#   - name: infra-pd
#     namespace: llm-d-pd
#     chart: llm-d-infra/llm-d-infra
#     version: v1.3.6
# =============================================================================
export LLMDBENCH_VLLM_INFRA_CHART_VERSION="v1.3.6"

# =============================================================================
# SOURCE: guides/pd-disaggregation/helmfile.yaml.gotmpl
# Lines 53-56:
#   - name: gaie-pd
#     namespace: llm-d-pd
#     chart: oci://registry.k8s.io/gateway-api-inference-extension/charts/inferencepool
#     version: v1.3.0
# =============================================================================
export LLMDBENCH_VLLM_GAIE_CHART_VERSION="v1.3.0"

# =============================================================================
# SOURCE: guides/pd-disaggregation/helmfile.yaml.gotmpl
# Lines 88-91:
#   - name: ms-pd
#     namespace: llm-d-pd
#     chart: llm-d-modelservice/llm-d-modelservice
#     version: v0.4.5
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_CHART_VERSION="v0.4.5"

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_hpu.yaml
# Lines 4-6:
#   accelerator:
#     type: intel-gaudi
#     dra: true
# =============================================================================
export LLMDBENCH_VLLM_COMMON_ACCELERATOR_RESOURCE="intel-gaudi"
export LLMDBENCH_VLLM_COMMON_ACCELERATOR_USE_DRA=true

# =============================================================================
# SOURCE: guides/pd-disaggregation/gaie-pd/values.yaml
# Lines 7-10:
#   image:
#     name: llm-d-inference-scheduler
#     hub: ghcr.io/llm-d
#     tag: v0.5.0
# =============================================================================
export LLMDBENCH_LLMD_INFERENCESCHEDULER_IMAGE_TAG="v0.5.0"

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_hpu.yaml
# Lines 40-41:
#   containers:
#   - name: vllm
#     image: "opea/vllm-gaudi:1.22.0"
#
# NOTE: HPU uses OPEA vLLM Gaudi image instead of standard llm-d image
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_IMAGE="opea/vllm-gaudi:1.22.0"
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_IMAGE="opea/vllm-gaudi:1.22.0"

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values.yaml
# Line 16: servicePort: 8000
# =============================================================================
export LLMDBENCH_VLLM_COMMON_INFERENCE_PORT=8000

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values.yaml
# Lines 17-20:
#   proxy:
#     image: ghcr.io/llm-d/llm-d-routing-sidecar:v0.5.0
#     connector: nixlv2
#     secure: false
# =============================================================================
export LLMDBENCH_LLMD_ROUTINGSIDECAR_IMAGE_TAG="v0.5.0"
export LLMDBENCH_LLMD_ROUTINGSIDECAR_ENABLED=true
export LLMDBENCH_LLMD_ROUTINGSIDECAR_CONNECTOR="nixlv2"

# =============================================================================
# DECODE STAGE CONFIGURATION
# =============================================================================

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_hpu.yaml
# Lines 28-31:
#   decode:
#     parallelism:
#       tensor: 4
#     replicas: 1
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_REPLICAS=1
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM=4

# =============================================================================
# SOURCE: Benchmark framework convention (not in guide)
# Using custom command to inject preprocessing and exact vLLM args
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_MODEL_COMMAND=custom
export LLMDBENCH_VLLM_COMMON_PREPROCESS="python3 /setup/preprocess/set_llmdbench_environment.py; source \$HOME/llmdbench_env.sh"
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS=$LLMDBENCH_VLLM_COMMON_PREPROCESS

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_hpu.yaml
# Lines 45-50 (decode container args):
#   args:
#     - "--enforce-eager"
#     - "--gpu-memory-utilization"
#     - "0.9"
#     - "--kv-transfer-config"
#     - '{"kv_connector":"NixlConnector","kv_role":"kv_both","kv_buffer_device":"cpu"}'
# =============================================================================
export LLMDBENCH_VLLM_COMMON_ACCELERATOR_MEM_UTIL=0.9

export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS
REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS; \
vllm serve /model-cache/models/REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--host 0.0.0.0 \
--served-model-name REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--port REPLACE_ENV_LLMDBENCH_VLLM_COMMON_METRICS_PORT \
--tensor-parallel-size REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM \
--enforce-eager \
--gpu-memory-utilization REPLACE_ENV_LLMDBENCH_VLLM_COMMON_ACCELERATOR_MEM_UTIL \
--kv-transfer-config '{"kv_connector":"NixlConnector","kv_role":"kv_both","kv_buffer_device":"cpu"}'
EOF

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_hpu.yaml
# Lines 51-77 (decode container env vars):
#   env:
#     - name: OMPI_MCA_btl_vader_single_copy_mechanism
#       value: "none"
#     - name: HABANA_LOGS
#       value: "/tmp/habana_logs"
#     - name: UCX_TLS
#       value: "tcp"
#     - name: VLLM_NIXL_SIDE_CHANNEL_HOST
#       valueFrom:
#         fieldRef:
#           fieldPath: status.podIP
#     - name: VLLM_NIXL_SIDE_CHANNEL_PORT
#       value: "5600"
#     - name: VLLM_LOGGING_LEVEL
#       value: "DEBUG"
#     - name: VLLM_SKIP_WARMUP
#       value: "true"
#     - name: PT_HPU_LAZY_MODE
#       value: "1"
#     - name: PT_HPU_ENABLE_LAZY_COLLECTIVES
#       value: "true"
#     - name: LD_LIBRARY_PATH
#       value: "/usr/lib/nixl_ofi:/usr/local/lib:/usr/lib:/opt/amazon/openmpi/lib:/usr/lib/habanalabs"
#     - name: DO_NOT_TRACK
#       value: "1"
#     - name: VLLM_USE_V1
#       value: "1"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_TO_YAML=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_TO_YAML
- name: OMPI_MCA_btl_vader_single_copy_mechanism
  value: "none"
- name: HABANA_LOGS
  value: "/tmp/habana_logs"
- name: UCX_TLS
  value: "tcp"
- name: VLLM_NIXL_SIDE_CHANNEL_HOST
  valueFrom:
    fieldRef:
      fieldPath: status.podIP
- name: VLLM_NIXL_SIDE_CHANNEL_PORT
  value: "5600"
- name: VLLM_LOGGING_LEVEL
  value: "DEBUG"
- name: VLLM_SKIP_WARMUP
  value: "true"
- name: PT_HPU_LAZY_MODE
  value: "1"
- name: PT_HPU_ENABLE_LAZY_COLLECTIVES
  value: "true"
- name: LD_LIBRARY_PATH
  value: "/usr/lib/nixl_ofi:/usr/local/lib:/usr/lib:/opt/amazon/openmpi/lib:/usr/lib/habanalabs"
- name: DO_NOT_TRACK
  value: "1"
- name: VLLM_USE_V1
  value: "1"
EOF

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_hpu.yaml
# Lines 42-43:
#   securityContext:
#     privileged: true
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_CONTAINER_CONFIG=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_CONTAINER_CONFIG
securityContext:
  privileged: true
EOF

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_hpu.yaml
# Lines 86-92 (decode volume mounts):
#   volumeMounts:
#   - name: metrics-volume
#     mountPath: /.config
#   - name: shm
#     mountPath: /dev/shm
#   - name: torch-compile-cache
#     mountPath: /.cache
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS}
- name: preprocesses
  mountPath: /setup/preprocess
- name: metrics-volume
  mountPath: /.config
- name: shm
  mountPath: /dev/shm
- name: torch-compile-cache
  mountPath: /.cache
EOF

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_hpu.yaml
# Lines 114-122 (decode volumes):
#   volumes:
#   - name: metrics-volume
#     emptyDir: {}
#   - name: shm
#     emptyDir:
#       medium: Memory
#       sizeLimit: "16Gi"
#   - name: torch-compile-cache
#     emptyDir: {}
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_SHM_MEM="16Gi"

export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES}
- name: preprocesses
  configMap:
    defaultMode: 0755
    name: llm-d-benchmark-preprocesses
- name: metrics-volume
  emptyDir: {}
- name: shm
  emptyDir:
    medium: Memory
    sizeLimit: REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_SHM_MEM
- name: torch-compile-cache
  emptyDir: {}
EOF

# =============================================================================
# PREFILL STAGE CONFIGURATION
# =============================================================================

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_hpu.yaml
# Lines 125-128:
#   prefill:
#     parallelism:
#       tensor: 1
#     replicas: 4
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_REPLICAS=4
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_TENSOR_PARALLELISM=1

# =============================================================================
# SOURCE: Benchmark framework convention (not in guide)
# Using custom command to inject preprocessing and exact vLLM args
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_MODEL_COMMAND=custom
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_PREPROCESS=$LLMDBENCH_VLLM_COMMON_PREPROCESS

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_hpu.yaml
# Lines 142-147 (prefill container args):
#   args:
#     - "--enforce-eager"
#     - "--gpu-memory-utilization"
#     - "0.9"
#     - "--kv-transfer-config"
#     - '{"kv_connector":"NixlConnector","kv_role":"kv_both","kv_buffer_device":"cpu"}'
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_ARGS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_ARGS
REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_PREFILL_PREPROCESS; \
vllm serve /model-cache/models/REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--host 0.0.0.0 \
--served-model-name REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--port REPLACE_ENV_LLMDBENCH_VLLM_COMMON_METRICS_PORT \
--tensor-parallel-size REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_PREFILL_TENSOR_PARALLELISM \
--enforce-eager \
--gpu-memory-utilization REPLACE_ENV_LLMDBENCH_VLLM_COMMON_ACCELERATOR_MEM_UTIL \
--kv-transfer-config '{"kv_connector":"NixlConnector","kv_role":"kv_both","kv_buffer_device":"cpu"}'
EOF

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_hpu.yaml
# Lines 148-174 (prefill container env vars):
#   env:
#     - name: OMPI_MCA_btl_vader_single_copy_mechanism
#       value: "none"
#     - name: HABANA_LOGS
#       value: "/tmp/habana_logs"
#     - name: UCX_TLS
#       value: "tcp"
#     - name: VLLM_NIXL_SIDE_CHANNEL_HOST
#       valueFrom:
#         fieldRef:
#           fieldPath: status.podIP
#     - name: VLLM_NIXL_SIDE_CHANNEL_PORT
#       value: "5600"
#     - name: VLLM_LOGGING_LEVEL
#       value: "DEBUG"
#     - name: VLLM_SKIP_WARMUP
#       value: "true"
#     - name: PT_HPU_LAZY_MODE
#       value: "1"
#     - name: PT_HPU_ENABLE_LAZY_COLLECTIVES
#       value: "true"
#     - name: LD_LIBRARY_PATH
#       value: "/usr/lib/nixl_ofi:/usr/local/lib:/usr/lib:/opt/amazon/openmpi/lib:/usr/lib/habanalabs"
#     - name: DO_NOT_TRACK
#       value: "1"
#     - name: VLLM_USE_V1
#       value: "1"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_ENVVARS_TO_YAML=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_PREFILL_ENVVARS_TO_YAML
- name: OMPI_MCA_btl_vader_single_copy_mechanism
  value: "none"
- name: HABANA_LOGS
  value: "/tmp/habana_logs"
- name: UCX_TLS
  value: "tcp"
- name: VLLM_NIXL_SIDE_CHANNEL_HOST
  valueFrom:
    fieldRef:
      fieldPath: status.podIP
- name: VLLM_NIXL_SIDE_CHANNEL_PORT
  value: "5600"
- name: VLLM_LOGGING_LEVEL
  value: "DEBUG"
- name: VLLM_SKIP_WARMUP
  value: "true"
- name: PT_HPU_LAZY_MODE
  value: "1"
- name: PT_HPU_ENABLE_LAZY_COLLECTIVES
  value: "true"
- name: LD_LIBRARY_PATH
  value: "/usr/lib/nixl_ofi:/usr/local/lib:/usr/lib:/opt/amazon/openmpi/lib:/usr/lib/habanalabs"
- name: DO_NOT_TRACK
  value: "1"
- name: VLLM_USE_V1
  value: "1"
EOF

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_hpu.yaml
# Lines 139-140:
#   securityContext:
#     privileged: true
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_CONTAINER_CONFIG=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_CONTAINER_CONFIG
securityContext:
  privileged: true
EOF

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_hpu.yaml
# Lines 183-189 (prefill volume mounts):
#   volumeMounts:
#   - name: metrics-volume
#     mountPath: /.config
#   - name: shm
#     mountPath: /dev/shm
#   - name: torch-compile-cache
#     mountPath: /.cache
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUME_MOUNTS=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUME_MOUNTS}
- name: preprocesses
  mountPath: /setup/preprocess
- name: metrics-volume
  mountPath: /.config
- name: shm
  mountPath: /dev/shm
- name: torch-compile-cache
  mountPath: /.cache
EOF

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_hpu.yaml
# Lines 212-220 (prefill volumes):
#   volumes:
#   - name: metrics-volume
#     emptyDir: {}
#   - name: shm
#     emptyDir:
#       medium: Memory
#       sizeLimit: "16Gi"
#   - name: torch-compile-cache
#     emptyDir: {}
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_SHM_MEM="16Gi"

export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUMES=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUMES}
- name: preprocesses
  configMap:
    defaultMode: 0755
    name: llm-d-benchmark-preprocesses
- name: metrics-volume
  emptyDir: {}
- name: shm
  emptyDir:
    medium: Memory
    sizeLimit: REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_PREFILL_SHM_MEM
- name: torch-compile-cache
  emptyDir: {}
EOF

# =============================================================================
# GAIE CONFIGURATION
# =============================================================================

# =============================================================================
# SOURCE: guides/pd-disaggregation/gaie-pd/values.yaml
# Lines 3-6:
#   flags:
#     # in vLLM 10.0+, the metric is renamed while upstream GAIE is still using the old name as default.
#     # See https://github.com/kubernetes-sigs/gateway-api-inference-extension/pull/1905.
#     kv-cache-usage-percentage-metric: "vllm:kv_cache_usage_perc"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_FLAGS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_GAIE_FLAGS
kv-cache-usage-percentage-metric: "vllm:kv_cache_usage_perc"
EOF

# =============================================================================
# SOURCE: guides/pd-disaggregation/gaie-pd/values.yaml
# Line 13: pluginsConfigFile: "pd-config.yaml"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_PLUGINS_CONFIGFILE="pd-config.yaml"

# =============================================================================
# SOURCE: guides/pd-disaggregation/gaie-pd/values.yaml
# Lines 14-39:
#   pluginsCustomConfig:
#     pd-config.yaml: |
#       # ALWAYS DO PD IN THIS EXAMPLE (THRESHOLD 0)
#       apiVersion: inference.networking.x-k8s.io/v1alpha1
#       kind: EndpointPickerConfig
#       plugins:
#       - type: prefill-header-handler
#       - type: prefill-filter
#       - type: decode-filter
#       - type: queue-scorer
#       - type: pd-profile-handler
#         parameters:
#           threshold: 0
#           hashBlockSize: 5
#       schedulingProfiles:
#       - name: prefill
#         plugins:
#         - pluginRef: prefill-filter
#         - pluginRef: queue-scorer
#           weight: 1.0
#       - name: decode
#         plugins:
#         - pluginRef: decode-filter
#         - pluginRef: queue-scorer
#           weight: 1.0
#       # All profiles using max score picker by default
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_CUSTOM_PLUGINS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_GAIE_CUSTOM_PLUGINS
pd-config.yaml: |
  # ALWAYS DO PD IN THIS EXAMPLE (THRESHOLD 0)
  apiVersion: inference.networking.x-k8s.io/v1alpha1
  kind: EndpointPickerConfig
  plugins:
  - type: prefill-header-handler
  - type: prefill-filter
  - type: decode-filter
  - type: queue-scorer
  - type: pd-profile-handler
    parameters:
      threshold: 0
      hashBlockSize: 5
  schedulingProfiles:
  - name: prefill
    plugins:
    - pluginRef: prefill-filter
    - pluginRef: queue-scorer
      weight: 1.0
  - name: decode
    plugins:
    - pluginRef: decode-filter
    - pluginRef: queue-scorer
      weight: 1.0
  # All profiles using max score picker by default
EOF

# =============================================================================
# HARNESS CONFIGURATION
# =============================================================================

# =============================================================================
# SOURCE: Benchmark framework defaults
# =============================================================================
export LLMDBENCH_HARNESS_NAME=inference-perf
export LLMDBENCH_HARNESS_EXPERIMENT_PROFILE=sanity_random.yaml
export LLMDBENCH_CONTROL_WORK_DIR=~/data/pd-disaggregation-hpu
