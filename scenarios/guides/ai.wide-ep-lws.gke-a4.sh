# WIDE-EP-LWS (GKE-A4) WELL LIT PATH
# Based on https://github.com/llm-d/llm-d/tree/main/guides/wide-ep-lws
# Variant: gke-a4
# Generated by llm-d-benchmark guide converter
#
# Kustomize Hierarchy:
# guides/wide-ep-lws/manifests/modelserver/gke-a4/kustomization.yaml
#     └── ../gke/kustomization.yaml
#             └── ../base/kustomization.yaml
#                     └── decode.yaml, prefill.yaml

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Line 9 (label): llm-d.ai/model: DeepSeek-R1-0528
# Line 84: vllm serve deepseek-ai/DeepSeek-R1-0528
#
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/prefill.yaml
# Line 9 (label): llm-d.ai/model: DeepSeek-R1-0528
# Line 68: vllm serve deepseek-ai/DeepSeek-R1-0528
# =============================================================================
export LLMDBENCH_DEPLOY_MODEL_LIST="deepseek-ai/DeepSeek-R1-0528"

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Line 65: image: ghcr.io/llm-d/llm-d-cuda:v0.5.0
#
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/prefill.yaml
# Line 36: image: ghcr.io/llm-d/llm-d-cuda:v0.5.0
# =============================================================================
export LLMDBENCH_LLMD_IMAGE_TAG="v0.5.0"

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/inferencepool.values.yaml
# Lines 7-9:
#   image:
#     hub: ghcr.io/llm-d
#     tag: v0.5.0
# =============================================================================
export LLMDBENCH_LLMD_INFERENCESCHEDULER_IMAGE_TAG="v0.5.0"

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Line 32: image: ghcr.io/llm-d/llm-d-routing-sidecar:v0.5.0
# =============================================================================
export LLMDBENCH_LLMD_ROUTINGSIDECAR_IMAGE_TAG="v0.5.0"

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Line 13: spec.replicas: 1
# Line 15: leaderWorkerTemplate.size: 2
#
# NOTE: This is a LeaderWorkerSet deployment with 1 replica group of 2 pods
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_MULTINODE=true
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_REPLICAS=1
export LLMDBENCH_VLLM_COMMON_NUM_WORKERS_PARALLELISM=2

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/prefill.yaml
# Line 13: spec.replicas: 1
# Line 15: leaderWorkerTemplate.size: 2
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_REPLICAS=1

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Lines 119-120:
#   - name: TP_SIZE
#     value: "1"
# Lines 117-118:
#   - name: DP_SIZE_LOCAL
#     value: "8"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM=1
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_DATA_LOCAL_PARALLELISM=8

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/prefill.yaml
# Lines 93-94:
#   - name: TP_SIZE
#     value: "1"
# Lines 91-92:
#   - name: DP_SIZE_LOCAL
#     value: "8"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_TENSOR_PARALLELISM=1
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_DATA_LOCAL_PARALLELISM=8

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Lines 185-195 (resources):
#   limits:
#     memory: 512Gi
#   requests:
#     cpu: 32
#     memory: 512Gi
#     nvidia.com/gpu: "8"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_NR=32
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_MEM=512Gi
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_ACCELERATOR_NR=8

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/prefill.yaml
# Lines 160-170 (resources):
#   limits:
#     memory: 512Gi
#   requests:
#     cpu: 32
#     memory: 512Gi
#     nvidia.com/gpu: "8"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_CPU_NR=32
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_CPU_MEM=512Gi
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_ACCELERATOR_NR=8

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Lines 52-55:
#   - name: dshm
#     emptyDir:
#       medium: Memory
#       sizeLimit: 2Gi
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_SHM_MEM=2Gi

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/prefill.yaml
# Lines 23-26:
#   - name: dshm
#     emptyDir:
#       medium: Memory
#       sizeLimit: 2Gi
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_SHM_MEM=2Gi

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Line 33 (routing-proxy init container): --port=8000 --vllm-port=8200
# Line 85: vllm serve ... --port 8200
#
# NOTE: Decode uses routing proxy on port 8000, vLLM on port 8200
# =============================================================================
export LLMDBENCH_LLMD_ROUTINGSIDECAR_ENABLED=true
export LLMDBENCH_LLMD_ROUTINGSIDECAR_CONNECTOR=nixlv2
export LLMDBENCH_VLLM_COMMON_INFERENCE_PORT=8000

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/prefill.yaml
# Line 69: vllm serve ... --port 8000
#
# NOTE: Prefill vLLM listens directly on port 8000 (no routing proxy)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_INFERENCE_PORT=8000

# =============================================================================
# SOURCE: Benchmark framework convention (not in guide)
# Using custom command to inject preprocessing and exact vLLM args
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_MODEL_COMMAND=custom
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_MODEL_COMMAND=custom
export LLMDBENCH_VLLM_COMMON_PREPROCESS="python3 /setup/preprocess/set_llmdbench_environment.py; source \$HOME/llmdbench_env.sh"
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS=$LLMDBENCH_VLLM_COMMON_PREPROCESS
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_PREPROCESS=$LLMDBENCH_VLLM_COMMON_PREPROCESS

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Lines 73-115 (vLLM args):
#   exec vllm serve \
#     deepseek-ai/DeepSeek-R1-0528 \
#     --port 8200 \
#     --trust-remote-code \
#     --disable-uvicorn-access-log \
#     --data-parallel-hybrid-lb \
#     --enable-expert-parallel \
#     --tensor-parallel-size $TP_SIZE \
#     --data-parallel-size $((LWS_GROUP_SIZE * DP_SIZE_LOCAL)) \
#     --data-parallel-size-local $DP_SIZE_LOCAL \
#     --data-parallel-address ${LWS_LEADER_ADDRESS} \
#     --data-parallel-rpc-port 5555 \
#     --data-parallel-start-rank $START_RANK \
#     --kv_transfer_config '{"kv_connector":"NixlConnector","kv_role":"kv_both","kv_load_failure_policy":"fail"}' \
#     --async-scheduling \
#     --enable-dbo \
#     --dbo-decode-token-threshold 32 \
#     --enable-eplb \
#     --eplb-config '{"window_size":"1000","step_interval":"3000","num_redundant_experts":"32","log_balancedness":"False"}' \
#     --compilation_config '{"cudagraph_mode": "FULL_DECODE_ONLY"}' \
#     --kv-cache-memory-bytes=${KV_CACHE_MEMORY_BYTES-}
#
# NOTE: Decode-specific flags. LWS-specific args (--data-parallel-address, etc.)
# are automatically set by the modelservice Helm chart when multinode=true.
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS
REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS; \
vllm serve /model-cache/models/REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--host 0.0.0.0 \
--served-model-name REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--port REPLACE_ENV_LLMDBENCH_VLLM_COMMON_METRICS_PORT \
--trust-remote-code \
--disable-uvicorn-access-log \
--data-parallel-hybrid-lb \
--enable-expert-parallel \
--tensor-parallel-size REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM \
--kv_transfer_config '{"kv_connector":"NixlConnector","kv_role":"kv_both","kv_load_failure_policy":"fail"}' \
--async-scheduling \
--enable-dbo \
--dbo-decode-token-threshold 32 \
--enable-eplb \
--eplb-config '{"window_size":"1000","step_interval":"3000","num_redundant_experts":"32","log_balancedness":"False"}' \
--compilation_config '{"cudagraph_mode": "FULL_DECODE_ONLY"}'
EOF

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/prefill.yaml
# Lines 57-84 (vLLM args):
#   exec vllm serve \
#     deepseek-ai/DeepSeek-R1-0528 \
#     --port 8000 \
#     --trust-remote-code \
#     --disable-uvicorn-access-log \
#     --data-parallel-hybrid-lb \
#     --enable-expert-parallel \
#     --tensor-parallel-size $TP_SIZE \
#     --data-parallel-size $((LWS_GROUP_SIZE * DP_SIZE_LOCAL)) \
#     --data-parallel-size-local $DP_SIZE_LOCAL \
#     --data-parallel-address ${LWS_LEADER_ADDRESS} \
#     --data-parallel-rpc-port 5555 \
#     --data-parallel-start-rank $START_RANK \
#     --kv_transfer_config '{"kv_connector":"NixlConnector","kv_role":"kv_both","kv_load_failure_policy":"fail"}' \
#     --async-scheduling \
#     --enable-dbo \
#     --dbo-prefill-token-threshold 32 \
#     --enable-eplb \
#     --eplb-config '{"window_size":"1000","step_interval":"3000","num_redundant_experts":"32","log_balancedness":"False"}' \
#     --gpu-memory-utilization 0.75
#
# NOTE: Prefill-specific flags. Uses --dbo-prefill-token-threshold and explicit
# --gpu-memory-utilization 0.75 (vs decode which uses --kv-cache-memory-bytes).
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_ARGS=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_ARGS
REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_PREFILL_PREPROCESS; \
vllm serve /model-cache/models/REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--host 0.0.0.0 \
--served-model-name REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--port REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_PREFILL_INFERENCE_PORT \
--trust-remote-code \
--disable-uvicorn-access-log \
--data-parallel-hybrid-lb \
--enable-expert-parallel \
--tensor-parallel-size REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_PREFILL_TENSOR_PARALLELISM \
--kv_transfer_config '{"kv_connector":"NixlConnector","kv_role":"kv_both","kv_load_failure_policy":"fail"}' \
--async-scheduling \
--enable-dbo \
--dbo-prefill-token-threshold 32 \
--enable-eplb \
--eplb-config '{"window_size":"1000","step_interval":"3000","num_redundant_experts":"32","log_balancedness":"False"}' \
--gpu-memory-utilization 0.75
EOF

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Lines 117-175 (environment variables):
#   - name: VLLM_MOE_DP_CHUNK_SIZE
#     value: "384"
#   - name: DP_SIZE_LOCAL
#     value: "8"
#   ... (21 more environment variables)
#
# PLUS: guides/wide-ep-lws/manifests/modelserver/gke/kustomization.yaml
# Lines 85-104 (GKE-specific environment variables):
#   - name: DEEP_EP_DEVICE_TO_HCA_MAPPING
#     value: "0:mlx5_0:1,1:mlx5_1:1,..."
#   - name: BASH_ENV
#     value: "/usr/local/gib/scripts/set_nccl_env.sh"
#   - name: NVSHMEM_DISABLED_GDRCOPY
#     value: "true"
#
# PLUS: guides/wide-ep-lws/manifests/modelserver/gke-a4/kustomization.yaml
# Lines 7-11 (GKE-A4 specific):
#   - name: KV_CACHE_MEMORY_BYTES
#     value: "63000000000"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_TO_YAML=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_TO_YAML
- name: VLLM_MOE_DP_CHUNK_SIZE
  value: "384"
- name: DP_SIZE_LOCAL
  value: "8"
- name: TP_SIZE
  value: "1"
- name: TRITON_LIBCUDA_PATH
  value: /usr/lib64
- name: VLLM_SKIP_P2P_CHECK
  value: "1"
- name: VLLM_RANDOMIZE_DP_DUMMY_INPUTS
  value: "1"
- name: VLLM_USE_DEEP_GEMM
  value: "1"
- name: VLLM_ALL2ALL_BACKEND
  value: deepep_low_latency
- name: NVIDIA_GDRCOPY
  value: enabled
- name: NVSHMEM_REMOTE_TRANSPORT
  value: ibgda
- name: NVSHMEM_IB_ENABLE_IBGDA
  value: "true"
- name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
  value: eth0
- name: GLOO_SOCKET_IFNAME
  value: eth0
- name: NCCL_SOCKET_IFNAME
  value: eth0
- name: VLLM_LOGGING_LEVEL
  value: INFO
- name: VLLM_NIXL_SIDE_CHANNEL_HOST
  valueFrom:
    fieldRef:
      fieldPath: status.podIP
- name: CUDA_CACHE_PATH
  value: /var/cache/vllm/cuda
- name: CCACHE_DIR
  value: /var/cache/vllm/ccache
- name: VLLM_CACHE_ROOT
  value: /var/cache/vllm/vllm
- name: FLASHINFER_WORKSPACE_BASE
  value: /var/cache/vllm/flashinfer
- name: HF_HUB_CACHE
  value: /var/cache/huggingface
- name: DEEP_EP_DEVICE_TO_HCA_MAPPING
  value: "0:mlx5_0:1,1:mlx5_1:1,2:mlx5_2:1,3:mlx5_3:1,4:mlx5_4:1,5:mlx5_5:1,6:mlx5_6:1,7:mlx5_7:1"
- name: BASH_ENV
  value: "/usr/local/gib/scripts/set_nccl_env.sh"
- name: NVSHMEM_DISABLED_GDRCOPY
  value: "true"
- name: KV_CACHE_MEMORY_BYTES
  value: "63000000000"
EOF

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/prefill.yaml
# Lines 91-147 (environment variables):
#   Similar to decode but with different VLLM_ALL2ALL_BACKEND value
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_ENVVARS_TO_YAML=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_PREFILL_ENVVARS_TO_YAML
- name: DP_SIZE_LOCAL
  value: "8"
- name: TP_SIZE
  value: "1"
- name: TRITON_LIBCUDA_PATH
  value: /usr/lib64
- name: VLLM_SKIP_P2P_CHECK
  value: "1"
- name: VLLM_RANDOMIZE_DP_DUMMY_INPUTS
  value: "1"
- name: VLLM_USE_DEEP_GEMM
  value: "1"
- name: VLLM_ALL2ALL_BACKEND
  value: deepep_high_throughput
- name: NVIDIA_GDRCOPY
  value: enabled
- name: NVSHMEM_REMOTE_TRANSPORT
  value: ibgda
- name: NVSHMEM_IB_ENABLE_IBGDA
  value: "true"
- name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
  value: eth0
- name: GLOO_SOCKET_IFNAME
  value: eth0
- name: NCCL_SOCKET_IFNAME
  value: eth0
- name: VLLM_LOGGING_LEVEL
  value: INFO
- name: VLLM_NIXL_SIDE_CHANNEL_HOST
  valueFrom:
    fieldRef:
      fieldPath: status.podIP
- name: CUDA_CACHE_PATH
  value: /var/cache/vllm/cuda
- name: CCACHE_DIR
  value: /var/cache/vllm/ccache
- name: VLLM_CACHE_ROOT
  value: /var/cache/vllm/vllm
- name: FLASHINFER_WORKSPACE_BASE
  value: /var/cache/vllm/flashinfer
- name: HF_HUB_CACHE
  value: /var/cache/huggingface
- name: DEEP_EP_DEVICE_TO_HCA_MAPPING
  value: "0:mlx5_0:1,1:mlx5_1:1,2:mlx5_2:1,3:mlx5_3:1,4:mlx5_4:1,5:mlx5_5:1,6:mlx5_6:1,7:mlx5_7:1"
- name: BASH_ENV
  value: "/usr/local/gib/scripts/set_nccl_env.sh"
- name: NVSHMEM_DISABLED_GDRCOPY
  value: "true"
EOF

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Lines 51-58 (volumes):
#   - name: dshm
#   - name: hf-cache
#   - name: jit-cache
#
# PLUS: guides/wide-ep-lws/manifests/modelserver/gke/kustomization.yaml
# Lines 75-79 (GIB volume):
#   - name: gib
#     hostPath:
#       path: /home/kubernetes/bin/gib
#
# Lines 107-116 (replace hf-cache and jit-cache with GKE paths):
#   - name: hf-cache
#     hostPath:
#       path: /mnt/stateful_partition/kube-ephemeral-ssd/shared_disk/vllm-hf-cache/
#   - name: jit-cache
#     hostPath:
#       path: /mnt/stateful_partition/kube-ephemeral-ssd/shared_disk/vllm-jit-cache/
#
# PLUS: Benchmark framework standard volumes (preprocesses)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES=$(mktemp)
cat << 'EOF' > ${LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES}
- name: preprocesses
  configMap:
    defaultMode: 0755
    name: llm-d-benchmark-preprocesses
- name: dshm
  emptyDir:
    medium: Memory
    sizeLimit: REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_SHM_MEM
- name: hf-cache
  hostPath:
    path: /mnt/stateful_partition/kube-ephemeral-ssd/shared_disk/vllm-hf-cache/
    type: DirectoryOrCreate
- name: jit-cache
  hostPath:
    path: /mnt/stateful_partition/kube-ephemeral-ssd/shared_disk/vllm-jit-cache/
    type: DirectoryOrCreate
- name: gib
  hostPath:
    path: /home/kubernetes/bin/gib
    type: ""
EOF

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/prefill.yaml
# Lines 22-29 (volumes) - same as decode
#
# PLUS: GKE patches from kustomization.yaml (same as decode)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUMES=$(mktemp)
cat << 'EOF' > ${LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUMES}
- name: preprocesses
  configMap:
    defaultMode: 0755
    name: llm-d-benchmark-preprocesses
- name: dshm
  emptyDir:
    medium: Memory
    sizeLimit: REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_PREFILL_SHM_MEM
- name: hf-cache
  hostPath:
    path: /mnt/stateful_partition/kube-ephemeral-ssd/shared_disk/vllm-hf-cache/
    type: DirectoryOrCreate
- name: jit-cache
  hostPath:
    path: /mnt/stateful_partition/kube-ephemeral-ssd/shared_disk/vllm-jit-cache/
    type: DirectoryOrCreate
- name: gib
  hostPath:
    path: /home/kubernetes/bin/gib
    type: ""
EOF

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Lines 196-204 (volume mounts):
#   - name: dshm
#     mountPath: /dev/shm
#   - name: hf-cache
#     mountPath: /var/cache/huggingface
#   - name: jit-cache
#     mountPath: /var/cache/vllm
#
# PLUS: guides/wide-ep-lws/manifests/modelserver/gke/kustomization.yaml
# Lines 80-83 (GIB mount):
#   - name: gib
#     mountPath: /usr/local/gib
#
# PLUS: Benchmark framework standard mounts (preprocesses)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS=$(mktemp)
cat << 'EOF' > ${LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS}
- name: dshm
  mountPath: /dev/shm
- name: preprocesses
  mountPath: /setup/preprocess
- name: hf-cache
  mountPath: /var/cache/huggingface
- name: jit-cache
  mountPath: /var/cache/vllm
- name: gib
  mountPath: /usr/local/gib
EOF

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/prefill.yaml
# Lines 171-179 (volume mounts) - same as decode
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUME_MOUNTS=$(mktemp)
cat << 'EOF' > ${LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUME_MOUNTS}
- name: dshm
  mountPath: /dev/shm
- name: preprocesses
  mountPath: /setup/preprocess
- name: hf-cache
  mountPath: /var/cache/huggingface
- name: jit-cache
  mountPath: /var/cache/vllm
- name: gib
  mountPath: /usr/local/gib
EOF

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/gke/kustomization.yaml
# Lines 45-67 (pod annotations for GKE RDMA):
#   networking.gke.io/default-interface: 'eth0'
#   networking.gke.io/interfaces: |
#     [
#       {"interfaceName":"eth0","network":"default"},
#       {"interfaceName":"eth2","network":"rdma-0"},
#       ... (eth3-eth9 for rdma-1 through rdma-7)
#     ]
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_PODANNOTATIONS=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_PODANNOTATIONS
networking.gke.io/default-interface: 'eth0'
networking.gke.io/interfaces: |
  [
    {"interfaceName":"eth0","network":"default"},
    {"interfaceName":"eth2","network":"rdma-0"},
    {"interfaceName":"eth3","network":"rdma-1"},
    {"interfaceName":"eth4","network":"rdma-2"},
    {"interfaceName":"eth5","network":"rdma-3"},
    {"interfaceName":"eth6","network":"rdma-4"},
    {"interfaceName":"eth7","network":"rdma-5"},
    {"interfaceName":"eth8","network":"rdma-6"},
    {"interfaceName":"eth9","network":"rdma-7"}
  ]
EOF

export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_PODANNOTATIONS=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_PREFILL_PODANNOTATIONS
networking.gke.io/default-interface: 'eth0'
networking.gke.io/interfaces: |
  [
    {"interfaceName":"eth0","network":"default"},
    {"interfaceName":"eth2","network":"rdma-0"},
    {"interfaceName":"eth3","network":"rdma-1"},
    {"interfaceName":"eth4","network":"rdma-2"},
    {"interfaceName":"eth5","network":"rdma-3"},
    {"interfaceName":"eth6","network":"rdma-4"},
    {"interfaceName":"eth7","network":"rdma-5"},
    {"interfaceName":"eth8","network":"rdma-6"},
    {"interfaceName":"eth9","network":"rdma-7"}
  ]
EOF

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/gke/kustomization.yaml
# Lines 39-42 (privileged container for GPU-initiated RDMA):
#   - op: add
#     path: /spec/.../securityContext/privileged
#     value: true
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_CONTAINER_CONFIG=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_CONTAINER_CONFIG
securityContext:
  privileged: true
  capabilities:
    add:
      - IPC_LOCK
      - SYS_RAWIO
  runAsGroup: 0
  runAsUser: 0
EOF

export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_CONTAINER_CONFIG=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_CONTAINER_CONFIG
securityContext:
  privileged: true
  capabilities:
    add:
      - IPC_LOCK
      - SYS_RAWIO
  runAsGroup: 0
  runAsUser: 0
EOF

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/inferencepool.values.yaml
# Lines 14-16:
#   pluginsConfigFile: "custom-plugins.yaml"
#   pluginsCustomConfig:
#     custom-plugins.yaml: |
#       ... (custom plugin YAML content)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_PLUGINS_CONFIGFILE="custom-plugins.yaml"

export LLMDBENCH_VLLM_MODELSERVICE_GAIE_CUSTOM_PLUGINS=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_GAIE_CUSTOM_PLUGINS
custom-plugins.yaml: |
  # ALWAYS DO PD IN THIS EXAMPLE (THRESHOLD 0)
  # This example uses random routing
  # since it's not yet possible to route to individual DP ranks
  apiVersion: inference.networking.x-k8s.io/v1alpha1
  kind: EndpointPickerConfig
  plugins:
  - type: prefill-header-handler
  - type: prefill-filter
  - type: decode-filter
  - type: random-picker
    parameters:
      maxNumOfEndpoints: 1
  - type: pd-profile-handler
    parameters:
      threshold: 0
      hashBlockSize: 5
  schedulingProfiles:
  - name: prefill
    plugins:
    - pluginRef: prefill-filter
    - pluginRef: random-picker
  - name: decode
    plugins:
    - pluginRef: decode-filter
    - pluginRef: random-picker
EOF

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/inferencepool.values.yaml
# Lines 2-4:
#   flags:
#     kv-cache-usage-percentage-metric: "vllm:kv_cache_usage_perc"
#     v: 1
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_FLAGS=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_GAIE_FLAGS
kv-cache-usage-percentage-metric: "vllm:kv_cache_usage_perc"
v: 1
EOF

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/inferencepool.values.yaml
# Lines 47-49:
#   provider:
#     name: istio
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GATEWAY_CLASS_NAME=istio

# =============================================================================
# SOURCE: Benchmark framework defaults
# =============================================================================
export LLMDBENCH_HARNESS_NAME=inference-perf
export LLMDBENCH_HARNESS_EXPERIMENT_PROFILE=sanity_random.yaml
export LLMDBENCH_CONTROL_WORK_DIR=~/data/wide-ep-lws-gke-a4
