# WIDE-EP-LWS (GKE-A4) WELL LIT PATH
# Based on https://github.com/llm-d/llm-d/tree/main/guides/wide-ep-lws
# Variant: gke-a4 (B200 GPUs)
# Generated by llm-d-benchmark guide converter
#
# ⚠️  IMPORTANT NOTE ⚠️
# This guide uses LeaderWorkerSet for deployment, which is not directly supported by
# llm-d-benchmark's standard modelservice deployment method. This scenario file captures
# the configuration values for reference, but you may need to deploy this guide manually
# using kubectl/kustomize:
#   kubectl apply -k https://github.com/llm-d/llm-d/tree/main/guides/wide-ep-lws/manifests/modelserver/gke-a4
#
# Then use llm-d-benchmark's run.sh to benchmark the deployed service.
#
# Kustomize Hierarchy:
# manifests/modelserver/gke-a4/kustomization.yaml
#     └── ../gke/kustomization.yaml
#             └── ../base/kustomization.yaml
#                     └── decode.yaml, prefill.yaml, serviceAccount.yaml

# =============================================================================
# SOURCE: manifests/modelserver/base/decode.yaml
# Line 10:
#   llm-d.ai/model: DeepSeek-R1-0528
# =============================================================================
export LLMDBENCH_DEPLOY_MODEL_LIST="deepseek-ai/DeepSeek-R1-0528"

# =============================================================================
# SOURCE: manifests/modelserver/base/decode.yaml
# Line 58:
#   image: ghcr.io/llm-d/llm-d-cuda:v0.5.0
# =============================================================================
export LLMDBENCH_LLMD_IMAGE_TAG="v0.5.0"

# =============================================================================
# SOURCE: manifests/inferencepool.values.yaml
# Lines 8-11:
#   image:
#     name: llm-d-inference-scheduler
#     hub: ghcr.io/llm-d
#     tag: v0.5.0
# =============================================================================
export LLMDBENCH_LLMD_INFERENCESCHEDULER_IMAGE_TAG="v0.5.0"

# =============================================================================
# SOURCE: manifests/modelserver/base/decode.yaml
# Line 36:
#   image: ghcr.io/llm-d/llm-d-routing-sidecar:v0.5.0
# =============================================================================
export LLMDBENCH_LLMD_ROUTINGSIDECAR_IMAGE_TAG="v0.5.0"

# =============================================================================
# SOURCE: manifests/modelserver/base/decode.yaml
# Lines 200-204:
#   resources:
#     limits:
#       ephemeral-storage: 1Ti
#       memory: 512Gi
#       nvidia.com/gpu: "8"
# =============================================================================
export LLMDBENCH_VLLM_COMMON_CPU_MEM="512Gi"
export LLMDBENCH_VLLM_COMMON_EPHEMERAL_STORAGE="1Ti"

# =============================================================================
# SOURCE: manifests/modelserver/base/decode.yaml
# Line 13: replicas: 1
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_REPLICAS=1

# =============================================================================
# SOURCE: manifests/modelserver/base/decode.yaml
# Lines 93-94:
#   --tensor-parallel-size $TP_SIZE
#   --data-parallel-size $((LWS_GROUP_SIZE * DP_SIZE_LOCAL))
# PLUS Lines 126-127:
#   - name: TP_SIZE
#     value: "1"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM=1

# =============================================================================
# SOURCE: manifests/modelserver/base/decode.yaml
# Lines 95:
#   --data-parallel-size-local $DP_SIZE_LOCAL
# PLUS Lines 124-125:
#   - name: DP_SIZE_LOCAL
#     value: "8"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_DATA_LOCAL_PARALLELISM=8

# =============================================================================
# SOURCE: Benchmark framework convention (not in guide)
# Using custom command to inject preprocessing and exact vLLM args
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_MODEL_COMMAND=custom
export LLMDBENCH_VLLM_COMMON_PREPROCESS="python3 /setup/preprocess/set_llmdbench_environment.py; source \$HOME/llmdbench_env.sh"
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS=$LLMDBENCH_VLLM_COMMON_PREPROCESS

# =============================================================================
# SOURCE: manifests/modelserver/base/decode.yaml
# Lines 86-111 (vLLM command and arguments)
#   exec vllm serve \
#     deepseek-ai/DeepSeek-R1-0528 \
#     --port 8200 \
#     --trust-remote-code \
#     --disable-uvicorn-access-log \
#     --data-parallel-hybrid-lb \
#     --enable-expert-parallel \
#     --tensor-parallel-size $TP_SIZE \
#     --data-parallel-size $((LWS_GROUP_SIZE * DP_SIZE_LOCAL)) \
#     --data-parallel-size-local $DP_SIZE_LOCAL \
#     --data-parallel-address ${LWS_LEADER_ADDRESS} \
#     --data-parallel-rpc-port 5555 \
#     --data-parallel-start-rank $START_RANK \
#     --kv_transfer_config '{"kv_connector":"NixlConnector",...}' \
#     --async-scheduling \
#     --enable-dbo \
#     --dbo-decode-token-threshold 32 \
#     --enable-eplb \
#     --eplb-config '{"window_size":"1000",...}' \
#     --compilation_config '{"cudagraph_mode": "FULL_DECODE_ONLY"}' \
#     --kv-cache-memory-bytes=${KV_CACHE_MEMORY_BYTES-}
#
# NOTE: Port mapping - Guide uses port 8200 for vLLM, benchmark uses proxy pattern:
#   - INFERENCE_PORT (8000): proxy/sidecar listens here
#   - METRICS_PORT (8200): vLLM actually listens here
# NOTE: LeaderWorkerSet specific args like --data-parallel-address, --data-parallel-start-rank
#   are not directly supported in llm-d-benchmark's standard modelservice deployment
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS
REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS; \
vllm serve /model-cache/models/REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--host 0.0.0.0 \
--served-model-name REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--port REPLACE_ENV_LLMDBENCH_VLLM_COMMON_METRICS_PORT \
--trust-remote-code \
--disable-uvicorn-access-log \
--data-parallel-hybrid-lb \
--enable-expert-parallel \
--tensor-parallel-size REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM \
--kv_transfer_config '{"kv_connector":"NixlConnector","kv_role":"kv_both","kv_load_failure_policy":"fail"}' \
--async-scheduling \
--enable-dbo \
--dbo-decode-token-threshold 32 \
--enable-eplb \
--eplb-config '{"window_size":"1000","step_interval":"3000","num_redundant_experts":"32","log_balancedness":"False"}' \
--compilation_config '{"cudagraph_mode": "FULL_DECODE_ONLY"}'
EOF

# =============================================================================
# SOURCE: manifests/modelserver/base/decode.yaml
# Lines 112-169 (environment variables)
#   - name: VLLM_MOE_DP_CHUNK_SIZE
#     value: "384"
#   - name: DP_SIZE_LOCAL
#     value: "8"
#   - name: TP_SIZE
#     value: "1"
#   [... and many more env vars ...]
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_TO_YAML=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_TO_YAML
- name: VLLM_MOE_DP_CHUNK_SIZE
  value: "384"
- name: DP_SIZE_LOCAL
  value: "8"
- name: TP_SIZE
  value: "1"
- name: TRITON_LIBCUDA_PATH
  value: /usr/lib64
- name: VLLM_SKIP_P2P_CHECK
  value: "1"
- name: VLLM_RANDOMIZE_DP_DUMMY_INPUTS
  value: "1"
- name: VLLM_USE_DEEP_GEMM
  value: "1"
- name: VLLM_ALL2ALL_BACKEND
  value: deepep_low_latency
- name: NVIDIA_GDRCOPY
  value: enabled
- name: NVSHMEM_REMOTE_TRANSPORT
  value: ibgda
- name: NVSHMEM_IB_ENABLE_IBGDA
  value: "true"
- name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
  value: eth0
- name: GLOO_SOCKET_IFNAME
  value: eth0
- name: NCCL_SOCKET_IFNAME
  value: eth0
- name: VLLM_LOGGING_LEVEL
  value: INFO
- name: CUDA_CACHE_PATH
  value: /var/cache/vllm/cuda
- name: CCACHE_DIR
  value: /var/cache/vllm/ccache
- name: VLLM_CACHE_ROOT
  value: /var/cache/vllm/vllm
- name: FLASHINFER_WORKSPACE_BASE
  value: /var/cache/vllm/flashinfer
- name: HF_HUB_CACHE
  value: /var/cache/huggingface
EOF

# =============================================================================
# SOURCE: manifests/modelserver/base/decode.yaml
# Lines 210-216 (volume mounts):
#   volumeMounts:
#     - name: dshm
#       mountPath: /dev/shm
#     - name: hf-cache
#       mountPath: /var/cache/huggingface
#     - name: jit-cache
#       mountPath: /var/cache/vllm
#
# PLUS manifests/modelserver/gke/kustomization.yaml
# Lines 99-102:
#   - op: add
#     path: /spec/leaderWorkerTemplate/workerTemplate/spec/containers/0/volumeMounts/-
#     value:
#       mountPath: /usr/local/gib
#       name: gib
#
# PLUS Benchmark framework standard volumes (preprocesses, metrics-volume, etc.)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS}
- name: dshm
  mountPath: /dev/shm
- name: preprocesses
  mountPath: /setup/preprocess
- name: hf-cache
  mountPath: /var/cache/huggingface
- name: jit-cache
  mountPath: /var/cache/vllm
- name: gib
  mountPath: /usr/local/gib
EOF

# =============================================================================
# SOURCE: manifests/modelserver/base/decode.yaml
# Lines 47-55 (base volumes):
#   volumes:
#     - name: dshm
#       emptyDir:
#         medium: Memory
#         sizeLimit: 2Gi
#     - name: hf-cache
#       emptyDir: {}
#     - name: jit-cache
#       emptyDir: {}
#
# PLUS manifests/modelserver/gke/kustomization.yaml
# Lines 92-97 (GKE gib volume):
#   - op: add
#     path: /spec/leaderWorkerTemplate/workerTemplate/spec/volumes/-
#     value:
#       name: gib
#       hostPath:
#         path: /home/kubernetes/bin/gib
#         type: ""
#
# PLUS Lines 129-141 (GKE cache volumes):
#   - op: replace
#     path: /spec/leaderWorkerTemplate/workerTemplate/spec/volumes/1
#     value:
#       name: hf-cache
#       hostPath:
#         path: /mnt/stateful_partition/kube-ephemeral-ssd/shared_disk/vllm-hf-cache/
#         type: DirectoryOrCreate
#   - op: replace
#     path: /spec/leaderWorkerTemplate/workerTemplate/spec/volumes/2
#     value:
#       name: jit-cache
#       hostPath:
#         path: /mnt/stateful_partition/kube-ephemeral-ssd/shared_disk/vllm-jit-cache/
#         type: DirectoryOrCreate
#
# PLUS Benchmark framework standard volumes (preprocesses configMap first in list)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES}
- name: preprocesses
  configMap:
    defaultMode: 0755
    name: llm-d-benchmark-preprocesses
- name: dshm
  emptyDir:
    medium: Memory
    sizeLimit: 2Gi
- name: hf-cache
  hostPath:
    path: /mnt/stateful_partition/kube-ephemeral-ssd/shared_disk/vllm-hf-cache/
    type: DirectoryOrCreate
- name: jit-cache
  hostPath:
    path: /mnt/stateful_partition/kube-ephemeral-ssd/shared_disk/vllm-jit-cache/
    type: DirectoryOrCreate
- name: gib
  hostPath:
    path: /home/kubernetes/bin/gib
    type: ""
EOF

# =============================================================================
# SOURCE: manifests/modelserver/gke-a4/kustomization.yaml
# Lines 12-16 (B200-specific KV cache memory limit):
#   - op: add
#     path: /spec/leaderWorkerTemplate/workerTemplate/spec/containers/0/env/-
#     value:
#       name: KV_CACHE_MEMORY_BYTES
#       value: "63000000000"
#
# NOTE: Added to DECODE_ENVVARS_TO_YAML above, and as --kv-cache-memory-bytes flag
# =============================================================================
# (Already included in ENVVARS and EXTRA_ARGS above)

# =============================================================================
# SOURCE: manifests/modelserver/base/prefill.yaml
# Line 13: replicas: 1
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_REPLICAS=1

# =============================================================================
# SOURCE: manifests/modelserver/base/prefill.yaml
# Lines 76-77:
#   --tensor-parallel-size $TP_SIZE
#   --data-parallel-size $((LWS_GROUP_SIZE * DP_SIZE_LOCAL))
# PLUS Lines 98-99:
#   - name: TP_SIZE
#     value: "1"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_TENSOR_PARALLELISM=1

# =============================================================================
# SOURCE: manifests/modelserver/base/prefill.yaml
# Lines 78:
#   --data-parallel-size-local $DP_SIZE_LOCAL
# PLUS Lines 96-97:
#   - name: DP_SIZE_LOCAL
#     value: "8"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_DATA_LOCAL_PARALLELISM=8

# =============================================================================
# SOURCE: manifests/modelserver/base/prefill.yaml
# Line 93:
#   --gpu-memory-utilization 0.75
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_ACCELERATOR_MEM_UTIL=0.75

# =============================================================================
# SOURCE: Benchmark framework convention (not in guide)
# Using custom command to inject preprocessing and exact vLLM args
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_MODEL_COMMAND=custom
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_PREPROCESS=$LLMDBENCH_VLLM_COMMON_PREPROCESS

# =============================================================================
# SOURCE: manifests/modelserver/base/prefill.yaml
# Lines 69-93 (vLLM command and arguments)
#   exec vllm serve \
#     deepseek-ai/DeepSeek-R1-0528 \
#     --port 8000 \
#     --trust-remote-code \
#     --disable-uvicorn-access-log \
#     --data-parallel-hybrid-lb \
#     --enable-expert-parallel \
#     --tensor-parallel-size $TP_SIZE \
#     --data-parallel-size $((LWS_GROUP_SIZE * DP_SIZE_LOCAL)) \
#     --data-parallel-size-local $DP_SIZE_LOCAL \
#     --data-parallel-address ${LWS_LEADER_ADDRESS} \
#     --data-parallel-rpc-port 5555 \
#     --data-parallel-start-rank $START_RANK \
#     --kv_transfer_config '{"kv_connector":"NixlConnector",...}' \
#     --async-scheduling \
#     --enable-dbo \
#     --dbo-prefill-token-threshold 32 \
#     --enable-eplb \
#     --eplb-config '{"window_size":"1000",...}' \
#     --gpu-memory-utilization 0.75
#
# NOTE: Prefill uses port 8000 directly (no sidecar proxy)
# NOTE: LeaderWorkerSet specific args are not directly supported in llm-d-benchmark
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_ARGS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_ARGS
REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_PREFILL_PREPROCESS; \
vllm serve /model-cache/models/REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--host 0.0.0.0 \
--served-model-name REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--port REPLACE_ENV_LLMDBENCH_VLLM_COMMON_INFERENCE_PORT \
--trust-remote-code \
--disable-uvicorn-access-log \
--data-parallel-hybrid-lb \
--enable-expert-parallel \
--tensor-parallel-size REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_PREFILL_TENSOR_PARALLELISM \
--kv_transfer_config '{"kv_connector":"NixlConnector","kv_role":"kv_both","kv_load_failure_policy":"fail"}' \
--async-scheduling \
--enable-dbo \
--dbo-prefill-token-threshold 32 \
--enable-eplb \
--eplb-config '{"window_size":"1000","step_interval":"3000","num_redundant_experts":"32","log_balancedness":"False"}' \
--gpu-memory-utilization REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_PREFILL_ACCELERATOR_MEM_UTIL
EOF

# =============================================================================
# SOURCE: manifests/modelserver/base/prefill.yaml
# Lines 95-141 (environment variables, mostly same as decode but different ALL2ALL_BACKEND)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_ENVVARS_TO_YAML=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_PREFILL_ENVVARS_TO_YAML
- name: DP_SIZE_LOCAL
  value: "8"
- name: TP_SIZE
  value: "1"
- name: TRITON_LIBCUDA_PATH
  value: /usr/lib64
- name: VLLM_SKIP_P2P_CHECK
  value: "1"
- name: VLLM_RANDOMIZE_DP_DUMMY_INPUTS
  value: "1"
- name: VLLM_USE_DEEP_GEMM
  value: "1"
- name: VLLM_ALL2ALL_BACKEND
  value: deepep_high_throughput
- name: NVIDIA_GDRCOPY
  value: enabled
- name: NVSHMEM_REMOTE_TRANSPORT
  value: ibgda
- name: NVSHMEM_IB_ENABLE_IBGDA
  value: "true"
- name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
  value: eth0
- name: GLOO_SOCKET_IFNAME
  value: eth0
- name: NCCL_SOCKET_IFNAME
  value: eth0
- name: VLLM_LOGGING_LEVEL
  value: INFO
- name: CUDA_CACHE_PATH
  value: /var/cache/vllm/cuda
- name: CCACHE_DIR
  value: /var/cache/vllm/ccache
- name: VLLM_CACHE_ROOT
  value: /var/cache/vllm/vllm
- name: FLASHINFER_WORKSPACE_BASE
  value: /var/cache/vllm/flashinfer
- name: HF_HUB_CACHE
  value: /var/cache/huggingface
EOF

# =============================================================================
# SOURCE: manifests/modelserver/base/prefill.yaml
# Lines 182-188 (volume mounts, same as decode but no sidecar volumes)
#
# PLUS manifests/modelserver/gke/kustomization.yaml GKE-specific mounts
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUME_MOUNTS=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUME_MOUNTS}
- name: dshm
  mountPath: /dev/shm
- name: preprocesses
  mountPath: /setup/preprocess
- name: hf-cache
  mountPath: /var/cache/huggingface
- name: jit-cache
  mountPath: /var/cache/vllm
- name: gib
  mountPath: /usr/local/gib
EOF

# =============================================================================
# SOURCE: manifests/modelserver/base/prefill.yaml
# Lines 27-35 (base volumes, same as decode)
#
# PLUS manifests/modelserver/gke/kustomization.yaml GKE-specific volumes
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUMES=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUMES}
- name: preprocesses
  configMap:
    defaultMode: 0755
    name: llm-d-benchmark-preprocesses
- name: dshm
  emptyDir:
    medium: Memory
    sizeLimit: 2Gi
- name: hf-cache
  hostPath:
    path: /mnt/stateful_partition/kube-ephemeral-ssd/shared_disk/vllm-hf-cache/
    type: DirectoryOrCreate
- name: jit-cache
  hostPath:
    path: /mnt/stateful_partition/kube-ephemeral-ssd/shared_disk/vllm-jit-cache/
    type: DirectoryOrCreate
- name: gib
  hostPath:
    path: /home/kubernetes/bin/gib
    type: ""
EOF

# =============================================================================
# SOURCE: manifests/inferencepool.values.yaml
# Line 21:
#   pluginsConfigFile: "custom-plugins.yaml"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_PLUGINS_CONFIGFILE="custom-plugins.yaml"

# =============================================================================
# SOURCE: manifests/inferencepool.values.yaml
# Lines 22-48:
#   pluginsCustomConfig:
#     custom-plugins.yaml: |
#       # ALWAYS DO PD IN THIS EXAMPLE (THRESHOLD 0)
#       # This example uses random routing...
#       apiVersion: inference.networking.x-k8s.io/v1alpha1
#       kind: EndpointPickerConfig
#       plugins:
#       - type: prefill-header-handler
#       - type: prefill-filter
#       - type: decode-filter
#       - type: random-picker
#         parameters:
#           maxNumOfEndpoints: 1
#       - type: pd-profile-handler
#         parameters:
#           threshold: 0
#           hashBlockSize: 5
#       schedulingProfiles:
#       - name: prefill
#         plugins:
#         - pluginRef: prefill-filter
#         - pluginRef: random-picker
#       - name: decode
#         plugins:
#         - pluginRef: decode-filter
#         - pluginRef: random-picker
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_CUSTOM_PLUGINS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_GAIE_CUSTOM_PLUGINS
custom-plugins.yaml: |
  # ALWAYS DO PD IN THIS EXAMPLE (THRESHOLD 0)
  # This example uses random routing
  # since it's not yet possible to route to individual DP ranks
  apiVersion: inference.networking.x-k8s.io/v1alpha1
  kind: EndpointPickerConfig
  plugins:
  - type: prefill-header-handler
  - type: prefill-filter
  - type: decode-filter
  - type: random-picker
    parameters:
      maxNumOfEndpoints: 1
  - type: pd-profile-handler
    parameters:
      threshold: 0
      hashBlockSize: 5
  schedulingProfiles:
  - name: prefill
    plugins:
    - pluginRef: prefill-filter
    - pluginRef: random-picker
  - name: decode
    plugins:
    - pluginRef: decode-filter
    - pluginRef: random-picker
EOF

# =============================================================================
# SOURCE: manifests/inferencepool.values.yaml
# Lines 3-7:
#   flags:
#     kv-cache-usage-percentage-metric: "vllm:kv_cache_usage_perc"
#     v: 1
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_FLAGS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_GAIE_FLAGS
kv-cache-usage-percentage-metric: "vllm:kv_cache_usage_perc"
v: 1
EOF

# =============================================================================
# SOURCE: manifests/inferencepool.values.yaml
# Lines 57-58:
#   provider:
#     name: istio
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GATEWAY_CLASS_NAME="istio"

# =============================================================================
# SOURCE: Benchmark framework defaults
# =============================================================================
export LLMDBENCH_HARNESS_NAME=inference-perf
export LLMDBENCH_HARNESS_EXPERIMENT_PROFILE=sanity_random.yaml
export LLMDBENCH_CONTROL_WORK_DIR=~/data/wide-ep-lws-gke-a4
