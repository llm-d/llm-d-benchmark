#!/bin/bash
# PREFILL/DECODE (P/D) DISAGGREGATION - XPU WELL LIT PATH
# Based on https://github.com/llm-d/llm-d/tree/main/guides/pd-disaggregation
# Variant: xpu (Intel GPU)
# Generated by llm-d-benchmark guide converter

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_xpu.yaml
# Lines 7-10:
#   uri: "hf://Qwen/Qwen3-0.6B"
#   name: "Qwen/Qwen3-0.6B"
# =============================================================================
export LLMDBENCH_DEPLOY_MODEL_LIST="Qwen/Qwen3-0.6B"

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_xpu.yaml
# Line 8:
#   size: 10Gi
# =============================================================================
export LLMDBENCH_VLLM_COMMON_PVC_MODEL_CACHE_SIZE="10Gi"

# =============================================================================
# SOURCE: guides/pd-disaggregation/helmfile.yaml.gotmpl
# Infra release (llm-d-infra chart version v1.3.6)
# =============================================================================
export LLMDBENCH_VLLM_INFRA_CHART_VERSION="v1.3.6"

# =============================================================================
# SOURCE: guides/pd-disaggregation/helmfile.yaml.gotmpl
# Model service release (llm-d-modelservice chart version v0.4.5)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_CHART_VERSION="v0.4.5"

# =============================================================================
# SOURCE: guides/pd-disaggregation/helmfile.yaml.gotmpl
# GAIE release (inferencepool chart version v1.3.0)
# =============================================================================
export LLMDBENCH_VLLM_GAIE_CHART_VERSION="v1.3.0"

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_xpu.yaml
# Lines 3-5:
#   accelerator:
#     type: intel-i915
#     dra: true
# =============================================================================
export LLMDBENCH_VLLM_COMMON_ACCELERATOR_RESOURCE="intel-i915"

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_xpu.yaml
# Lines 19-21:
#   routing:
#     servicePort: 8000
#     proxy:
#       connector: nixlv2
# =============================================================================
export LLMDBENCH_LLMD_ROUTINGSIDECAR_CONNECTOR="nixlv2"
export LLMDBENCH_LLMD_ROUTINGSIDECAR_ENABLED=true

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_xpu.yaml
# Line 22:
#   image: ghcr.io/llm-d/llm-d-routing-sidecar:v0.5.0
#
# NOTE: Extracting version tag from routing sidecar image specification
# =============================================================================
export LLMDBENCH_LLMD_ROUTINGSIDECAR_IMAGE_TAG="v0.5.0"

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_xpu.yaml
# Lines 38-39 (decode) and 115-116 (prefill):
#   containers:
#     - image: ghcr.io/llm-d/llm-d-xpu:v0.5.0
#
# NOTE: Both decode and prefill use the same image version
# =============================================================================
export LLMDBENCH_LLMD_IMAGE_TAG="v0.5.0"

# =============================================================================
# SOURCE: guides/pd-disaggregation/gaie-pd/values.yaml
# Lines 7-9:
#   inferenceExtension:
#     image:
#       tag: v0.5.0
# =============================================================================
export LLMDBENCH_LLMD_INFERENCESCHEDULER_IMAGE_TAG="v0.5.0"

# =============================================================================
# Decode Stage Configuration
# =============================================================================

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_xpu.yaml
# Lines 26-28:
#   decode:
#     parallelism:
#       tensor: 1
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM=1

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_xpu.yaml
# Lines 29-30:
#   create: true
#   replicas: 1
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_REPLICAS=1

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_xpu.yaml
# Lines 72-76:
#   resources:
#     limits:
#       memory: 64Gi
#       cpu: "16"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_MEM="64Gi"
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_NR="16"

# =============================================================================
# SOURCE: Benchmark framework convention (not in guide)
# Using custom command to inject preprocessing and exact vLLM args
# =============================================================================
export LLMDBENCH_VLLM_COMMON_PREPROCESS="python3 /setup/preprocess/set_llmdbench_environment.py; source \$HOME/llmdbench_env.sh"
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_MODEL_COMMAND=custom
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS=$LLMDBENCH_VLLM_COMMON_PREPROCESS

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_xpu.yaml
# Lines 41-49 (decode container args):
#   args:
#     - "--block-size"
#     - "64"
#     - "--kv-transfer-config"
#     - '{"kv_connector":"NixlConnector", "kv_role":"kv_both", "kv_buffer_device":"cpu"}'
#     - "--disable-log-requests"
#     - "--disable-uvicorn-access-log"
#     - "--max-model-len"
#     - "32000"
#
# NOTE: Port mapping - Guide uses port 8200 but benchmark uses proxy pattern:
#   - INFERENCE_PORT (8000): proxy/sidecar listens here
#   - METRICS_PORT (8200): vLLM actually listens here
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS
REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS; \
vllm serve /model-cache/models/REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--host 0.0.0.0 \
--served-model-name REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--port REPLACE_ENV_LLMDBENCH_VLLM_COMMON_METRICS_PORT \
--block-size 64 \
--kv-transfer-config '{"kv_connector":"NixlConnector", "kv_role":"kv_both", "kv_buffer_device":"cpu"}' \
--disable-log-requests \
--disable-uvicorn-access-log \
--max-model-len 32000 \
--tensor-parallel-size REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM
EOF

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_xpu.yaml
# Lines 54-67 (decode container environment variables):
#   env:
#     - name: UCX_TLS
#       value: "tcp"
#     - name: VLLM_NIXL_SIDE_CHANNEL_HOST
#       valueFrom:
#         fieldRef:
#           fieldPath: status.podIP
#     - name: VLLM_NIXL_SIDE_CHANNEL_PORT
#       value: "5600"
#     - name: VLLM_USE_V1
#       value: "1"
#     - name: TORCH_LLM_ALLREDUCE
#       value: "1"
#     - name: VLLM_WORKER_MULTIPROC_METHOD
#       value: "spawn"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_TO_YAML=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_TO_YAML
- name: UCX_TLS
  value: "tcp"
- name: VLLM_NIXL_SIDE_CHANNEL_HOST
  valueFrom:
    fieldRef:
      fieldPath: status.podIP
- name: VLLM_NIXL_SIDE_CHANNEL_PORT
  value: "5600"
- name: VLLM_USE_V1
  value: "1"
- name: TORCH_LLM_ALLREDUCE
  value: "1"
- name: VLLM_WORKER_MULTIPROC_METHOD
  value: "spawn"
EOF

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_xpu.yaml
# Lines 78-83 (decode volume mounts):
#   volumeMounts:
#     - name: metrics-volume
#       mountPath: /.config
#     - name: shm
#       mountPath: /dev/shm
#     - name: torch-compile-cache
#       mountPath: /.cache
#
# PLUS benchmark framework standard mounts (preprocesses)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS
- name: preprocesses
  mountPath: /setup/preprocess
- name: metrics-volume
  mountPath: /.config
- name: shm
  mountPath: /dev/shm
- name: torch-compile-cache
  mountPath: /.cache
EOF

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_xpu.yaml
# Lines 99-108 (decode volumes):
#   volumes:
#     - name: metrics-volume
#       emptyDir: {}
#     - name: shm
#       emptyDir:
#         medium: Memory
#         sizeLimit: "16Gi"
#     - name: torch-compile-cache
#       emptyDir: {}
#
# PLUS benchmark framework standard volumes (preprocesses configMap)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES
- name: preprocesses
  configMap:
    defaultMode: 0755
    name: llm-d-benchmark-preprocesses
- name: metrics-volume
  emptyDir: {}
- name: shm
  emptyDir:
    medium: Memory
    sizeLimit: "16Gi"
- name: torch-compile-cache
  emptyDir: {}
EOF

# =============================================================================
# Prefill Stage Configuration
# =============================================================================

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_xpu.yaml
# Lines 110-112:
#   prefill:
#     parallelism:
#       tensor: 1
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_TENSOR_PARALLELISM=1

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_xpu.yaml
# Lines 113-114:
#   create: true
#   replicas: 3
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_REPLICAS=3

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_xpu.yaml
# Lines 156-160:
#   resources:
#     limits:
#       memory: 64Gi
#       cpu: "8"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_CPU_MEM="64Gi"
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_CPU_NR="8"

# =============================================================================
# SOURCE: Benchmark framework convention (not in guide)
# Using custom command to inject preprocessing and exact vLLM args
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_MODEL_COMMAND=custom
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_PREPROCESS=$LLMDBENCH_VLLM_COMMON_PREPROCESS

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_xpu.yaml
# Lines 125-132 (prefill container args):
#   args:
#     - "--block-size"
#     - "64"
#     - "--kv-transfer-config"
#     - '{"kv_connector":"NixlConnector", "kv_role":"kv_both", "kv_buffer_device":"cpu"}'
#     - "--disable-uvicorn-access-log"
#     - "--max-model-len"
#     - "32000"
#
# NOTE: Prefill uses same KV transfer config as decode for P/D disaggregation
# NOTE: Prefill does NOT include --disable-log-requests (unlike decode)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_ARGS=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_ARGS
REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_PREFILL_PREPROCESS; \
vllm serve /model-cache/models/REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--host 0.0.0.0 \
--served-model-name REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--port REPLACE_ENV_LLMDBENCH_VLLM_COMMON_METRICS_PORT \
--block-size 64 \
--kv-transfer-config '{"kv_connector":"NixlConnector", "kv_role":"kv_both", "kv_buffer_device":"cpu"}' \
--disable-uvicorn-access-log \
--max-model-len 32000 \
--tensor-parallel-size REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_PREFILL_TENSOR_PARALLELISM
EOF

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_xpu.yaml
# Lines 138-151 (prefill container environment variables):
#   env:
#     - name: UCX_TLS
#       value: "tcp"
#     - name: VLLM_NIXL_SIDE_CHANNEL_HOST
#       valueFrom:
#         fieldRef:
#           fieldPath: status.podIP
#     - name: VLLM_NIXL_SIDE_CHANNEL_PORT
#       value: "5600"
#     - name: VLLM_USE_V1
#       value: "1"
#     - name: TORCH_LLM_ALLREDUCE
#       value: "1"
#     - name: VLLM_WORKER_MULTIPROC_METHOD
#       value: "spawn"
#
# NOTE: Prefill uses identical environment variables as decode
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_ENVVARS_TO_YAML=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_PREFILL_ENVVARS_TO_YAML
- name: UCX_TLS
  value: "tcp"
- name: VLLM_NIXL_SIDE_CHANNEL_HOST
  valueFrom:
    fieldRef:
      fieldPath: status.podIP
- name: VLLM_NIXL_SIDE_CHANNEL_PORT
  value: "5600"
- name: VLLM_USE_V1
  value: "1"
- name: TORCH_LLM_ALLREDUCE
  value: "1"
- name: VLLM_WORKER_MULTIPROC_METHOD
  value: "spawn"
EOF

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_xpu.yaml
# Lines 162-167 (prefill volume mounts):
#   volumeMounts:
#     - name: metrics-volume
#       mountPath: /.config
#     - name: shm
#       mountPath: /dev/shm
#     - name: torch-compile-cache
#       mountPath: /.cache
#
# PLUS benchmark framework standard mounts (preprocesses)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUME_MOUNTS=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUME_MOUNTS
- name: preprocesses
  mountPath: /setup/preprocess
- name: metrics-volume
  mountPath: /.config
- name: shm
  mountPath: /dev/shm
- name: torch-compile-cache
  mountPath: /.cache
EOF

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_xpu.yaml
# Lines 183-192 (prefill volumes):
#   volumes:
#     - name: metrics-volume
#       emptyDir: {}
#     - name: shm
#       emptyDir:
#         medium: Memory
#         sizeLimit: "16Gi"
#     - name: torch-compile-cache
#       emptyDir: {}
#
# PLUS benchmark framework standard volumes (preprocesses configMap)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUMES=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUMES
- name: preprocesses
  configMap:
    defaultMode: 0755
    name: llm-d-benchmark-preprocesses
- name: metrics-volume
  emptyDir: {}
- name: shm
  emptyDir:
    medium: Memory
    sizeLimit: "16Gi"
- name: torch-compile-cache
  emptyDir: {}
EOF

# =============================================================================
# GAIE (Gateway API Inference Extension) Configuration
# =============================================================================

# =============================================================================
# SOURCE: guides/pd-disaggregation/gaie-pd/values.yaml
# Lines 3-5:
#   inferenceExtension:
#     flags:
#       kv-cache-usage-percentage-metric: "vllm:kv_cache_usage_perc"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_FLAGS=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_GAIE_FLAGS
kv-cache-usage-percentage-metric: "vllm:kv_cache_usage_perc"
EOF

# =============================================================================
# SOURCE: guides/pd-disaggregation/gaie-pd/values.yaml
# Line 12:
#   pluginsConfigFile: "pd-config.yaml"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_PLUGINS_CONFIGFILE="pd-config.yaml"

# =============================================================================
# SOURCE: guides/pd-disaggregation/gaie-pd/values.yaml
# Lines 13-43:
#   pluginsCustomConfig:
#     pd-config.yaml: |
#       <full YAML content for P/D routing configuration>
#
# NOTE: This custom plugin configuration defines the prefill/decode routing
#       logic for disaggregated inference. The "pd-profile-handler" with
#       threshold: 0 forces ALL requests through P/D path.
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_CUSTOM_PLUGINS=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_GAIE_CUSTOM_PLUGINS
pd-config.yaml: |
  # ALWAYS DO PD IN THIS EXAMPLE (THRESHOLD 0)
  apiVersion: inference.networking.x-k8s.io/v1alpha1
  kind: EndpointPickerConfig
  plugins:
  - type: prefill-header-handler
  - type: prefill-filter
  - type: decode-filter
  - type: queue-scorer
  - type: pd-profile-handler
    parameters:
      threshold: 0
      hashBlockSize: 5
  schedulingProfiles:
  - name: prefill
    plugins:
    - pluginRef: prefill-filter
    - pluginRef: queue-scorer
      weight: 1.0
  - name: decode
    plugins:
    - pluginRef: decode-filter
    - pluginRef: queue-scorer
      weight: 1.0
  # All profiles using max score picker by default
EOF

# =============================================================================
# Benchmark Framework Defaults
# =============================================================================
export LLMDBENCH_HARNESS_NAME=inference-perf
export LLMDBENCH_HARNESS_EXPERIMENT_PROFILE=sanity_random.yaml
export LLMDBENCH_CONTROL_WORK_DIR=~/data/pd-disaggregation-xpu
