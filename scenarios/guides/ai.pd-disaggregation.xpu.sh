# PD-DISAGGREGATION XPU WELL LIT PATH
# Based on https://github.com/llm-d/llm-d/tree/main/guides/pd-disaggregation
# Variant: Intel XPU (Data Center GPU Max 1550 / Battlemage)
# Generated by llm-d-benchmark guide converter

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_xpu.yaml
# Lines 8-10:
#   modelArtifacts:
#     uri: "hf://Qwen/Qwen3-0.6B"
#     size: 10Gi
# =============================================================================
export LLMDBENCH_DEPLOY_MODEL_LIST="Qwen/Qwen3-0.6B"

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_xpu.yaml
# Line 10:
#   size: 10Gi
# =============================================================================
export LLMDBENCH_VLLM_COMMON_PVC_MODEL_CACHE_SIZE="10Gi"

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values.yaml
# Lines 35, 118:
#   decode.containers[0].image: ghcr.io/llm-d/llm-d-cuda:v0.5.0
#   prefill.containers[0].image: ghcr.io/llm-d/llm-d-cuda:v0.5.0
# =============================================================================
export LLMDBENCH_LLMD_IMAGE_TAG="v0.5.0"

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_xpu.yaml
# Lines 1-3:
#   accelerator:
#     type: intel-i915
#     dra: true
# =============================================================================
export LLMDBENCH_VLLM_COMMON_ACCELERATOR_RESOURCE="intel-i915"

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values.yaml
# Lines 38-39, 121-122:
#   - "--block-size"
#   - "128"
# NOTE: XPU values_xpu.yaml overrides this to 64
# =============================================================================
export LLMDBENCH_VLLM_COMMON_BLOCK_SIZE=64

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values.yaml
# Lines 43-44, 126-127:
#   - "--max-model-len"
#   - "32000"
# =============================================================================
export LLMDBENCH_VLLM_COMMON_MAX_MODEL_LEN=32000

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values.yaml
# Lines 58-60, 141-143:
#   resources:
#     limits:
#       memory: 64Gi
# =============================================================================
export LLMDBENCH_VLLM_COMMON_CPU_MEM="64Gi"

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values.yaml
# Lines 99-101, 183-185:
#   shm:
#     emptyDir:
#       sizeLimit: "16Gi"
# =============================================================================
export LLMDBENCH_VLLM_COMMON_SHM_MEM="16Gi"

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values.yaml
# Lines 22-26:
#   decode:
#     parallelism:
#       tensor: 4
#     create: true
#     replicas: 1
# NOTE: XPU variant uses tensor: 1 (from README.xpu.md)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_REPLICAS=1
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM=1

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values.yaml
# Lines 59, 63:
#   cpu: "16"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_NR=16

# =============================================================================
# SOURCE: Benchmark framework convention (not in guide)
# Using custom command to inject preprocessing and exact vLLM args
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_MODEL_COMMAND=custom
export LLMDBENCH_VLLM_COMMON_PREPROCESS="python3 /setup/preprocess/set_llmdbench_environment.py; source \$HOME/llmdbench_env.sh"
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS=$LLMDBENCH_VLLM_COMMON_PREPROCESS

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values.yaml
# Lines 37-44 (decode vLLM arguments):
#   args:
#     - "--block-size"
#     - "128"
#     - "--kv-transfer-config"
#     - '{"kv_connector":"NixlConnector", "kv_role":"kv_both"}'
#     - "--disable-uvicorn-access-log"
#     - "--max-model-len"
#     - "32000"
#
# NOTE: Port mapping - Guide uses port 8200 for decode, but benchmark uses proxy pattern:
#   - INFERENCE_PORT (8000): proxy/sidecar listens here
#   - METRICS_PORT (8200): vLLM actually listens here
# NOTE: XPU variant uses block-size 64 and kv_role "kv_consumer" for decode
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS
REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS; \
vllm serve /model-cache/models/REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--host 0.0.0.0 \
--served-model-name REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--port REPLACE_ENV_LLMDBENCH_VLLM_COMMON_METRICS_PORT \
--block-size REPLACE_ENV_LLMDBENCH_VLLM_COMMON_BLOCK_SIZE \
--kv-transfer-config '{"kv_connector":"NixlConnector","kv_role":"kv_consumer"}' \
--disable-uvicorn-access-log \
--max-model-len REPLACE_ENV_LLMDBENCH_VLLM_COMMON_MAX_MODEL_LEN \
--tensor-parallel-size REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM
EOF

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_xpu.yaml
# Lines 74-79 (decode environment variables):
#   env:
#     - name: VLLM_USE_V1
#       value: "1"
#     - name: TORCH_LLM_ALLREDUCE
#       value: "1"
#     - name: VLLM_WORKER_MULTIPROC_METHOD
#       value: "spawn"
#
# PLUS lines 46-49 (VLLM_NIXL_SIDE_CHANNEL_HOST from base values.yaml):
#   - name: VLLM_NIXL_SIDE_CHANNEL_HOST
#     valueFrom:
#       fieldRef:
#         fieldPath: status.podIP
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_TO_YAML=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_TO_YAML
- name: VLLM_USE_V1
  value: "1"
- name: TORCH_LLM_ALLREDUCE
  value: "1"
- name: VLLM_WORKER_MULTIPROC_METHOD
  value: "spawn"
- name: VLLM_NIXL_SIDE_CHANNEL_HOST
  valueFrom:
    fieldRef:
      fieldPath: status.podIP
EOF

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values.yaml
# Lines 66-72 (decode volume mounts):
#   volumeMounts:
#     - name: metrics-volume
#       mountPath: /.config
#     - name: shm
#       mountPath: /dev/shm
#     - name: torch-compile-cache
#       mountPath: /.cache
#
# PLUS benchmark framework standard volume mount (preprocesses)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS}
- name: preprocesses
  mountPath: /setup/preprocess
- name: metrics-volume
  mountPath: /.config
- name: shm
  mountPath: /dev/shm
- name: torch-compile-cache
  mountPath: /.cache
EOF

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values.yaml
# Lines 95-103 (decode volumes):
#   volumes:
#     - name: metrics-volume
#       emptyDir: {}
#     - name: shm
#       emptyDir:
#         medium: Memory
#         sizeLimit: "16Gi"
#     - name: torch-compile-cache
#       emptyDir: {}
#
# PLUS benchmark framework standard volumes (preprocesses ConfigMap - listed first)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES}
- name: preprocesses
  configMap:
    defaultMode: 0755
    name: llm-d-benchmark-preprocesses
- name: metrics-volume
  emptyDir: {}
- name: shm
  emptyDir:
    medium: Memory
    sizeLimit: REPLACE_ENV_LLMDBENCH_VLLM_COMMON_SHM_MEM
- name: torch-compile-cache
  emptyDir: {}
EOF

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values.yaml
# Lines 105-109:
#   prefill:
#     parallelism:
#       tensor: 1
#     create: true
#     replicas: 4
# NOTE: XPU README.xpu.md specifies 3 prefill replicas
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_REPLICAS=3
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_TENSOR_PARALLELISM=1

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values.yaml
# Lines 143, 147:
#   cpu: "8"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_CPU_NR=8

# =============================================================================
# SOURCE: Benchmark framework convention (not in guide)
# Using custom command to inject preprocessing and exact vLLM args
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_MODEL_COMMAND=custom
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_PREPROCESS=$LLMDBENCH_VLLM_COMMON_PREPROCESS

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values.yaml
# Lines 120-127 (prefill vLLM arguments):
#   args:
#     - "--block-size"
#     - "128"
#     - "--kv-transfer-config"
#     - '{"kv_connector":"NixlConnector", "kv_role":"kv_both"}'
#     - "--disable-uvicorn-access-log"
#     - "--max-model-len"
#     - "32000"
#
# NOTE: Prefill uses port 8000 in guide, but benchmark uses METRICS_PORT (8200)
# NOTE: XPU variant uses block-size 64 and kv_role "kv_producer" for prefill
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_ARGS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_ARGS
REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_PREFILL_PREPROCESS; \
vllm serve /model-cache/models/REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--host 0.0.0.0 \
--served-model-name REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--port REPLACE_ENV_LLMDBENCH_VLLM_COMMON_METRICS_PORT \
--block-size REPLACE_ENV_LLMDBENCH_VLLM_COMMON_BLOCK_SIZE \
--kv-transfer-config '{"kv_connector":"NixlConnector","kv_role":"kv_producer"}' \
--disable-uvicorn-access-log \
--max-model-len REPLACE_ENV_LLMDBENCH_VLLM_COMMON_MAX_MODEL_LEN \
--tensor-parallel-size REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_PREFILL_TENSOR_PARALLELISM
EOF

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_xpu.yaml
# Lines 148-153 (prefill environment variables):
#   env:
#     - name: VLLM_USE_V1
#       value: "1"
#     - name: TORCH_LLM_ALLREDUCE
#       value: "1"
#     - name: VLLM_WORKER_MULTIPROC_METHOD
#       value: "spawn"
#
# PLUS lines 129-132 (VLLM_NIXL_SIDE_CHANNEL_HOST from base values.yaml):
#   - name: VLLM_NIXL_SIDE_CHANNEL_HOST
#     valueFrom:
#       fieldRef:
#         fieldPath: status.podIP
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_ENVVARS_TO_YAML=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_PREFILL_ENVVARS_TO_YAML
- name: VLLM_USE_V1
  value: "1"
- name: TORCH_LLM_ALLREDUCE
  value: "1"
- name: VLLM_WORKER_MULTIPROC_METHOD
  value: "spawn"
- name: VLLM_NIXL_SIDE_CHANNEL_HOST
  valueFrom:
    fieldRef:
      fieldPath: status.podIP
EOF

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values.yaml
# Lines 150-156 (prefill volume mounts):
#   volumeMounts:
#     - name: metrics-volume
#       mountPath: /.config
#     - name: shm
#       mountPath: /dev/shm
#     - name: torch-compile-cache
#       mountPath: /.cache
#
# PLUS benchmark framework standard volume mount (preprocesses)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUME_MOUNTS=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUME_MOUNTS}
- name: preprocesses
  mountPath: /setup/preprocess
- name: metrics-volume
  mountPath: /.config
- name: shm
  mountPath: /dev/shm
- name: torch-compile-cache
  mountPath: /.cache
EOF

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values.yaml
# Lines 179-187 (prefill volumes):
#   volumes:
#     - name: metrics-volume
#       emptyDir: {}
#     - name: shm
#       emptyDir:
#         medium: Memory
#         sizeLimit: "16Gi"
#     - name: torch-compile-cache
#       emptyDir: {}
#
# PLUS benchmark framework standard volumes (preprocesses ConfigMap - listed first)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUMES=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUMES}
- name: preprocesses
  configMap:
    defaultMode: 0755
    name: llm-d-benchmark-preprocesses
- name: metrics-volume
  emptyDir: {}
- name: shm
  emptyDir:
    medium: Memory
    sizeLimit: REPLACE_ENV_LLMDBENCH_VLLM_COMMON_SHM_MEM
- name: torch-compile-cache
  emptyDir: {}
EOF

# =============================================================================
# SOURCE: guides/pd-disaggregation/gaie-pd/values.yaml
# Line 11:
#   pluginsConfigFile: "pd-config.yaml"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_PLUGINS_CONFIGFILE="pd-config.yaml"

# =============================================================================
# SOURCE: guides/pd-disaggregation/gaie-pd/values.yaml
# Lines 12-35:
#   pluginsCustomConfig:
#     pd-config.yaml: |
#       apiVersion: inference.networking.x-k8s.io/v1alpha1
#       kind: EndpointPickerConfig
#       plugins:
#       - type: prefill-header-handler
#       - type: prefill-filter
#       - type: decode-filter
#       - type: queue-scorer
#       - type: pd-profile-handler
#         parameters:
#           threshold: 0
#           hashBlockSize: 5
#       schedulingProfiles:
#       - name: prefill
#         plugins:
#         - pluginRef: prefill-filter
#         - pluginRef: queue-scorer
#           weight: 1.0
#       - name: decode
#         plugins:
#         - pluginRef: decode-filter
#         - pluginRef: queue-scorer
#           weight: 1.0
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_CUSTOM_PLUGINS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_GAIE_CUSTOM_PLUGINS
pd-config.yaml: |
  apiVersion: inference.networking.x-k8s.io/v1alpha1
  kind: EndpointPickerConfig
  plugins:
  - type: prefill-header-handler
  - type: prefill-filter
  - type: decode-filter
  - type: queue-scorer
  - type: pd-profile-handler
    parameters:
      threshold: 0
      hashBlockSize: 5
  schedulingProfiles:
  - name: prefill
    plugins:
    - pluginRef: prefill-filter
    - pluginRef: queue-scorer
      weight: 1.0
  - name: decode
    plugins:
    - pluginRef: decode-filter
    - pluginRef: queue-scorer
      weight: 1.0
EOF

# =============================================================================
# SOURCE: guides/pd-disaggregation/gaie-pd/values.yaml
# Lines 3-4:
#   flags:
#     kv-cache-usage-percentage-metric: "vllm:kv_cache_usage_perc"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_FLAGS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_GAIE_FLAGS
kv-cache-usage-percentage-metric: "vllm:kv_cache_usage_perc"
EOF

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values.yaml
# Lines 18-19:
#   routing.proxy.image: ghcr.io/llm-d/llm-d-routing-sidecar:v0.5.0
# =============================================================================
export LLMDBENCH_LLMD_ROUTINGSIDECAR_IMAGE_TAG="v0.5.0"

# =============================================================================
# SOURCE: guides/pd-disaggregation/gaie-pd/values.yaml
# Lines 5-8:
#   image:
#     name: llm-d-inference-scheduler
#     hub: ghcr.io/llm-d
#     tag: v0.5.0
# =============================================================================
export LLMDBENCH_LLMD_INFERENCESCHEDULER_IMAGE_TAG="v0.5.0"

# =============================================================================
# SOURCE: Benchmark framework defaults
# =============================================================================
export LLMDBENCH_HARNESS_NAME=inference-perf
export LLMDBENCH_HARNESS_EXPERIMENT_PROFILE=sanity_random.yaml
export LLMDBENCH_CONTROL_WORK_DIR=~/data/pd-disaggregation-xpu
