#!/usr/bin/env bash
# INFERENCE-SCHEDULING WELL LIT PATH - XPU VARIANT
# Based on https://github.com/llm-d/llm-d/tree/main/guides/inference-scheduling
# Hardware: Intel XPU (Data Center GPU Max 1550 with i915 driver)
# Generated by llm-d-benchmark guide converter

# =============================================================================
# SOURCE: ms-inference-scheduling/values_xpu.yaml
# Lines 1-8:
#   modelArtifacts:
#     name: Qwen/Qwen3-0.6B
#     uri: "hf://Qwen/Qwen3-0.6B"
#     size: 10Gi
#     authSecretName: "llm-d-hf-token"
# =============================================================================
export LLMDBENCH_DEPLOY_MODEL_LIST="Qwen/Qwen3-0.6B"
export LLMDBENCH_VLLM_COMMON_PVC_MODEL_CACHE_SIZE="10Gi"
export LLMDBENCH_VLLM_COMMON_HF_TOKEN_NAME="llm-d-hf-token"

# =============================================================================
# SOURCE: ms-inference-scheduling/values_xpu.yaml
# Lines 15-17:
#   accelerator:
#     type: intel-i915
#     dra: true
#
# NOTE: Intel Data Center GPU Max 1550 using Dynamic Resource Allocation
# =============================================================================
export LLMDBENCH_VLLM_COMMON_ACCELERATOR_RESOURCE="intel-i915"

# =============================================================================
# SOURCE: ms-inference-scheduling/values_xpu.yaml
# Lines 24-25:
#   decode:
#     create: true
#     replicas: 2
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_REPLICAS=2

# =============================================================================
# SOURCE: ms-inference-scheduling/values_xpu.yaml
# Lines 35-36:
#   containers:
#     - name: "vllm"
#       image: ghcr.io/llm-d/llm-d-xpu:v0.5.0
# =============================================================================
export LLMDBENCH_LLMD_IMAGE_TAG="v0.5.0"
export LLMDBENCH_LLMD_IMAGE_NAME="llm-d-xpu"

# =============================================================================
# SOURCE: ms-inference-scheduling/values_xpu.yaml
# Lines 55-61:
#       resources:
#         limits:
#           cpu: "8"
#           memory: 24Gi
#         requests:
#           cpu: "4"
#           memory: 12Gi
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_NR="4"
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_MEM="12Gi"

# =============================================================================
# SOURCE: Benchmark framework convention (not in guide)
# Using custom command to inject preprocessing and exact vLLM args
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_MODEL_COMMAND=custom
export LLMDBENCH_VLLM_COMMON_PREPROCESS="python3 /setup/preprocess/set_llmdbench_environment.py; source \$HOME/llmdbench_env.sh"
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS=$LLMDBENCH_VLLM_COMMON_PREPROCESS

# =============================================================================
# SOURCE: ms-inference-scheduling/values_xpu.yaml
# Lines 37-40:
#       modelCommand: "vllmServe"
#       args:
#         - "--dtype"
#         - "float16"
#         - "--disable-sliding-window"
#
# NOTE: Port mapping - Guide uses vllmServe which defaults to port 8000, but
# benchmark uses proxy pattern:
#   - INFERENCE_PORT (8000): proxy/sidecar listens here
#   - METRICS_PORT (8200): vLLM actually listens here
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS
REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS; \
vllm serve /model-cache/models/REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--host 0.0.0.0 \
--served-model-name REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--port REPLACE_ENV_LLMDBENCH_VLLM_COMMON_METRICS_PORT \
--dtype float16 \
--disable-sliding-window \
--disable-log-requests
EOF

# =============================================================================
# SOURCE: ms-inference-scheduling/values_xpu.yaml
# Lines 41-49:
#       securityContext:
#         fsGroup: 107
#         supplementalGroups:
#           - 107
#       env:
#         - name: VLLM_LOGGING_LEVEL
#           value: DEBUG
#
# PLUS patterns.md XPU environment variables (lines 177-189)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_TO_YAML=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_TO_YAML
- name: VLLM_LOGGING_LEVEL
  value: DEBUG
- name: VLLM_USE_V1
  value: "1"
- name: TORCH_LLM_ALLREDUCE
  value: "1"
- name: VLLM_WORKER_MULTIPROC_METHOD
  value: "spawn"
- name: UCX_TLS
  value: "tcp"
EOF

# =============================================================================
# SOURCE: ms-inference-scheduling/values_xpu.yaml
# Lines 63-68:
#       volumeMounts:
#         - name: metrics-volume
#           mountPath: /.config
#         - name: torch-compile-cache
#           mountPath: /.cache
#
# PLUS benchmark framework standard volumes (preprocesses configMap)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS
- name: preprocesses
  mountPath: /setup/preprocess
- name: metrics-volume
  mountPath: /.config
- name: torch-compile-cache
  mountPath: /.cache
EOF

# =============================================================================
# SOURCE: ms-inference-scheduling/values_xpu.yaml
# Lines 89-92:
#   volumes:
#     - name: metrics-volume
#       emptyDir: {}
#     - name: torch-compile-cache
#       emptyDir: {}
#
# PLUS benchmark framework standard volumes (preprocesses configMap must be first)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES
- name: preprocesses
  configMap:
    defaultMode: 0755
    name: llm-d-benchmark-preprocesses
- name: metrics-volume
  emptyDir: {}
- name: torch-compile-cache
  emptyDir: {}
EOF

# =============================================================================
# SOURCE: ms-inference-scheduling/values_xpu.yaml
# Lines 41-43:
#       securityContext:
#         fsGroup: 107
#         supplementalGroups:
#           - 107
#
# NOTE: Extra pod config for XPU security context
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_POD_CONFIG=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_POD_CONFIG
{
  "securityContext": {
    "fsGroup": 107,
    "supplementalGroups": [107]
  }
}
EOF

# =============================================================================
# SOURCE: ms-inference-scheduling/values_xpu.yaml
# Lines 94-95:
#   prefill:
#     create: false
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_REPLICAS=0

# =============================================================================
# SOURCE: gaie-inference-scheduling/values.yaml
# Lines 1-8:
#   inferenceExtension:
#     replicas: 1
#     flags:
#       kv-cache-usage-percentage-metric: "vllm:kv_cache_usage_perc"
#     image:
#       name: llm-d-inference-scheduler
#       hub: ghcr.io/llm-d
#       tag: v0.5.0
# =============================================================================
export LLMDBENCH_LLMD_INFERENCESCHEDULER_IMAGE_TAG="v0.5.0"
export LLMDBENCH_LLMD_INFERENCESCHEDULER_IMAGE_REGISTRY="ghcr.io"
export LLMDBENCH_LLMD_INFERENCESCHEDULER_IMAGE_REPO="llm-d"
export LLMDBENCH_LLMD_INFERENCESCHEDULER_IMAGE_NAME="llm-d-inference-scheduler"

# =============================================================================
# SOURCE: gaie-inference-scheduling/values.yaml
# Lines 3-4:
#     flags:
#       kv-cache-usage-percentage-metric: "vllm:kv_cache_usage_perc"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_FLAGS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_GAIE_FLAGS
kv-cache-usage-percentage-metric: "vllm:kv_cache_usage_perc"
EOF

# =============================================================================
# SOURCE: gaie-inference-scheduling/values.yaml
# Line 11:
#   pluginsConfigFile: "default-plugins.yaml"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_PLUGINS_CONFIGFILE="default-plugins.yaml"

# =============================================================================
# SOURCE: Benchmark framework defaults
# =============================================================================
export LLMDBENCH_HARNESS_NAME=inference-perf
export LLMDBENCH_HARNESS_EXPERIMENT_PROFILE=sanity_random.yaml
export LLMDBENCH_CONTROL_WORK_DIR=~/data/inference-scheduling-xpu
