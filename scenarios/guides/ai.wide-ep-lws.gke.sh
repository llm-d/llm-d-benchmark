# WIDE EXPERT PARALLELISM (LeaderWorkerSet) WELL LIT PATH - GKE
# Based on https://github.com/llm-d/llm-d/tree/main/guides/wide-ep-lws
# Variant: GKE (Google Kubernetes Engine with RDMA)
# Generated by llm-d-benchmark guide converter
#
# Kustomize Hierarchy:
# guides/wide-ep-lws/manifests/modelserver/gke/kustomization.yaml
#     └── ../base/kustomization.yaml
#             └── decode.yaml, prefill.yaml, serviceAccount.yaml

# =============================================================================
# SOURCE: guides/wide-ep-lws/README.md
# Model: DeepSeek-R1-0528
# Hardware: 32 Nvidia H200 or B200 GPUs with RDMA networking
# =============================================================================
export LLMDBENCH_DEPLOY_MODEL_LIST="deepseek-ai/DeepSeek-R1-0528"

# =============================================================================
# SOURCE: manifests/modelserver/base/decode.yaml
# Lines 66-67:
#   image: ghcr.io/llm-d/llm-d-cuda:v0.5.0
# =============================================================================
export LLMDBENCH_LLMD_IMAGE_TAG="v0.5.0"

# =============================================================================
# SOURCE: manifests/inferencepool.values.yaml
# Lines 9-11:
#   image:
#     name: llm-d-inference-scheduler
#     tag: v0.5.0
# =============================================================================
export LLMDBENCH_LLMD_INFERENCESCHEDULER_IMAGE_TAG="v0.5.0"

# =============================================================================
# SOURCE: manifests/modelserver/base/decode.yaml
# Lines 38-39:
#   image: ghcr.io/llm-d/llm-d-routing-sidecar:v0.5.0
# =============================================================================
export LLMDBENCH_LLMD_ROUTINGSIDECAR_IMAGE_TAG="v0.5.0"

# =============================================================================
# SOURCE: manifests/modelserver/base/decode.yaml
# Line 12: replicas: 1
# Line 14: size: 2
# NOTE: LeaderWorkerSet deployment with 1 replica, 2 pods per replica
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_MULTINODE=true
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_REPLICAS=1
export LLMDBENCH_VLLM_COMMON_NUM_WORKERS_PARALLELISM=2

# =============================================================================
# SOURCE: manifests/modelserver/base/prefill.yaml
# Line 12: replicas: 1
# Line 14: size: 2
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_REPLICAS=1

# =============================================================================
# SOURCE: manifests/modelserver/base/decode.yaml
# Lines 127-130:
#   - name: DP_SIZE_LOCAL
#     value: "8"
#   - name: TP_SIZE
#     value: "1"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_DATA_LOCAL_PARALLELISM=8
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM=1

# =============================================================================
# SOURCE: manifests/modelserver/base/prefill.yaml
# Lines 91-94:
#   - name: DP_SIZE_LOCAL
#     value: "8"
#   - name: TP_SIZE
#     value: "1"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_DATA_LOCAL_PARALLELISM=8
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_TENSOR_PARALLELISM=1

# =============================================================================
# SOURCE: manifests/modelserver/base/decode.yaml
# Lines 205-212:
#   resources:
#     limits:
#       memory: 512Gi
#       nvidia.com/gpu: "8"
#       ephemeral-storage: 1Ti
#     requests:
#       cpu: 32
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_NR=32
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_MEM=512Gi

# =============================================================================
# SOURCE: manifests/modelserver/base/prefill.yaml
# Lines 163-170:
#   resources:
#     limits:
#       memory: 512Gi
#       nvidia.com/gpu: "8"
#       ephemeral-storage: 1Ti
#     requests:
#       cpu: 32
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_CPU_NR=32
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_CPU_MEM=512Gi

# =============================================================================
# SOURCE: manifests/modelserver/base/decode.yaml
# Lines 28-30:
#   - name: dshm
#     emptyDir:
#       sizeLimit: 2Gi
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_SHM_MEM=2Gi

# =============================================================================
# SOURCE: manifests/modelserver/base/prefill.yaml
# Lines 28-30:
#   - name: dshm
#     emptyDir:
#       sizeLimit: 2Gi
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_SHM_MEM=2Gi

# =============================================================================
# SOURCE: manifests/inferencepool.values.yaml
# Lines 14-45:
#   pluginsConfigFile: "custom-plugins.yaml"
#   pluginsCustomConfig:
#     custom-plugins.yaml: |
#       <custom P/D routing configuration>
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_PLUGINS_CONFIGFILE="custom-plugins.yaml"

export LLMDBENCH_VLLM_MODELSERVICE_GAIE_CUSTOM_PLUGINS=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_GAIE_CUSTOM_PLUGINS
custom-plugins.yaml: |
  # ALWAYS DO PD IN THIS EXAMPLE (THRESHOLD 0)
  # This example uses random routing
  # since it's not yet possible to route to individual DP ranks
  apiVersion: inference.networking.x-k8s.io/v1alpha1
  kind: EndpointPickerConfig
  plugins:
  - type: prefill-header-handler
  - type: prefill-filter
  - type: decode-filter
  - type: random-picker
    parameters:
      maxNumOfEndpoints: 1
  - type: pd-profile-handler
    parameters:
      threshold: 0
      hashBlockSize: 5
  schedulingProfiles:
  - name: prefill
    plugins:
    - pluginRef: prefill-filter
    - pluginRef: random-picker
  - name: decode
    plugins:
    - pluginRef: decode-filter
    - pluginRef: random-picker
EOF

# =============================================================================
# SOURCE: manifests/inferencepool.values.yaml
# Lines 3-6:
#   flags:
#     kv-cache-usage-percentage-metric: "vllm:kv_cache_usage_perc"
#     v: 1
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_FLAGS=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_GAIE_FLAGS
kv-cache-usage-percentage-metric: "vllm:kv_cache_usage_perc"
v: 1
EOF

# =============================================================================
# SOURCE: manifests/inferencepool.values.yaml
# Lines 56-58:
#   provider:
#     name: istio
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GATEWAY_CLASS_NAME=istio

# =============================================================================
# SOURCE: Benchmark framework convention (not in guide)
# Using custom command to inject preprocessing and exact vLLM args
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_MODEL_COMMAND=custom
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_MODEL_COMMAND=custom

export LLMDBENCH_VLLM_COMMON_PREPROCESS="python3 /setup/preprocess/set_llmdbench_environment.py; source \$HOME/llmdbench_env.sh"
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS=$LLMDBENCH_VLLM_COMMON_PREPROCESS
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_PREPROCESS=$LLMDBENCH_VLLM_COMMON_PREPROCESS

# =============================================================================
# SOURCE: manifests/modelserver/base/decode.yaml
# Lines 78-112 (vLLM serve command and args):
#   vllm serve deepseek-ai/DeepSeek-R1-0528 \
#     --port 8200 \
#     --trust-remote-code \
#     --disable-uvicorn-access-log \
#     --data-parallel-hybrid-lb \
#     --enable-expert-parallel \
#     --tensor-parallel-size $TP_SIZE \
#     --data-parallel-size $((LWS_GROUP_SIZE * DP_SIZE_LOCAL)) \
#     --data-parallel-size-local $DP_SIZE_LOCAL \
#     --data-parallel-address ${LWS_LEADER_ADDRESS} \
#     --data-parallel-rpc-port 5555 \
#     --data-parallel-start-rank $START_RANK \
#     --kv_transfer_config '{"kv_connector":"NixlConnector",...}' \
#     --async-scheduling \
#     --enable-dbo \
#     --dbo-decode-token-threshold 32 \
#     --enable-eplb \
#     --eplb-config '{"window_size":"1000",...}' \
#     --compilation_config '{"cudagraph_mode": "FULL_DECODE_ONLY"}' \
#     --kv-cache-memory-bytes=${KV_CACHE_MEMORY_BYTES-}
#
# NOTE: Decode stage uses routing-sidecar at port 8000 forwarding to vLLM at 8200
# NOTE: LWS-specific args (--data-parallel-address, etc.) are managed by framework
# =============================================================================
export LLMDBENCH_LLMD_ROUTINGSIDECAR_ENABLED=true
export LLMDBENCH_LLMD_ROUTINGSIDECAR_CONNECTOR="nixlv2"

export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS
REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS; \
vllm serve /model-cache/models/REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--host 0.0.0.0 \
--served-model-name REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--port REPLACE_ENV_LLMDBENCH_VLLM_COMMON_METRICS_PORT \
--trust-remote-code \
--disable-uvicorn-access-log \
--data-parallel-hybrid-lb \
--enable-expert-parallel \
--tensor-parallel-size REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM \
--kv_transfer_config '{"kv_connector":"NixlConnector","kv_role":"kv_both","kv_load_failure_policy":"fail"}' \
--async-scheduling \
--enable-dbo \
--dbo-decode-token-threshold 32 \
--enable-eplb \
--eplb-config '{"window_size":"1000","step_interval":"3000","num_redundant_experts":"32","log_balancedness":"False"}' \
--compilation_config '{"cudagraph_mode": "FULL_DECODE_ONLY"}'
EOF

# =============================================================================
# SOURCE: manifests/modelserver/base/prefill.yaml
# Lines 56-87 (vLLM serve command and args):
#   vllm serve deepseek-ai/DeepSeek-R1-0528 \
#     --port 8000 \
#     --trust-remote-code \
#     --disable-uvicorn-access-log \
#     --data-parallel-hybrid-lb \
#     --enable-expert-parallel \
#     --tensor-parallel-size $TP_SIZE \
#     --data-parallel-size $((LWS_GROUP_SIZE * DP_SIZE_LOCAL)) \
#     --data-parallel-size-local $DP_SIZE_LOCAL \
#     --data-parallel-address ${LWS_LEADER_ADDRESS} \
#     --data-parallel-rpc-port 5555 \
#     --data-parallel-start-rank $START_RANK \
#     --kv_transfer_config '{"kv_connector":"NixlConnector",...}' \
#     --async-scheduling \
#     --enable-dbo \
#     --dbo-prefill-token-threshold 32 \
#     --enable-eplb \
#     --eplb-config '{"window_size":"1000",...}' \
#     --gpu-memory-utilization 0.75
#
# NOTE: Prefill stage uses vLLM directly at port 8000 (no sidecar)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_ARGS=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_ARGS
REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_PREFILL_PREPROCESS; \
vllm serve /model-cache/models/REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--host 0.0.0.0 \
--served-model-name REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--port REPLACE_ENV_LLMDBENCH_VLLM_COMMON_INFERENCE_PORT \
--trust-remote-code \
--disable-uvicorn-access-log \
--data-parallel-hybrid-lb \
--enable-expert-parallel \
--tensor-parallel-size REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_PREFILL_TENSOR_PARALLELISM \
--kv_transfer_config '{"kv_connector":"NixlConnector","kv_role":"kv_both","kv_load_failure_policy":"fail"}' \
--async-scheduling \
--enable-dbo \
--dbo-prefill-token-threshold 32 \
--enable-eplb \
--eplb-config '{"window_size":"1000","step_interval":"3000","num_redundant_experts":"32","log_balancedness":"False"}' \
--gpu-memory-utilization 0.75
EOF

# =============================================================================
# SOURCE: manifests/modelserver/base/decode.yaml
# Lines 115-182 (environment variables):
#   - name: VLLM_MOE_DP_CHUNK_SIZE
#     value: "384"
#   - name: DP_SIZE_LOCAL
#     value: "8"
#   <...many environment variables...>
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_TO_YAML=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_TO_YAML
- name: VLLM_MOE_DP_CHUNK_SIZE
  value: "384"
- name: DP_SIZE_LOCAL
  value: "8"
- name: TP_SIZE
  value: "1"
- name: TRITON_LIBCUDA_PATH
  value: /usr/lib64
- name: VLLM_SKIP_P2P_CHECK
  value: "1"
- name: VLLM_RANDOMIZE_DP_DUMMY_INPUTS
  value: "1"
- name: VLLM_USE_DEEP_GEMM
  value: "1"
- name: VLLM_ALL2ALL_BACKEND
  value: deepep_low_latency
- name: NVIDIA_GDRCOPY
  value: enabled
- name: NVSHMEM_REMOTE_TRANSPORT
  value: ibgda
- name: NVSHMEM_IB_ENABLE_IBGDA
  value: "true"
- name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
  value: eth0
- name: GLOO_SOCKET_IFNAME
  value: eth0
- name: NCCL_SOCKET_IFNAME
  value: eth0
- name: VLLM_LOGGING_LEVEL
  value: INFO
- name: VLLM_NIXL_SIDE_CHANNEL_HOST
  valueFrom:
    fieldRef:
      fieldPath: status.podIP
- name: CUDA_CACHE_PATH
  value: /var/cache/vllm/cuda
- name: CCACHE_DIR
  value: /var/cache/vllm/ccache
- name: VLLM_CACHE_ROOT
  value: /var/cache/vllm/vllm
- name: FLASHINFER_WORKSPACE_BASE
  value: /var/cache/vllm/flashinfer
- name: HF_HUB_CACHE
  value: /var/cache/huggingface
EOF

# =============================================================================
# SOURCE: manifests/modelserver/base/prefill.yaml
# Lines 90-160 (environment variables):
#   - name: DP_SIZE_LOCAL
#     value: "8"
#   - name: TP_SIZE
#     value: "1"
#   <...many environment variables...>
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_ENVVARS_TO_YAML=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_PREFILL_ENVVARS_TO_YAML
- name: DP_SIZE_LOCAL
  value: "8"
- name: TP_SIZE
  value: "1"
- name: TRITON_LIBCUDA_PATH
  value: /usr/lib64
- name: VLLM_SKIP_P2P_CHECK
  value: "1"
- name: VLLM_RANDOMIZE_DP_DUMMY_INPUTS
  value: "1"
- name: VLLM_USE_DEEP_GEMM
  value: "1"
- name: VLLM_ALL2ALL_BACKEND
  value: deepep_high_throughput
- name: NVIDIA_GDRCOPY
  value: enabled
- name: NVSHMEM_REMOTE_TRANSPORT
  value: ibgda
- name: NVSHMEM_IB_ENABLE_IBGDA
  value: "true"
- name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
  value: eth0
- name: GLOO_SOCKET_IFNAME
  value: eth0
- name: NCCL_SOCKET_IFNAME
  value: eth0
- name: VLLM_LOGGING_LEVEL
  value: INFO
- name: VLLM_NIXL_SIDE_CHANNEL_HOST
  valueFrom:
    fieldRef:
      fieldPath: status.podIP
- name: CUDA_CACHE_PATH
  value: /var/cache/vllm/cuda
- name: CCACHE_DIR
  value: /var/cache/vllm/ccache
- name: VLLM_CACHE_ROOT
  value: /var/cache/vllm/vllm
- name: FLASHINFER_WORKSPACE_BASE
  value: /var/cache/vllm/flashinfer
- name: HF_HUB_CACHE
  value: /var/cache/huggingface
EOF

# =============================================================================
# SOURCE: manifests/modelserver/gke/kustomization.yaml
# Lines 70-104 (GKE-specific environment variable patches):
#   - op: add
#     path: /spec/leaderWorkerTemplate/workerTemplate/spec/containers/0/env/-
#     value:
#       name: DEEP_EP_DEVICE_TO_HCA_MAPPING
#       value: "0:mlx5_0:1,..."
#   - op: add (BASH_ENV)
#   - op: add (NVSHMEM_DISABLED_GDRCOPY)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_GKE=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_GKE
- name: DEEP_EP_DEVICE_TO_HCA_MAPPING
  value: "0:mlx5_0:1,1:mlx5_1:1,2:mlx5_2:1,3:mlx5_3:1,4:mlx5_4:1,5:mlx5_5:1,6:mlx5_6:1,7:mlx5_7:1"
- name: BASH_ENV
  value: "/usr/local/gib/scripts/set_nccl_env.sh"
- name: NVSHMEM_DISABLED_GDRCOPY
  value: "true"
EOF

export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_ENVVARS_GKE=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_PREFILL_ENVVARS_GKE
- name: DEEP_EP_DEVICE_TO_HCA_MAPPING
  value: "0:mlx5_0:1,1:mlx5_1:1,2:mlx5_2:1,3:mlx5_3:1,4:mlx5_4:1,5:mlx5_5:1,6:mlx5_6:1,7:mlx5_7:1"
- name: BASH_ENV
  value: "/usr/local/gib/scripts/set_nccl_env.sh"
- name: NVSHMEM_DISABLED_GDRCOPY
  value: "true"
EOF

# Merge GKE-specific env vars with base env vars
cat $LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_GKE >> $LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_TO_YAML
cat $LLMDBENCH_VLLM_MODELSERVICE_PREFILL_ENVVARS_GKE >> $LLMDBENCH_VLLM_MODELSERVICE_PREFILL_ENVVARS_TO_YAML

# =============================================================================
# SOURCE: manifests/modelserver/base/decode.yaml + gke/kustomization.yaml
# Lines 26-34 (base volumes) + Lines 81-100 (GKE patches):
#   volumes:
#     - name: dshm (already handled via SHM_MEM)
#     - name: hf-cache (emptyDir, replaced by GKE hostPath)
#     - name: jit-cache (emptyDir, replaced by GKE hostPath)
#   GKE patches:
#     - name: gib (hostPath: /home/kubernetes/bin/gib)
#     - Replace hf-cache and jit-cache with local SSD paths
#
# PLUS benchmark framework standard volumes (preprocesses)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES
- name: preprocesses
  configMap:
    defaultMode: 0755
    name: llm-d-benchmark-preprocesses
- name: gib
  hostPath:
    path: /home/kubernetes/bin/gib
    type: ""
- name: hf-cache
  hostPath:
    path: /mnt/stateful_partition/kube-ephemeral-ssd/shared_disk/vllm-hf-cache/
    type: DirectoryOrCreate
- name: jit-cache
  hostPath:
    path: /mnt/stateful_partition/kube-ephemeral-ssd/shared_disk/vllm-jit-cache/
    type: DirectoryOrCreate
EOF

export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUMES=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUMES
- name: preprocesses
  configMap:
    defaultMode: 0755
    name: llm-d-benchmark-preprocesses
- name: gib
  hostPath:
    path: /home/kubernetes/bin/gib
    type: ""
- name: hf-cache
  hostPath:
    path: /mnt/stateful_partition/kube-ephemeral-ssd/shared_disk/vllm-hf-cache/
    type: DirectoryOrCreate
- name: jit-cache
  hostPath:
    path: /mnt/stateful_partition/kube-ephemeral-ssd/shared_disk/vllm-jit-cache/
    type: DirectoryOrCreate
EOF

# =============================================================================
# SOURCE: manifests/modelserver/base/decode.yaml + gke/kustomization.yaml
# Lines 213-225 (base volumeMounts) + Lines 87-92 (GKE gib mount):
#   volumeMounts:
#     - name: dshm
#       mountPath: /dev/shm
#     - name: hf-cache
#       mountPath: /var/cache/huggingface
#     - name: jit-cache
#       mountPath: /var/cache/vllm
#   GKE patch:
#     - name: gib
#       mountPath: /usr/local/gib
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS
- name: preprocesses
  mountPath: /setup/preprocess
- name: gib
  mountPath: /usr/local/gib
- name: hf-cache
  mountPath: /var/cache/huggingface
- name: jit-cache
  mountPath: /var/cache/vllm
EOF

export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUME_MOUNTS=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUME_MOUNTS
- name: preprocesses
  mountPath: /setup/preprocess
- name: gib
  mountPath: /usr/local/gib
- name: hf-cache
  mountPath: /var/cache/huggingface
- name: jit-cache
  mountPath: /var/cache/vllm
EOF

# =============================================================================
# SOURCE: manifests/modelserver/gke/kustomization.yaml
# Lines 11-44 (pod affinity patches for prefill and decode):
#   - op: add
#     path: /spec/leaderWorkerTemplate/workerTemplate/spec/affinity
#     value:
#       podAffinity:
#         preferredDuringSchedulingIgnoredDuringExecution:
#         - weight: 2
#           topologyKey: cloud.google.com/gce-topology-block
#         - weight: 1
#           topologyKey: cloud.google.com/gce-topology-subblock
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_POD_CONFIG=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_POD_CONFIG
affinity:
  podAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 2
      podAffinityTerm:
        labelSelector:
          matchLabels:
            llm-d.ai/role: decode
        matchLabelKeys:
        - component
        topologyKey: cloud.google.com/gce-topology-block
    - weight: 1
      podAffinityTerm:
        labelSelector:
          matchLabels:
            llm-d.ai/role: decode
        matchLabelKeys:
        - component
        topologyKey: cloud.google.com/gce-topology-subblock
EOF

export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_POD_CONFIG=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_POD_CONFIG
affinity:
  podAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 2
      podAffinityTerm:
        labelSelector:
          matchLabels:
            llm-d.ai/role: prefill
        matchLabelKeys:
        - component
        topologyKey: cloud.google.com/gce-topology-block
    - weight: 1
      podAffinityTerm:
        labelSelector:
          matchLabels:
            llm-d.ai/role: prefill
        matchLabelKeys:
        - component
        topologyKey: cloud.google.com/gce-topology-subblock
EOF

# =============================================================================
# SOURCE: manifests/modelserver/gke/kustomization.yaml
# Lines 49-69 (GKE RDMA networking annotations and privileged security):
#   - op: add
#     path: /spec/leaderWorkerTemplate/workerTemplate/spec/containers/0/securityContext/privileged
#     value: true
#   - op: add
#     path: /spec/leaderWorkerTemplate/workerTemplate/metadata/annotations
#     value:
#       networking.gke.io/default-interface: 'eth0'
#       networking.gke.io/interfaces: |
#         [<eth0-eth9 configuration>]
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_PODANNOTATIONS=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_PODANNOTATIONS
networking.gke.io/default-interface: 'eth0'
networking.gke.io/interfaces: |
  [
    {"interfaceName":"eth0","network":"default"},
    {"interfaceName":"eth2","network":"rdma-0"},
    {"interfaceName":"eth3","network":"rdma-1"},
    {"interfaceName":"eth4","network":"rdma-2"},
    {"interfaceName":"eth5","network":"rdma-3"},
    {"interfaceName":"eth6","network":"rdma-4"},
    {"interfaceName":"eth7","network":"rdma-5"},
    {"interfaceName":"eth8","network":"rdma-6"},
    {"interfaceName":"eth9","network":"rdma-7"}
  ]
EOF

export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_PODANNOTATIONS=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_PREFILL_PODANNOTATIONS
networking.gke.io/default-interface: 'eth0'
networking.gke.io/interfaces: |
  [
    {"interfaceName":"eth0","network":"default"},
    {"interfaceName":"eth2","network":"rdma-0"},
    {"interfaceName":"eth3","network":"rdma-1"},
    {"interfaceName":"eth4","network":"rdma-2"},
    {"interfaceName":"eth5","network":"rdma-3"},
    {"interfaceName":"eth6","network":"rdma-4"},
    {"interfaceName":"eth7","network":"rdma-5"},
    {"interfaceName":"eth8","network":"rdma-6"},
    {"interfaceName":"eth9","network":"rdma-7"}
  ]
EOF

# =============================================================================
# SOURCE: manifests/modelserver/gke/kustomization.yaml
# Lines 49-51:
#   - op: add
#     path: /spec/leaderWorkerTemplate/workerTemplate/spec/containers/0/securityContext/privileged
#     value: true
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_CONTAINER_CONFIG=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_CONTAINER_CONFIG
securityContext:
  privileged: true
EOF

export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_CONTAINER_CONFIG=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_CONTAINER_CONFIG
securityContext:
  privileged: true
EOF

# =============================================================================
# SOURCE: Benchmark framework defaults
# =============================================================================
export LLMDBENCH_HARNESS_NAME=inference-perf
export LLMDBENCH_HARNESS_EXPERIMENT_PROFILE=sanity_random.yaml
export LLMDBENCH_CONTROL_WORK_DIR=~/data/wide-ep-lws-gke
