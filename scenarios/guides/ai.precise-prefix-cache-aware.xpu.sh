# PRECISE PREFIX CACHE AWARE ROUTING - XPU VARIANT
# Based on https://github.com/llm-d/llm-d/tree/main/guides/precise-prefix-cache-aware
# Variant: Intel XPU (Data Center GPU Max 1550)
# Generated by llm-d-benchmark guide converter

# =============================================================================
# SOURCE: ms-kv-events/values_xpu.yaml
# Lines 2-3:
#   name: Qwen/Qwen3-0.6B
#   size: 10Gi
# =============================================================================
export LLMDBENCH_DEPLOY_MODEL_LIST="Qwen/Qwen3-0.6B"

# =============================================================================
# SOURCE: ms-kv-events/values_xpu.yaml
# Line 3: size: 10Gi
# =============================================================================
export LLMDBENCH_VLLM_COMMON_PVC_MODEL_CACHE_SIZE="10Gi"

# =============================================================================
# SOURCE: ms-kv-events/values_xpu.yaml
# Lines 10-11:
#   accelerator:
#     type: intel-i915
# NOTE: XPU variant uses Intel Data Center GPU Max 1550
# =============================================================================
export LLMDBENCH_VLLM_COMMON_ACCELERATOR_RESOURCE="intel.com/i915"

# =============================================================================
# SOURCE: ms-kv-events/values_xpu.yaml
# Line 14: replicas: 2
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_REPLICAS=2

# =============================================================================
# SOURCE: ms-kv-events/values_xpu.yaml
# Lines 18-19:
#   tensor: 1
#   data: 1
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM=1

# =============================================================================
# SOURCE: ms-kv-events/values_xpu.yaml
# Lines 23-24:
#   image: ghcr.io/llm-d/llm-d-xpu:v0.5.0
# NOTE: XPU variant uses specialized Intel XPU container image
# =============================================================================
export LLMDBENCH_LLMD_IMAGE_NAME="llm-d-xpu"
export LLMDBENCH_LLMD_IMAGE_TAG="v0.5.0"

# =============================================================================
# SOURCE: ms-kv-events/values_xpu.yaml
# Lines 30-36:
#   resources:
#     requests:
#       cpu: 4
#       memory: 12Gi
#     limits:
#       cpu: 8
#       memory: 24Gi
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_NR=4
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_MEM="12Gi"

# =============================================================================
# SOURCE: ms-kv-events/values_xpu.yaml
# Lines 46-52:
#   volumes:
#     - name: dshm
#       emptyDir:
#         medium: Memory
#         sizeLimit: 6Gi
# =============================================================================
export LLMDBENCH_VLLM_COMMON_SHM_MEM="6Gi"

# =============================================================================
# SOURCE: ms-kv-events/values_xpu.yaml
# Lines 25-29:
#   args:
#     - "--block-size=64"
#     - "--gpu-memory-utilization=0.95"
#     - "--prefix-caching-hash-algo sha256_cbor"
#     - "--kv-events ..."
# NOTE: Block size is explicitly set for precise prefix cache tracking
# =============================================================================
export LLMDBENCH_VLLM_COMMON_BLOCK_SIZE=64

# =============================================================================
# SOURCE: Benchmark framework convention (not in guide)
# Using custom command to inject preprocessing and exact vLLM args
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_MODEL_COMMAND=custom
export LLMDBENCH_VLLM_COMMON_PREPROCESS="python3 /setup/preprocess/set_llmdbench_environment.py; source \$HOME/llmdbench_env.sh"
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS=$LLMDBENCH_VLLM_COMMON_PREPROCESS

# =============================================================================
# SOURCE: ms-kv-events/values_xpu.yaml
# Lines 25-29:
#   args:
#     - "--block-size=64"
#     - "--gpu-memory-utilization=0.95"
#     - "--prefix-caching-hash-algo sha256_cbor"
#     - "--kv-events endpoint=tcp://gaie-${GAIE_RELEASE_NAME_POSTFIX}-epp.$(NAMESPACE).svc.cluster.local:5557,channel_type=zmq,topic_prefix=kv@,concurrency=4,publisher_kind=precise_scorer"
#
# NOTE: KV events enable precise prefix cache tracking via ZMQ publisher
# NOTE: Using benchmark standard ports (METRICS_PORT=8200 for vLLM)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS
REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS; \
vllm serve /model-cache/models/REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--host 0.0.0.0 \
--served-model-name REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--port REPLACE_ENV_LLMDBENCH_VLLM_COMMON_METRICS_PORT \
--block-size 64 \
--gpu-memory-utilization 0.95 \
--prefix-caching-hash-algo sha256_cbor \
--kv-events endpoint=tcp://gaie-kv-events-epp.REPLACE_ENV_LLMDBENCH_VLLM_COMMON_NAMESPACE.svc.cluster.local:5557,channel_type=zmq,topic_prefix=kv@,concurrency=4,publisher_kind=precise_scorer \
--disable-log-requests
EOF

# =============================================================================
# SOURCE: ms-kv-events/values_xpu.yaml
# Lines 37-45:
#   env:
#     - name: VLLM_LOGGING_LEVEL
#       value: DEBUG
#     - name: TRITON_CACHE_DIR
#       value: /.triton-cache
#     - name: NAMESPACE
#       valueFrom: ...
#     - name: POD_IP
#       valueFrom: ...
#     - name: GAIE_RELEASE_NAME_POSTFIX
#       value: kv-events
#
# NOTE: XPU requires specific environment variables for optimal performance
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_TO_YAML=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_TO_YAML
- name: VLLM_USE_V1
  value: "1"
- name: TORCH_LLM_ALLREDUCE
  value: "1"
- name: VLLM_WORKER_MULTIPROC_METHOD
  value: "spawn"
- name: UCX_TLS
  value: "tcp"
- name: VLLM_LOGGING_LEVEL
  value: DEBUG
- name: TRITON_CACHE_DIR
  value: /.triton-cache
- name: GAIE_RELEASE_NAME_POSTFIX
  value: kv-events
EOF

# =============================================================================
# SOURCE: ms-kv-events/values_xpu.yaml
# Lines 46-58:
#   volumes:
#     - name: dshm
#       emptyDir:
#         medium: Memory
#         sizeLimit: 6Gi
#     - name: torch-compile-cache
#       emptyDir: {}
#     - name: triton-cache
#       emptyDir: {}
#
# PLUS benchmark framework standard volumes (preprocesses, etc.)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES}
- name: preprocesses
  configMap:
    defaultMode: 0755
    name: llm-d-benchmark-preprocesses
- name: dshm
  emptyDir:
    medium: Memory
    sizeLimit: REPLACE_ENV_LLMDBENCH_VLLM_COMMON_SHM_MEM
- name: torch-compile-cache
  emptyDir: {}
- name: triton-cache
  emptyDir: {}
EOF

# =============================================================================
# SOURCE: ms-kv-events/values_xpu.yaml
# Implied from volumes section, lines 53-58
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS}
- name: dshm
  mountPath: /dev/shm
- name: preprocesses
  mountPath: /setup/preprocess
- name: torch-compile-cache
  mountPath: /tmp/torch-compile-cache
- name: triton-cache
  mountPath: /.triton-cache
EOF

# =============================================================================
# SOURCE: ms-kv-events/values_xpu.yaml
# Line 61: create: false
# NOTE: XPU variant uses decode-only architecture (no prefill/decode disaggregation)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_REPLICAS=0

# =============================================================================
# SOURCE: ms-kv-events/values_xpu.yaml
# Line 64:
#   proxy:
#     enabled: false
# NOTE: Routing proxy disabled; precise prefix cache routing handles distribution
# =============================================================================
export LLMDBENCH_LLMD_ROUTINGSIDECAR_CONNECTOR=""

# =============================================================================
# SOURCE: gaie-kv-events/values.yaml
# Lines 4-6:
#   image:
#     name: llm-d-inference-scheduler
#     hub: ghcr.io/llm-d
#     tag: v0.5.0
# =============================================================================
export LLMDBENCH_LLMD_INFERENCESCHEDULER_IMAGE_TAG="v0.5.0"

# =============================================================================
# SOURCE: gaie-kv-events/values.yaml
# Line 9: pluginsConfigFile: precise-prefix-cache-config.yaml
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_PLUGINS_CONFIGFILE="precise-prefix-cache-config.yaml"

# =============================================================================
# SOURCE: gaie-kv-events/values.yaml
# Lines 10-42:
#   pluginsCustomConfig:
#     precise-prefix-cache-config.yaml: |
#       apiVersion: inference.networking.x-k8s.io/v1alpha1
#       kind: EndpointPickerConfig
#       plugins:
#         - type: single-profile-handler
#         - type: precise-prefix-cache-scorer
#           parameters:
#             tokenProcessorConfig:
#               blockSize: 64
#             indexerConfig:
#               tokenizersPoolConfig:
#                 modelName: "Qwen/Qwen3-32B"
#                 hf:
#                   tokenizersCacheDir: "/tmp/tokenizers"
#             kvEventsConfig:
#               topicFilter: "kv@"
#               concurrency: 4
#               discoverPods: false
#               zmqEndpoint: "tcp://*:5557"
#         - type: kv-cache-utilization-scorer
#         - type: queue-scorer
#         - type: max-score-picker
#       schedulingProfiles:
#         - name: default
#           plugins:
#             - pluginRef: precise-prefix-cache-scorer
#               weight: 3.0
#             - pluginRef: kv-cache-utilization-scorer
#               weight: 2.0
#             - pluginRef: queue-scorer
#               weight: 2.0
#             - pluginRef: max-score-picker
#
# NOTE: Custom plugin config for precise prefix cache aware routing
# NOTE: Model name in tokenizer config must match actual deployed model
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_CUSTOM_PLUGINS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_GAIE_CUSTOM_PLUGINS
precise-prefix-cache-config.yaml: |
  apiVersion: inference.networking.x-k8s.io/v1alpha1
  kind: EndpointPickerConfig
  plugins:
    - type: single-profile-handler
    - type: precise-prefix-cache-scorer
      parameters:
        tokenProcessorConfig:
          blockSize: 64
        indexerConfig:
          tokenizersPoolConfig:
            modelName: "Qwen/Qwen3-0.6B"
            hf:
              tokenizersCacheDir: "/tmp/tokenizers"
        kvEventsConfig:
          topicFilter: "kv@"
          concurrency: 4
          discoverPods: false
          zmqEndpoint: "tcp://*:5557"
    - type: kv-cache-utilization-scorer
    - type: queue-scorer
    - type: max-score-picker
  schedulingProfiles:
    - name: default
      plugins:
        - pluginRef: precise-prefix-cache-scorer
          weight: 3.0
        - pluginRef: kv-cache-utilization-scorer
          weight: 2.0
        - pluginRef: queue-scorer
          weight: 2.0
        - pluginRef: max-score-picker
EOF

# =============================================================================
# SOURCE: helmfile.yaml.gotmpl
# Line 8: version: v1.3.6
# =============================================================================
export LLMDBENCH_VLLM_INFRA_CHART_VERSION="v1.3.6"

# =============================================================================
# SOURCE: helmfile.yaml.gotmpl
# Line 17: version: v1.3.0
# =============================================================================
export LLMDBENCH_VLLM_GAIE_CHART_VERSION="v1.3.0"

# =============================================================================
# SOURCE: helmfile.yaml.gotmpl
# Line 28: version: v0.4.5
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_CHART_VERSION="v0.4.5"

# =============================================================================
# SOURCE: Benchmark framework defaults
# =============================================================================
export LLMDBENCH_HARNESS_NAME=inference-perf
export LLMDBENCH_HARNESS_EXPERIMENT_PROFILE=sanity_random.yaml
export LLMDBENCH_CONTROL_WORK_DIR=~/data/precise-prefix-cache-aware-xpu
