# PRECISE PREFIX CACHE AWARE WELL LIT PATH (XPU)
# Based on https://github.com/llm-d/llm-d/tree/main/guides/precise-prefix-cache-aware
# Variant: XPU (Intel GPU)
# Generated by llm-d-benchmark guide converter
#
# This guide demonstrates precise prefix cache aware routing using vLLM KV-Events data
# to eliminate indexing service dependencies and improve cache hit rates at high throughput.
# XPU variant uses Intel i915 accelerators with smaller Qwen3-0.6B model.

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/ms-kv-events/values_xpu.yaml
# Lines 11-12:
#   name: Qwen/Qwen3-0.6B
#   uri: "hf://Qwen/Qwen3-0.6B"
# =============================================================================
export LLMDBENCH_DEPLOY_MODEL_LIST="Qwen/Qwen3-0.6B"

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/ms-kv-events/values_xpu.yaml
# Line 13:
#   size: 10Gi
# =============================================================================
export LLMDBENCH_VLLM_COMMON_PVC_MODEL_CACHE_SIZE=10Gi

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/helmfile.yaml.gotmpl
# Lines 39-41:
#   - name: infra-{rn}
#     chart: llm-d-infra/llm-d-infra
#     version: v1.3.6
# =============================================================================
export LLMDBENCH_VLLM_INFRA_CHART_VERSION="v1.3.6"

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/helmfile.yaml.gotmpl
# Lines 52-54:
#   - name: gaie-{rn}
#     chart: oci://registry.k8s.io/gateway-api-inference-extension/charts/inferencepool
#     version: v1.3.0
# =============================================================================
export LLMDBENCH_VLLM_GAIE_CHART_VERSION="v1.3.0"

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/helmfile.yaml.gotmpl
# Lines 81-83:
#   - name: ms-{rn}
#     chart: llm-d-modelservice/llm-d-modelservice
#     version: v0.4.5
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_CHART_VERSION="v0.4.5"

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/ms-kv-events/values_xpu.yaml
# Lines 5-7:
#   accelerator:
#     type: intel-i915
#     dra: true
# =============================================================================
export LLMDBENCH_VLLM_COMMON_ACCELERATOR_NAME="intel-i915"

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/ms-kv-events/values_xpu.yaml
# Lines 36-38:
#   image: ghcr.io/llm-d/llm-d-xpu:v0.5.0
#
# NOTE: XPU-specific container image
# =============================================================================
export LLMDBENCH_LLMD_IMAGE_TAG="v0.5.0"
export LLMDBENCH_LLMD_IMAGE_NAME="llm-d-xpu"

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/ms-kv-events/values_xpu.yaml
# Lines 28-29:
#   decode:
#     replicas: 2
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_REPLICAS=2

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/ms-kv-events/values_xpu.yaml
# Lines 82-87:
#   resources:
#     limits:
#       memory: 24Gi
#       cpu: "8"
#     requests:
#       cpu: "4"
#       memory: 12Gi
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_NR=4
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_MEM=12Gi

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/ms-kv-events/values_xpu.yaml
# Line 53:
#   --block-size 64
# =============================================================================
export LLMDBENCH_VLLM_COMMON_BLOCK_SIZE=64

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/ms-kv-events/values_xpu.yaml
# Lines 21-23:
#   routing:
#     proxy:
#       enabled: false
#
# NOTE: Routing sidecar disabled (no P/D disaggregation in this guide)
# =============================================================================
export LLMDBENCH_LLMD_ROUTINGSIDECAR_ENABLED=false

# =============================================================================
# SOURCE: Benchmark framework convention (not in guide)
# Using custom command to inject preprocessing and exact vLLM args
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_MODEL_COMMAND=custom
export LLMDBENCH_VLLM_COMMON_PREPROCESS="python3 /setup/preprocess/set_llmdbench_environment.py; source \$HOME/llmdbench_env.sh"
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS=$LLMDBENCH_VLLM_COMMON_PREPROCESS

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/ms-kv-events/values_xpu.yaml
# Lines 36-51:
#   modelCommand: custom
#   args:
#     - |
#       exec vllm serve Qwen/Qwen3-0.6B \
#         --port 8000 \
#         --served-model-name "Qwen/Qwen3-0.6B" \
#         --dtype float16 \
#         --disable-sliding-window \
#         --block-size 64 \
#         --prefix-caching-hash-algo sha256_cbor \
#         --kv-events-config "{...}"
#
# NOTE: Port mapping - Guide uses 8000 but benchmark uses proxy pattern:
#   - INFERENCE_PORT (8000): Service port
#   - METRICS_PORT (8200): vLLM listen port
#
# NOTE: KV events endpoint references GAIE service for precise prefix cache awareness
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS
REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS; \
vllm serve /model-cache/models/REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--host 0.0.0.0 \
--served-model-name REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--port REPLACE_ENV_LLMDBENCH_VLLM_COMMON_METRICS_PORT \
--dtype float16 \
--disable-sliding-window \
--block-size REPLACE_ENV_LLMDBENCH_VLLM_COMMON_BLOCK_SIZE \
--prefix-caching-hash-algo sha256_cbor \
--kv-events-config '{"enable_kv_cache_events":true,"publisher":"zmq","endpoint":"tcp://gaie-default-epp.REPLACE_ENV_LLMDBENCH_VLLM_COMMON_NAMESPACE.svc.cluster.local:5557","topic":"kv@${POD_IP}@REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL"}'
EOF

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/ms-kv-events/values_xpu.yaml
# Lines 52-55:
#   securityContext:
#     fsGroup: 107
#     supplementalGroups:
#       - 107
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_CONTAINER_CONFIG=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_CONTAINER_CONFIG
securityContext:
  fsGroup: 107
  supplementalGroups:
    - 107
EOF

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/ms-kv-events/values_xpu.yaml
# Lines 60-77:
#   env:
#     - name: GAIE_RELEASE_NAME_POSTFIX
#       value: "default"
#     - name: NAMESPACE
#       valueFrom:
#         fieldRef:
#           fieldPath: metadata.namespace
#     - name: PYTHONHASHSEED
#       value: "42"
#     - name: POD_IP
#       valueFrom:
#         fieldRef:
#           apiVersion: v1
#           fieldPath: status.podIP
#     - name: VLLM_LOGGING_LEVEL
#       value: DEBUG
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_TO_YAML=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_TO_YAML
- name: GAIE_RELEASE_NAME_POSTFIX
  value: "default"
- name: NAMESPACE
  valueFrom:
    fieldRef:
      fieldPath: metadata.namespace
- name: PYTHONHASHSEED
  value: "42"
- name: POD_IP
  valueFrom:
    fieldRef:
      apiVersion: v1
      fieldPath: status.podIP
- name: VLLM_LOGGING_LEVEL
  value: DEBUG
EOF

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/ms-kv-events/values_xpu.yaml
# Lines 89-92, 106-109:
#   volumeMounts:
#     - name: metrics-volume
#       mountPath: /.config
#     - name: torch-compile-cache
#       mountPath: /.cache
#   volumes:
#     - name: metrics-volume
#       emptyDir: {}
#     - name: torch-compile-cache
#       emptyDir: {}
#
# PLUS Benchmark framework standard volume mounts
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS
- name: preprocesses
  mountPath: /setup/preprocess
- name: metrics-volume
  mountPath: /.config
- name: torch-compile-cache
  mountPath: /.cache
EOF

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/ms-kv-events/values_xpu.yaml
# Lines 106-109:
#   volumes:
#     - name: metrics-volume
#       emptyDir: {}
#     - name: torch-compile-cache
#       emptyDir: {}
#
# PLUS Benchmark framework standard volumes
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES
- name: preprocesses
  configMap:
    defaultMode: 0755
    name: llm-d-benchmark-preprocesses
- name: metrics-volume
  emptyDir: {}
- name: torch-compile-cache
  emptyDir: {}
EOF

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/ms-kv-events/values_xpu.yaml
# Lines 112-114:
#   prefill:
#     create: false
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_REPLICAS=0

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/gaie-kv-events/values.yaml
# Lines 11-14:
#   image:
#     name: llm-d-inference-scheduler
#     hub: ghcr.io/llm-d
#     tag: v0.5.0
# =============================================================================
export LLMDBENCH_LLMD_INFERENCESCHEDULER_IMAGE_NAME="llm-d-inference-scheduler"
export LLMDBENCH_LLMD_INFERENCESCHEDULER_IMAGE_REGISTRY="ghcr.io"
export LLMDBENCH_LLMD_INFERENCESCHEDULER_IMAGE_REPO="llm-d"
export LLMDBENCH_LLMD_INFERENCESCHEDULER_IMAGE_TAG="v0.5.0"

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/gaie-kv-events/values.yaml
# Lines 3-6:
#   flags:
#     kv-cache-usage-percentage-metric: "vllm:kv_cache_usage_perc"
#     v: 4
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_FLAGS=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_GAIE_FLAGS
kv-cache-usage-percentage-metric: "vllm:kv_cache_usage_perc"
v: 4
EOF

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/gaie-kv-events/values.yaml
# Lines 19-24:
#   extProcPort: 9002
#   extraContainerPorts:
#     - name: zmq
#       containerPort: 5557
#       protocol: TCP
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_EXTRA_CONTAINER_PORTS=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_GAIE_EXTRA_CONTAINER_PORTS
- name: zmq
  containerPort: 5557
  protocol: TCP
EOF

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/gaie-kv-events/values.yaml
# Lines 25-29:
#   extraServicePorts:
#     - name: zmq
#       port: 5557
#       targetPort: 5557
#       protocol: TCP
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_EXTRA_SERVICE_PORTS=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_GAIE_EXTRA_SERVICE_PORTS
- name: zmq
  port: 5557
  targetPort: 5557
  protocol: TCP
EOF

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/gaie-kv-events/values.yaml
# Lines 38-52:
#   sidecar:
#     enabled: true
#     image: ghcr.io/llm-d/llm-d-uds-tokenizer:v0.5.1-rc1
#     name: tokenizer-uds
#     env:
#       - name: TOKENIZERS_DIR
#         value: /tokenizers
#       - name: HF_HOME
#         value: /tokenizers
#     volumeMounts:
#       - mountPath: /tokenizers
#         name: tokenizers
#       - mountPath: /tmp/tokenizer
#         name: tokenizer-uds
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_SIDECAR_ENABLED=true
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_SIDECAR_IMAGE="ghcr.io/llm-d/llm-d-uds-tokenizer:v0.5.1-rc1"
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_SIDECAR_NAME="tokenizer-uds"

export LLMDBENCH_VLLM_MODELSERVICE_GAIE_SIDECAR_ENV=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_GAIE_SIDECAR_ENV
- name: TOKENIZERS_DIR
  value: /tokenizers
- name: HF_HOME
  value: /tokenizers
EOF

export LLMDBENCH_VLLM_MODELSERVICE_GAIE_SIDECAR_VOLUME_MOUNTS=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_GAIE_SIDECAR_VOLUME_MOUNTS
- mountPath: /tokenizers
  name: tokenizers
- mountPath: /tmp/tokenizer
  name: tokenizer-uds
EOF

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/gaie-kv-events/values.yaml
# Lines 53-59:
#   volumes:
#     - name: tokenizers
#       emptyDir: {}
#     - name: tokenizer-uds
#       emptyDir: {}
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_EXTRA_VOLUMES=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_GAIE_EXTRA_VOLUMES
- name: tokenizers
  emptyDir: {}
- name: tokenizer-uds
  emptyDir: {}
EOF

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/gaie-kv-events/values.yaml
# Lines 60-62:
#   volumeMounts:
#     - mountPath: /tmp/tokenizer
#       name: tokenizer-uds
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_EXTRA_VOLUME_MOUNTS=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_GAIE_EXTRA_VOLUME_MOUNTS
- mountPath: /tmp/tokenizer
  name: tokenizer-uds
EOF

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/gaie-kv-events/values.yaml
# Lines 64-65:
#   pluginsConfigFile: "precise-prefix-cache-config.yaml"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_PLUGINS_CONFIGFILE="precise-prefix-cache-config.yaml"

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/gaie-kv-events/values.yaml
# Lines 79-130:
#   pluginsCustomConfig:
#     precise-prefix-cache-config.yaml: |
#       apiVersion: inference.networking.x-k8s.io/v1alpha1
#       kind: EndpointPickerConfig
#       plugins:
#         - type: single-profile-handler
#         - type: precise-prefix-cache-scorer
#           parameters:
#             tokenProcessorConfig:
#               blockSize: 64
#             indexerConfig:
#               tokenizersPoolConfig:
#                 modelName: Qwen/Qwen3-32B
#                 local: null
#                 hf: null
#                 uds:
#                   socketFile: /tmp/tokenizer/tokenizer-uds.socket
#             kvEventsConfig:
#               topicFilter: "kv@"
#               concurrency: 4
#               discoverPods: false
#               zmqEndpoint: "tcp://*:5557"
#         - type: kv-cache-utilization-scorer
#         - type: queue-scorer
#         - type: max-score-picker
#       schedulingProfiles:
#         - name: default
#           plugins:
#             - pluginRef: precise-prefix-cache-scorer
#               weight: 3.0
#             - pluginRef: kv-cache-utilization-scorer
#               weight: 2.0
#             - pluginRef: queue-scorer
#               weight: 2.0
#             - pluginRef: max-score-picker
#
# NOTE: Complete custom plugin configuration for precise prefix cache awareness
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_CUSTOM_PLUGINS=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_GAIE_CUSTOM_PLUGINS
precise-prefix-cache-config.yaml: |
  apiVersion: inference.networking.x-k8s.io/v1alpha1
  kind: EndpointPickerConfig
  plugins:
    - type: single-profile-handler
    - type: precise-prefix-cache-scorer
      parameters:
        tokenProcessorConfig:
          blockSize: 64
        indexerConfig:
          tokenizersPoolConfig:
            modelName: Qwen/Qwen3-0.6B
            local: null
            hf: null
            uds:
              socketFile: /tmp/tokenizer/tokenizer-uds.socket
        kvEventsConfig:
          topicFilter: "kv@"
          concurrency: 4
          discoverPods: false
          zmqEndpoint: "tcp://*:5557"
    - type: kv-cache-utilization-scorer
    - type: queue-scorer
    - type: max-score-picker
  schedulingProfiles:
    - name: default
      plugins:
        - pluginRef: precise-prefix-cache-scorer
          weight: 3.0
        - pluginRef: kv-cache-utilization-scorer
          weight: 2.0
        - pluginRef: queue-scorer
          weight: 2.0
        - pluginRef: max-score-picker
EOF

# =============================================================================
# SOURCE: Benchmark framework defaults
# =============================================================================
export LLMDBENCH_HARNESS_NAME=inference-perf
export LLMDBENCH_HARNESS_EXPERIMENT_PROFILE=sanity_random.yaml
export LLMDBENCH_CONTROL_WORK_DIR=~/data/precise-prefix-cache-aware-xpu
