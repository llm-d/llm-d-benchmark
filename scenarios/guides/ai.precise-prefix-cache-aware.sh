# PRECISE PREFIX CACHE AWARE WELL LIT PATH
# Based on https://github.com/llm-d/llm-d/tree/main/guides/precise-prefix-cache-aware
# Generated by llm-d-benchmark guide converter
#
# This guide demonstrates precise prefix cache aware routing leveraging vLLM KV-Events.
# The system automatically detects and routes requests to instances with cached key-value
# blocks, improving performance without requiring separate indexing services.

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/ms-kv-events/values.yaml
# Lines 4-5:
#   uri: "hf://Qwen/Qwen3-32B"
#   name: "Qwen/Qwen3-32B"
# =============================================================================
export LLMDBENCH_DEPLOY_MODEL_LIST="Qwen/Qwen3-32B"

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/ms-kv-events/values.yaml
# Line 6:
#   size: 80Gi
# =============================================================================
export LLMDBENCH_VLLM_COMMON_PVC_MODEL_CACHE_SIZE="80Gi"

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/gaie-kv-events/values.yaml
# Lines 7-9:
#   image:
#     name: llm-d-inference-scheduler
#     hub: ghcr.io/llm-d
#     tag: v0.5.0
#
# AND guides/precise-prefix-cache-aware/ms-kv-events/values.yaml
# Line 39:
#   image: ghcr.io/llm-d/llm-d-cuda:v0.5.0
# =============================================================================
export LLMDBENCH_LLMD_IMAGE_TAG="v0.5.0"
export LLMDBENCH_LLMD_INFERENCESCHEDULER_IMAGE_TAG="v0.5.0"

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/ms-kv-events/values.yaml
# Lines 20-22:
#   parallelism:
#     tensor: 2
#     data: 1
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM=2
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_DATA_PARALLELISM=1

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/ms-kv-events/values.yaml
# Line 23:
#   replicas: 8
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_REPLICAS=8

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/ms-kv-events/values.yaml
# Lines 57-62:
#   resources:
#     limits:
#       cpu: '32'
#       memory: 100Gi
#     requests:
#       cpu: '32'
#       memory: 100Gi
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_NR=32
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_MEM=100Gi

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/ms-kv-events/values.yaml
# Lines 82-85:
#   - name: shm
#     emptyDir:
#       medium: Memory
#       sizeLimit: 20Gi
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_SHM_MEM=20Gi

# =============================================================================
# SOURCE: Benchmark framework convention (not in guide)
# Using custom command to inject preprocessing and exact vLLM args
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_MODEL_COMMAND=custom
export LLMDBENCH_VLLM_COMMON_PREPROCESS="python3 /setup/preprocess/set_llmdbench_environment.py; source \$HOME/llmdbench_env.sh"
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS=$LLMDBENCH_VLLM_COMMON_PREPROCESS

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/ms-kv-events/values.yaml
# Lines 33-44:
#   modelCommand: vllmServe
#   args:
#     - "--block-size=64"
#     - "--kv-events-config"
#     - |-
#       {
#         "enable_kv_cache_events": true,
#         "publisher": "zmq",
#         "endpoint": "tcp://gaie-$(GAIE_RELEASE_NAME_POSTFIX)-epp.$(NAMESPACE).svc.cluster.local:5557",
#         "topic": "kv@$(POD_IP)@Qwen/Qwen3-32B"
#       }
#     - "--disable-uvicorn-access-log"
#     - "--gpu-memory-utilization=0.95"
# =============================================================================
export LLMDBENCH_VLLM_COMMON_BLOCK_SIZE=64
export LLMDBENCH_VLLM_COMMON_ACCELERATOR_MEM_UTIL=0.95

export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS
REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS; \
vllm serve /model-cache/models/REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--host 0.0.0.0 \
--served-model-name REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--port REPLACE_ENV_LLMDBENCH_VLLM_COMMON_METRICS_PORT \
--tensor-parallel-size REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM \
--block-size 64 \
--kv-events-config '{"enable_kv_cache_events":true,"publisher":"zmq","endpoint":"tcp://gaie-llmdbench-epp.REPLACE_ENV_LLMDBENCH_VLLM_COMMON_NAMESPACE.svc.cluster.local:5557","topic":"kv@\$(POD_IP)@REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL"}' \
--disable-uvicorn-access-log \
--gpu-memory-utilization REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_ACCELERATOR_MEM_UTIL
EOF

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/ms-kv-events/values.yaml
# Lines 45-57:
#   env:
#     - name: GAIE_RELEASE_NAME_POSTFIX
#     - name: NAMESPACE
#       valueFrom:
#         fieldRef:
#           fieldPath: metadata.namespace
#     - name: POD_IP
#       valueFrom:
#         fieldRef:
#           apiVersion: v1
#           fieldPath: status.podIP
#     - name: VLLM_LOGGING_LEVEL
#       value: DEBUG
#     - name: TRITON_CACHE_DIR
#       value: /.triton-cache
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_TO_YAML=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_TO_YAML
- name: GAIE_RELEASE_NAME_POSTFIX
  value: "llmdbench"
- name: NAMESPACE
  valueFrom:
    fieldRef:
      fieldPath: metadata.namespace
- name: POD_IP
  valueFrom:
    fieldRef:
      apiVersion: v1
      fieldPath: status.podIP
- name: VLLM_LOGGING_LEVEL
  value: DEBUG
- name: TRITON_CACHE_DIR
  value: /.triton-cache
EOF

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/ms-kv-events/values.yaml
# Lines 66-78:
#   volumeMounts:
#     - name: metrics-volume
#       mountPath: /.config
#     - name: shm
#       mountPath: /dev/shm
#     - name: torch-compile-cache
#       mountPath: /.cache
#     - name: triton-cache
#       mountPath: /.triton-cache
#
# PLUS benchmark framework standard volume mount (preprocesses)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS
- name: preprocesses
  mountPath: /setup/preprocess
- name: dshm
  mountPath: /dev/shm
- name: metrics-volume
  mountPath: /.config
- name: torch-compile-cache
  mountPath: /.cache
- name: triton-cache
  mountPath: /.triton-cache
EOF

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/ms-kv-events/values.yaml
# Lines 90-99:
#   volumes:
#     - name: metrics-volume
#       emptyDir: {}
#     - name: torch-compile-cache
#       emptyDir: {}
#     - name: shm
#       emptyDir:
#         medium: Memory
#         sizeLimit: 20Gi
#     - name: triton-cache
#       emptyDir: {}
#
# PLUS benchmark framework standard volumes (preprocesses, dshm)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES
- name: preprocesses
  configMap:
    defaultMode: 0755
    name: llm-d-benchmark-preprocesses
- name: dshm
  emptyDir:
    medium: Memory
    sizeLimit: REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_SHM_MEM
- name: metrics-volume
  emptyDir: {}
- name: torch-compile-cache
  emptyDir: {}
- name: triton-cache
  emptyDir: {}
EOF

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/ms-kv-events/values.yaml
# Lines 102-103:
#   prefill:
#     create: false
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_REPLICAS=0

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/gaie-kv-events/values.yaml
# Line 23:
#   pluginsConfigFile: "precise-prefix-cache-config.yaml"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_PLUGINS_CONFIGFILE="precise-prefix-cache-config.yaml"

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/gaie-kv-events/values.yaml
# Lines 3-5:
#   flags:
#     kv-cache-usage-percentage-metric: "vllm:kv_cache_usage_perc"
#     v: 4
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_FLAGS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_GAIE_FLAGS
kv-cache-usage-percentage-metric: "vllm:kv_cache_usage_perc"
v: 4
EOF

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/gaie-kv-events/values.yaml
# Lines 24-63:
#   pluginsCustomConfig:
#     precise-prefix-cache-config.yaml: |
#       apiVersion: inference.networking.x-k8s.io/v1alpha1
#       kind: EndpointPickerConfig
#       plugins:
#         - type: single-profile-handler
#         - type: precise-prefix-cache-scorer
#           parameters:
#             tokenProcessorConfig:
#               blockSize: 64
#             indexerConfig:
#               tokenizersPoolConfig:
#                 modelName: "Qwen/Qwen3-32B"
#                 hf:
#                   tokenizersCacheDir: "/tmp/tokenizers"
#             kvEventsConfig:
#               topicFilter: "kv@"
#               concurrency: 4
#               discoverPods: false
#               zmqEndpoint: "tcp://*:5557"
#         - type: kv-cache-utilization-scorer
#         - type: queue-scorer
#         - type: max-score-picker
#       schedulingProfiles:
#         - name: default
#           plugins:
#             - pluginRef: precise-prefix-cache-scorer
#               weight: 3.0
#             - pluginRef: kv-cache-utilization-scorer
#               weight: 2.0
#             - pluginRef: queue-scorer
#               weight: 2.0
#             - pluginRef: max-score-picker
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_CUSTOM_PLUGINS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_GAIE_CUSTOM_PLUGINS
precise-prefix-cache-config.yaml: |
  apiVersion: inference.networking.x-k8s.io/v1alpha1
  kind: EndpointPickerConfig
  plugins:
    - type: single-profile-handler
    - type: precise-prefix-cache-scorer
      parameters:
        tokenProcessorConfig:
          blockSize: 64
        indexerConfig:
          tokenizersPoolConfig:
            modelName: "Qwen/Qwen3-32B"
            hf:
              tokenizersCacheDir: "/tmp/tokenizers"
        kvEventsConfig:
          topicFilter: "kv@"
          concurrency: 4
          discoverPods: false
          zmqEndpoint: "tcp://*:5557"
    - type: kv-cache-utilization-scorer
    - type: queue-scorer
    - type: max-score-picker
  schedulingProfiles:
    - name: default
      plugins:
        - pluginRef: precise-prefix-cache-scorer
          weight: 3.0
        - pluginRef: kv-cache-utilization-scorer
          weight: 2.0
        - pluginRef: queue-scorer
          weight: 2.0
        - pluginRef: max-score-picker
EOF

# =============================================================================
# SOURCE: Benchmark framework defaults
# =============================================================================
export LLMDBENCH_HARNESS_NAME=inference-perf
export LLMDBENCH_HARNESS_EXPERIMENT_PROFILE=sanity_random.yaml
export LLMDBENCH_CONTROL_WORK_DIR=~/data/precise-prefix-cache-aware
