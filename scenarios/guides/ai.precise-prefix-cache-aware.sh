# PRECISE PREFIX CACHE AWARE WELL LIT PATH
# Based on https://github.com/llm-d/llm-d/tree/main/guides/precise-prefix-cache-aware
# Generated by llm-d-benchmark guide converter

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/ms-kv-events/values.yaml
# Lines 3-5:
#   uri: "hf://Qwen/Qwen3-32B"
#   name: "Qwen/Qwen3-32B"
#   size: 80Gi
# =============================================================================
export LLMDBENCH_DEPLOY_MODEL_LIST="Qwen/Qwen3-32B"
export LLMDBENCH_VLLM_COMMON_PVC_MODEL_CACHE_SIZE=80Gi

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/ms-kv-events/values.yaml
# Lines 33-34:
#   image: ghcr.io/llm-d/llm-d-cuda:v0.5.0
#   modelCommand: vllmServe
# =============================================================================
export LLMDBENCH_LLMD_IMAGE_TAG="v0.5.0"

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/helmfile.yaml.gotmpl
# Lines 35-37:
#   - name: infra-...
#     chart: llm-d-infra/llm-d-infra
#     version: v1.3.6
# =============================================================================
export LLMDBENCH_VLLM_INFRA_CHART_VERSION="v1.3.6"

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/helmfile.yaml.gotmpl
# Lines 50-52:
#   - name: gaie-...
#     chart: oci://registry.k8s.io/gateway-api-inference-extension/charts/inferencepool
#     version: v1.3.0
# =============================================================================
export LLMDBENCH_VLLM_GAIE_CHART_VERSION="v1.3.0"

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/helmfile.yaml.gotmpl
# Lines 77-79:
#   - name: ms-...
#     chart: llm-d-modelservice/llm-d-modelservice
#     version: v0.4.5
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_CHART_VERSION="v0.4.5"

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/ms-kv-events/values.yaml
# Lines 23-28:
#   decode:
#     create: true
#     parallelism:
#       tensor: 2
#       data: 1
#     replicas: 8
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_REPLICAS=8
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM=2

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/ms-kv-events/values.yaml
# Lines 73-80:
#   resources:
#     limits:
#       cpu: '32'
#       memory: 100Gi
#     requests:
#       cpu: '32'
#       memory: 100Gi
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_NR=32
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_MEM=100Gi

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/ms-kv-events/values.yaml
# Lines 119-122:
#   - name: shm
#     emptyDir:
#       medium: Memory
#       sizeLimit: 20Gi
# =============================================================================
export LLMDBENCH_VLLM_COMMON_SHM_MEM=20Gi

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/ms-kv-events/values.yaml
# Lines 36-42:
#   args:
#     - "--block-size=64"
#     - "--kv-events-config"
#     - ...
#     - "--disable-uvicorn-access-log"
#     - "--gpu-memory-utilization=0.95"
# =============================================================================
export LLMDBENCH_VLLM_COMMON_BLOCK_SIZE=64
export LLMDBENCH_VLLM_COMMON_ACCELERATOR_MEM_UTIL=0.95

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/ms-kv-events/values.yaml
# Lines 13-14:
#   routing:
#     proxy:
#       enabled: false
# =============================================================================
export LLMDBENCH_LLMD_ROUTINGSIDECAR_ENABLED=false

# =============================================================================
# SOURCE: Benchmark framework convention (not in guide)
# Using custom command to inject preprocessing and exact vLLM args
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_MODEL_COMMAND=custom
export LLMDBENCH_VLLM_COMMON_PREPROCESS="python3 /setup/preprocess/set_llmdbench_environment.py; source \$HOME/llmdbench_env.sh"
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS=$LLMDBENCH_VLLM_COMMON_PREPROCESS

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/ms-kv-events/values.yaml
# Lines 35-48:
#   modelCommand: vllmServe
#   args:
#     - "--block-size=64"
#     - "--kv-events-config"
#     - |-
#       {
#         "enable_kv_cache_events": true,
#         "publisher": "zmq",
#         "endpoint": "tcp://gaie-$(GAIE_RELEASE_NAME_POSTFIX)-epp.$(NAMESPACE).svc.cluster.local:5557",
#         "topic": "kv@$(POD_IP)@Qwen/Qwen3-32B"
#       }
#     - "--disable-uvicorn-access-log"
#     - "--gpu-memory-utilization=0.95"
#
# NOTE: The guide uses GAIE_RELEASE_NAME_POSTFIX which is set via helmfile.
#       In llm-d-benchmark, this is auto-generated, so we use a placeholder.
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS
REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS; \
vllm serve /model-cache/models/REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--host 0.0.0.0 \
--served-model-name REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--port REPLACE_ENV_LLMDBENCH_VLLM_COMMON_METRICS_PORT \
--tensor-parallel-size REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM \
--block-size REPLACE_ENV_LLMDBENCH_VLLM_COMMON_BLOCK_SIZE \
--kv-events-config '{"enable_kv_cache_events":true,"publisher":"zmq","endpoint":"tcp://gaie-kv-events-epp.$(NAMESPACE).svc.cluster.local:5557","topic":"kv@$(POD_IP)@Qwen/Qwen3-32B"}' \
--disable-uvicorn-access-log \
--gpu-memory-utilization REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_ACCELERATOR_MEM_UTIL
EOF

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/ms-kv-events/values.yaml
# Lines 49-63:
#   env:
#     - name: GAIE_RELEASE_NAME_POSTFIX
#     - name: NAMESPACE
#       valueFrom:
#         fieldRef:
#           fieldPath: metadata.namespace
#     - name: POD_IP
#       valueFrom:
#         fieldRef:
#           apiVersion: v1
#           fieldPath: status.podIP
#     - name: VLLM_LOGGING_LEVEL
#       value: DEBUG
#     - name: TRITON_CACHE_DIR
#       value: /.triton-cache
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_TO_YAML=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_TO_YAML
- name: GAIE_RELEASE_NAME_POSTFIX
  value: "kv-events"
- name: NAMESPACE
  valueFrom:
    fieldRef:
      fieldPath: metadata.namespace
- name: POD_IP
  valueFrom:
    fieldRef:
      apiVersion: v1
      fieldPath: status.podIP
- name: VLLM_LOGGING_LEVEL
  value: DEBUG
- name: TRITON_CACHE_DIR
  value: /.triton-cache
EOF

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/ms-kv-events/values.yaml
# Lines 81-91:
#   volumeMounts:
#     - name: metrics-volume
#       mountPath: /.config
#     - name: shm
#       mountPath: /dev/shm
#     - name: torch-compile-cache
#       mountPath: /.cache
#     - name: triton-cache
#       mountPath: /.triton-cache
#
# PLUS Benchmark framework standard volumes (preprocesses)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS
- name: dshm
  mountPath: /dev/shm
- name: preprocesses
  mountPath: /setup/preprocess
- name: metrics-volume
  mountPath: /.config
- name: torch-compile-cache
  mountPath: /.cache
- name: triton-cache
  mountPath: /.triton-cache
EOF

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/ms-kv-events/values.yaml
# Lines 113-125:
#   volumes:
#     - name: metrics-volume
#       emptyDir: {}
#     - name: torch-compile-cache
#       emptyDir: {}
#     - name: shm
#       emptyDir:
#         medium: Memory
#         sizeLimit: 20Gi
#     - name: triton-cache
#       emptyDir: {}
#
# PLUS Benchmark framework standard volumes (preprocesses configMap)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES
- name: preprocesses
  configMap:
    defaultMode: 0755
    name: llm-d-benchmark-preprocesses
- name: dshm
  emptyDir:
    medium: Memory
    sizeLimit: REPLACE_ENV_LLMDBENCH_VLLM_COMMON_SHM_MEM
- name: metrics-volume
  emptyDir: {}
- name: torch-compile-cache
  emptyDir: {}
- name: triton-cache
  emptyDir: {}
EOF

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/ms-kv-events/values.yaml
# Line 129:
#   prefill:
#     create: false
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_REPLICAS=0

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/gaie-kv-events/values.yaml
# Lines 1-10:
#   inferenceExtension:
#     replicas: 1
#     flags:
#       kv-cache-usage-percentage-metric: "vllm:kv_cache_usage_perc"
#       v: 4
#     image:
#       name: llm-d-inference-scheduler
#       hub: ghcr.io/llm-d
#       tag: v0.5.0
#       pullPolicy: Always
# =============================================================================
export LLMDBENCH_LLMD_INFERENCESCHEDULER_IMAGE_NAME="llm-d-inference-scheduler"
export LLMDBENCH_LLMD_INFERENCESCHEDULER_IMAGE_REGISTRY="ghcr.io"
export LLMDBENCH_LLMD_INFERENCESCHEDULER_IMAGE_REPO="llm-d"
export LLMDBENCH_LLMD_INFERENCESCHEDULER_IMAGE_TAG="v0.5.0"

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/gaie-kv-events/values.yaml
# Lines 3-5:
#   flags:
#     kv-cache-usage-percentage-metric: "vllm:kv_cache_usage_perc"
#     v: 4
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_FLAGS=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_GAIE_FLAGS
kv-cache-usage-percentage-metric: "vllm:kv_cache_usage_perc"
v: 4
EOF

# =============================================================================
# SOURCE: guides/precise-prefix-cache-aware/gaie-kv-events/values.yaml
# Lines 69-107:
#   pluginsConfigFile: "precise-prefix-cache-config.yaml"
#   pluginsCustomConfig:
#     precise-prefix-cache-config.yaml: |
#       apiVersion: inference.networking.x-k8s.io/v1alpha1
#       kind: EndpointPickerConfig
#       plugins:
#         - type: single-profile-handler
#         - type: precise-prefix-cache-scorer
#           parameters:
#             tokenProcessorConfig:
#               blockSize: 64
#             indexerConfig:
#               tokenizersPoolConfig:
#                 modelName: Qwen/Qwen3-32B
#                 local: null
#                 hf: null
#                 uds:
#                   socketFile: /tmp/tokenizer/tokenizer-uds.socket
#             kvEventsConfig:
#               topicFilter: "kv@"
#               concurrency: 4
#               discoverPods: false
#               zmqEndpoint: "tcp://*:5557"
#         - type: kv-cache-utilization-scorer
#         - type: queue-scorer
#         - type: max-score-picker
#       schedulingProfiles:
#         - name: default
#           plugins:
#             - pluginRef: precise-prefix-cache-scorer
#               weight: 3.0
#             - pluginRef: kv-cache-utilization-scorer
#               weight: 2.0
#             - pluginRef: queue-scorer
#               weight: 2.0
#             - pluginRef: max-score-picker
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_PLUGINS_CONFIGFILE="precise-prefix-cache-config.yaml"

export LLMDBENCH_VLLM_MODELSERVICE_GAIE_CUSTOM_PLUGINS=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_GAIE_CUSTOM_PLUGINS
precise-prefix-cache-config.yaml: |
  apiVersion: inference.networking.x-k8s.io/v1alpha1
  kind: EndpointPickerConfig
  plugins:
    - type: single-profile-handler
    - type: precise-prefix-cache-scorer
      parameters:
        tokenProcessorConfig:
          blockSize: 64
        indexerConfig:
          tokenizersPoolConfig:
            modelName: Qwen/Qwen3-32B
            local: null
            hf: null
            uds:
              socketFile: /tmp/tokenizer/tokenizer-uds.socket
        kvEventsConfig:
          topicFilter: "kv@"
          concurrency: 4
          discoverPods: false
          zmqEndpoint: "tcp://*:5557"
    - type: kv-cache-utilization-scorer
    - type: queue-scorer
    - type: max-score-picker
  schedulingProfiles:
    - name: default
      plugins:
        - pluginRef: precise-prefix-cache-scorer
          weight: 3.0
        - pluginRef: kv-cache-utilization-scorer
          weight: 2.0
        - pluginRef: queue-scorer
          weight: 2.0
        - pluginRef: max-score-picker
EOF

# =============================================================================
# SOURCE: Benchmark framework defaults
# =============================================================================
export LLMDBENCH_HARNESS_NAME=inference-perf
export LLMDBENCH_HARNESS_EXPERIMENT_PROFILE=sanity_random.yaml
export LLMDBENCH_CONTROL_WORK_DIR=~/data/precise-prefix-cache-aware
