# TIERED PREFIX CACHE (LMCACHE-CONNECTOR STORAGE) WELL LIT PATH
# Based on https://github.com/llm-d/llm-d/tree/main/guides/tiered-prefix-cache/storage/manifests/vllm/lmcache-connector
# Generated by llm-d-benchmark guide converter
#
# This guide demonstrates tiered prefix caching with LMCache connector:
# - HBM (GPU memory) as L1 cache
# - CPU RAM as L2 cache (200GB)
# - Shared storage as L3 cache (18TB, using PVC)
# - Direct I/O enabled for storage operations
#
# Kustomize Hierarchy:
# guides/tiered-prefix-cache/storage/manifests/vllm/lmcache-connector/kustomization.yaml
#     └── ../../../../cpu/manifests/vllm/lmcache-connector/kustomization.yaml
#             └── ../base/kustomization.yaml
#                     └── ../../../../../recipes/vllm/standard (base resources)

# =============================================================================
# SOURCE: guides/tiered-prefix-cache/cpu/manifests/vllm/lmcache-connector/kustomization.yaml
# Lines 19:
#   - "Qwen/Qwen3-32B"
# =============================================================================
export LLMDBENCH_DEPLOY_MODEL_LIST="Qwen/Qwen3-32B"

# =============================================================================
# SOURCE: guides/tiered-prefix-cache/cpu/manifests/vllm/lmcache-connector/kustomization.yaml
# Lines 12-13:
#   - op: replace
#     path: /spec/template/spec/containers/0/image
#     value: ghcr.io/llm-d/llm-d-cuda:v0.5.0
#
# NOTE: Guide specifies explicit v0.5.0 image tag
# =============================================================================
export LLMDBENCH_LLMD_IMAGE_TAG="v0.5.0"

# =============================================================================
# SOURCE: guides/tiered-prefix-cache/cpu/manifests/vllm/lmcache-connector/kustomization.yaml
# Lines 50-51:
#   requests:
#     memory: 400G
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_MEM=400Gi

# =============================================================================
# SOURCE: guides/tiered-prefix-cache/cpu/manifests/vllm/lmcache-connector/kustomization.yaml
# Lines 20-21:
#   - "--tensor-parallel-size"
#   - "2"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM=2

# =============================================================================
# SOURCE: guides/tiered-prefix-cache/cpu/manifests/vllm/lmcache-connector/kustomization.yaml
# Lines 29-30:
#   - "--gpu-memory-utilization"
#   - "0.8"
# =============================================================================
export LLMDBENCH_VLLM_COMMON_ACCELERATOR_MEM_UTIL=0.8

# =============================================================================
# SOURCE: Benchmark framework convention (not in guide)
# Using custom command to inject preprocessing and exact vLLM args
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_MODEL_COMMAND=custom
export LLMDBENCH_VLLM_COMMON_PREPROCESS="python3 /setup/preprocess/set_llmdbench_environment.py; source \$HOME/llmdbench_env.sh"
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS=$LLMDBENCH_VLLM_COMMON_PREPROCESS

# =============================================================================
# SOURCE: guides/tiered-prefix-cache/cpu/manifests/vllm/lmcache-connector/kustomization.yaml
# Lines 17-30 (container command and args):
#   - op: replace
#     path: /spec/template/spec/containers/0/args
#     value:
#       - "serve"
#       - "Qwen/Qwen3-32B"
#       - "--tensor-parallel-size"
#       - "2"
#       - "--port"
#       - "8000"
#       - "--max-num-seq"
#       - "1024"
#       - "--kv-transfer-config"
#       - '{"kv_connector":"LMCacheConnectorV1","kv_role":"kv_both"}'
#       - "--enable_prefix_caching"
#       - "--gpu-memory-utilization"
#       - "0.8"
#
# NOTE: Port mapping - Guide uses 8000 but benchmark uses proxy pattern:
#   - INFERENCE_PORT (8000): proxy/sidecar listens here
#   - METRICS_PORT (8200): vLLM actually listens here
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS
REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS; \
vllm serve /model-cache/models/REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--host 0.0.0.0 \
--served-model-name REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--port REPLACE_ENV_LLMDBENCH_VLLM_COMMON_METRICS_PORT \
--tensor-parallel-size REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM \
--max-num-seq 1024 \
--kv-transfer-config '{"kv_connector":"LMCacheConnectorV1","kv_role":"kv_both"}' \
--enable_prefix_caching \
--gpu-memory-utilization REPLACE_ENV_LLMDBENCH_VLLM_COMMON_ACCELERATOR_MEM_UTIL
EOF

# =============================================================================
# SOURCE: Multiple kustomization patches for environment variables
#
# From guides/tiered-prefix-cache/cpu/manifests/vllm/lmcache-connector/kustomization.yaml:
# Lines 33-35: LMCACHE_MAX_LOCAL_CPU_SIZE: 200.0
# Lines 37-39: PYTHONHASHSEED: 123
# Lines 41-43: PROMETHEUS_MULTIPROC_DIR: /tmp/lmcache_prometheus
#
# From guides/tiered-prefix-cache/storage/manifests/vllm/lmcache-connector/kustomization.yaml:
# Lines 10-13: LMCACHE_LOCAL_DISK: file:///mnt/files-storage/
# Lines 15-18: LMCACHE_EXTRA_CONFIG: {"use_odirect":"True"}
# Lines 24-27: LMCACHE_MAX_LOCAL_DISK_SIZE: 18000.0
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_TO_YAML=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_TO_YAML
- name: LMCACHE_MAX_LOCAL_CPU_SIZE
  value: "200.0"
- name: PYTHONHASHSEED
  value: "123"
- name: PROMETHEUS_MULTIPROC_DIR
  value: "/tmp/lmcache_prometheus"
- name: LMCACHE_LOCAL_DISK
  value: "file:///mnt/files-storage/"
- name: LMCACHE_EXTRA_CONFIG
  value: "{\"use_odirect\":\"True\"}"
- name: LMCACHE_MAX_LOCAL_DISK_SIZE
  value: "18000.0"
EOF

# =============================================================================
# SOURCE: Multiple kustomization patches for volume mounts
#
# From guides/tiered-prefix-cache/cpu/manifests/vllm/lmcache-connector/kustomization.yaml:
# Lines 64-66: lmcache-prometheus-metrics mount at /tmp/lmcache_prometheus
#
# From guides/tiered-prefix-cache/storage/manifests/vllm/lmcache-connector/kustomization.yaml:
# Lines 46-50: files-storage mount at /mnt/files-storage
#
# PLUS benchmark framework standard mounts (preprocesses, dshm)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS}
- name: dshm
  mountPath: /dev/shm
- name: preprocesses
  mountPath: /setup/preprocess
- name: lmcache-prometheus-metrics
  mountPath: /tmp/lmcache_prometheus
- name: files-storage
  mountPath: /mnt/files-storage
EOF

# =============================================================================
# SOURCE: Multiple kustomization patches for volumes
#
# From guides/tiered-prefix-cache/cpu/manifests/vllm/lmcache-connector/kustomization.yaml:
# Lines 59-61: lmcache-prometheus-metrics emptyDir volume
#
# From guides/tiered-prefix-cache/storage/manifests/vllm/lmcache-connector/kustomization.yaml:
# Lines 35-39: files-storage PVC volume referencing llm-d-kv-cache-storage
#
# PLUS benchmark framework standard volumes (preprocesses, dshm)
# NOTE: preprocesses configMap must be listed first
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES}
- name: preprocesses
  configMap:
    defaultMode: 0755
    name: llm-d-benchmark-preprocesses
- name: dshm
  emptyDir:
    medium: Memory
    sizeLimit: REPLACE_ENV_LLMDBENCH_VLLM_COMMON_SHM_MEM
- name: lmcache-prometheus-metrics
  emptyDir: {}
- name: files-storage
  persistentVolumeClaim:
    claimName: llm-d-kv-cache-storage
EOF

# =============================================================================
# SOURCE: guides/tiered-prefix-cache/storage/manifests/vllm/lmcache-connector/kustomization.yaml
# Lines 58-65: Init container to fix permissions on shared storage
#   - op: add
#     path: /spec/template/spec/initContainers
#     value:
#       - name: fix-lustre-permissions
#         image: busybox:latest
#         command: ["sh", "-c", "chmod -R g+w /mnt/files-storage/"]
#         volumeMounts:
#         - name: files-storage
#           mountPath: /mnt/files-storage
#
# NOTE: This init container grants group write permissions to ensure the vLLM
#       user can write to the shared storage PVC
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_POD_CONFIG=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_POD_CONFIG}
initContainers:
  - name: fix-lustre-permissions
    image: busybox:latest
    command: ["sh", "-c", "chmod -R g+w /mnt/files-storage/"]
    volumeMounts:
    - name: files-storage
      mountPath: /mnt/files-storage
EOF

# =============================================================================
# SOURCE: Guide does not use P/D disaggregation
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_REPLICAS=0

# =============================================================================
# SOURCE: Benchmark framework defaults
# =============================================================================
export LLMDBENCH_HARNESS_NAME=inference-perf
export LLMDBENCH_HARNESS_EXPERIMENT_PROFILE=sanity_random.yaml
export LLMDBENCH_CONTROL_WORK_DIR=~/data/tiered-prefix-cache-lmcache-storage

# =============================================================================
# IMPORTANT PREREQUISITES:
#
# 1. Create shared storage PVC BEFORE deploying:
#    The guide references PVC name "llm-d-kv-cache-storage" (see lines 35-39 of
#    storage/manifests/vllm/lmcache-connector/kustomization.yaml)
#
# 2. Ensure your cluster has a storage class that supports ReadWriteMany (RWX):
#    - Default storage class with RWX support, OR
#    - GCP Lustre (see guide for setup), OR
#    - IBM Storage Scale, CephFS, AWS Lustre, etc.
#
# 3. The PVC must provide adequate storage for the disk cache tier
#    (LMCACHE_MAX_LOCAL_DISK_SIZE is set to 18000.0 GB = 18TB)
#
# 4. Storage backend should support direct I/O for best performance
#    (LMCACHE_EXTRA_CONFIG enables use_odirect)
#
# =============================================================================
