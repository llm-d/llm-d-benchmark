# WIDE EXPERT PARALLELISM (EP/DP) WITH LEADERWORKERSET - COREWEAVE
# Based on https://github.com/llm-d/llm-d/tree/main/guides/wide-ep-lws
# Infrastructure: CoreWeave (32x H200 with InfiniBand)
# Generated by llm-d-benchmark guide converter

# NOTE: This guide uses LeaderWorkerSet CRD, not llm-d modelservice Helm chart.
# The configuration is extracted from the kustomize manifests but may not be
# directly compatible with the llm-d-benchmark standup process which assumes
# the modelservice approach. This scenario captures the vLLM configuration
# for reference and potential adaptation.

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Lines 6-10:
#   labels:
#     llm-d.ai/inference-serving: "true"
#     llm-d.ai/guide: "wide-ep-lws"
#     llm-d.ai/model: DeepSeek-R1-0528
#     llm-d.ai/role: decode
# =============================================================================
# validated
export LLMDBENCH_DEPLOY_MODEL_LIST="deepseek-ai/DeepSeek-R1-0528"

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Line 66: image: ghcr.io/llm-d/llm-d-cuda:v0.5.0
# =============================================================================
export LLMDBENCH_LLMD_IMAGE_TAG="v0.5.0"

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/inferencepool.values.yaml
# Lines 6-8:
#   image:
#     name: llm-d-inference-scheduler
#     tag: v0.5.0
# =============================================================================
export LLMDBENCH_LLMD_INFERENCESCHEDULER_IMAGE_TAG="v0.5.0"

# =============================================================================
# SOURCE: guides/wide-ep-lws/README.md
# Model size requires substantial storage for caching
# =============================================================================
# validated
export LLMDBENCH_VLLM_COMMON_PVC_MODEL_CACHE_SIZE="300Gi"

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Line 211: memory: 512Gi
# =============================================================================
# validated
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_MEM="512Gi"

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Line 209: cpu: 32
# =============================================================================
# validated
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_NR="32"

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Line 56: sizeLimit: 2Gi
# =============================================================================
# validation issue: different value
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_SHM_MEM="2Gi"

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Line 14: replicas: 1
# Plus Lines 15-16: size: 2 (LeaderWorkerSet creates 2 pods per replica)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_REPLICAS=1

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Line 122: DP_SIZE_LOCAL = 8
# Line 101: --tensor-parallel-size $TP_SIZE (where TP_SIZE=1 from line 124)
# =============================================================================
# validated
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM=1
# validated
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_DATA_LOCAL_PARALLELISM=8

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Line 213: nvidia.com/gpu: "8"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_ACCELERATOR_NR=8

# =============================================================================
# SOURCE: Benchmark framework convention (not in guide)
# Using custom command to inject preprocessing and exact vLLM args
# =============================================================================
# validated
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_MODEL_COMMAND=custom
# validated
export LLMDBENCH_VLLM_COMMON_PREPROCESS="python3 /setup/preprocess/set_llmdbench_environment.py; source \$HOME/llmdbench_env.sh"
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS=$LLMDBENCH_VLLM_COMMON_PREPROCESS

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Lines 90-115: vllm serve command and arguments
#
# NOTE: The guide uses LeaderWorkerSet with special LWS_* environment variables
# for multi-pod coordination. The benchmark framework doesn't currently support
# this pattern. This EXTRA_ARGS captures the core vLLM flags for reference.
#
# Port mapping: Guide uses 8200 for vLLM (with routing-proxy on 8000)
# Benchmark uses proxy pattern with INFERENCE_PORT=8000, METRICS_PORT=8200
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS
REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS; \
vllm serve /model-cache/models/REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--host 0.0.0.0 \
--served-model-name REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--port REPLACE_ENV_LLMDBENCH_VLLM_COMMON_METRICS_PORT \
--trust-remote-code \
--disable-uvicorn-access-log \
--data-parallel-hybrid-lb \
--enable-expert-parallel \
--tensor-parallel-size 1 \
--async-scheduling \
--enable-dbo \
--dbo-decode-token-threshold 32 \
--enable-eplb \
--eplb-config '{"window_size":"1000","step_interval":"3000","num_redundant_experts":"32","log_balancedness":"False"}' \
--compilation_config '{"cudagraph_mode": "FULL_DECODE_ONLY"}' \
--kv_transfer_config '{"kv_connector":"NixlConnector","kv_role":"kv_both","kv_load_failure_policy":"fail"}'
EOF

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Lines 117-180: Environment variables
# Plus guides/wide-ep-lws/manifests/modelserver/coreweave/kustomization.yaml
# Lines 21-33: Additional CoreWeave-specific environment variables
# =============================================================================
# validation issues
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_TO_YAML=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_TO_YAML
#### new env var
- name: VLLM_MOE_DP_CHUNK_SIZE
  value: "384"
- name: DP_SIZE_LOCAL
  value: "8"
- name: TP_SIZE
  value: "1"
- name: TRITON_LIBCUDA_PATH
  value: /usr/lib64
- name: VLLM_SKIP_P2P_CHECK
  value: "1"
- name: VLLM_RANDOMIZE_DP_DUMMY_INPUTS
  value: "1"
- name: VLLM_USE_DEEP_GEMM
  value: "1"
#### different value from COMMON
- name: VLLM_ALL2ALL_BACKEND
  value: deepep_low_latency
- name: NVIDIA_GDRCOPY
  value: enabled
- name: NVSHMEM_REMOTE_TRANSPORT
  value: ibgda
- name: NVSHMEM_IB_ENABLE_IBGDA
  value: "true"
- name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
  value: eth0
- name: GLOO_SOCKET_IFNAME
  value: eth0
- name: NCCL_SOCKET_IFNAME
  value: eth0
- name: VLLM_LOGGING_LEVEL
  value: INFO
- name: CUDA_CACHE_PATH
  value: /var/cache/vllm/cuda
- name: CCACHE_DIR
  value: /var/cache/vllm/ccache
- name: VLLM_CACHE_ROOT
  value: /var/cache/vllm/vllm
- name: FLASHINFER_WORKSPACE_BASE
  value: /var/cache/vllm/flashinfer
- name: HF_HUB_CACHE
  value: /var/cache/huggingface
- name: HF_HUB_DISABLE_XET
  value: "1"
- name: NCCL_IB_HCA
  value: "ibp"
- name: NVSHMEM_HCA_PREFIX
  value: "ibp"
EOF

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Lines 52-59, 218-221: Volume definitions and mounts
# Plus benchmark framework standard volumes (preprocesses, etc.)
# =============================================================================
# validation issues
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES}
- name: preprocesses
  configMap:
    defaultMode: 0755
    name: llm-d-benchmark-preprocesses
- name: dshm
  emptyDir:
    medium: Memory
    sizeLimit: REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_SHM_MEM
#### added these and are of different type than kustomize
- name: hf-cache
  emptyDir: {}
- name: jit-cache
  emptyDir: {}
EOF

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Lines 218-221: Volume mounts
# Plus benchmark framework standard mounts (preprocesses)
# =============================================================================
# validation issues
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS}
- name: preprocesses
  mountPath: /setup/preprocess
- name: dshm
  mountPath: /dev/shm
#### added these
- name: hf-cache
  mountPath: /var/cache/huggingface
- name: jit-cache
  mountPath: /var/cache/vllm
EOF

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/coreweave/kustomization.yaml
# Lines 34-44: RDMA/InfiniBand resource requirements for CoreWeave
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_CONTAINER_CONFIG=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_CONTAINER_CONFIG}
{
  "resources": {
    "limits": {
      "rdma/ib": "1"
    },
    "requests": {
      "rdma/ib": "1"
    }
  },
  "securityContext": {
    "capabilities": {
      "add": ["IPC_LOCK", "SYS_RAWIO"]
    },
    "runAsGroup": 0,
    "runAsUser": 0
  }
}
EOF

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/coreweave/kustomization.yaml
# Lines 45-54: Node affinity for H200 GPUs
# =============================================================================
export LLMDBENCH_VLLM_COMMON_AFFINITY=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_COMMON_AFFINITY}
nodeAffinity:
  requiredDuringSchedulingIgnoredDuringExecution:
    nodeSelectorTerms:
      - matchExpressions:
        - key: gpu.nvidia.com/model
          operator: In
          values:
            - H200
EOF

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/coreweave/kustomization.yaml
# Lines 55-56: Custom scheduler for CoreWeave
# =============================================================================
# validated
export LLMDBENCH_VLLM_COMMON_POD_SCHEDULER="custom-binpack-scheduler"

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/prefill.yaml
# Configuration for prefill stage (P/D disaggregation)
# =============================================================================
# validated
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_REPLICAS=1
# validated
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_TENSOR_PARALLELISM=1
# validated
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_DATA_LOCAL_PARALLELISM=8
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_ACCELERATOR_NR=8
# validated
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_CPU_NR="32"
# validated
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_CPU_MEM="512Gi"
# validation issue different value
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_SHM_MEM="2Gi"

# validated
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_MODEL_COMMAND=custom
# validated
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_PREPROCESS=$LLMDBENCH_VLLM_COMMON_PREPROCESS

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/prefill.yaml
# Lines 56-76: vllm serve command for prefill
# =============================================================================
#### validation issue exec vllm; env var; replaces REPLACE_ENV... NOT ALL ARGS
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_ARGS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_ARGS
REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_PREFILL_PREPROCESS; \
vllm serve /model-cache/models/REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--host 0.0.0.0 \
--served-model-name REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--port REPLACE_ENV_LLMDBENCH_VLLM_COMMON_METRICS_PORT \
--trust-remote-code \
--disable-uvicorn-access-log \
--data-parallel-hybrid-lb \
--enable-expert-parallel \
--tensor-parallel-size 1 \
--async-scheduling \
--enable-dbo \
--dbo-prefill-token-threshold 32 \
--enable-eplb \
--eplb-config '{"window_size":"1000","step_interval":"3000","num_redundant_experts":"32","log_balancedness":"False"}' \
--gpu-memory-utilization 0.75 \
--kv_transfer_config '{"kv_connector":"NixlConnector","kv_role":"kv_both","kv_load_failure_policy":"fail"}'
EOF

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/prefill.yaml
# Lines 78-137: Environment variables for prefill (similar to decode)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_ENVVARS_TO_YAML=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_PREFILL_ENVVARS_TO_YAML
- name: DP_SIZE_LOCAL
  value: "8"
- name: TP_SIZE
  value: "1"
- name: TRITON_LIBCUDA_PATH
  value: /usr/lib64
- name: VLLM_SKIP_P2P_CHECK
  value: "1"
- name: VLLM_RANDOMIZE_DP_DUMMY_INPUTS
  value: "1"
- name: VLLM_USE_DEEP_GEMM
  value: "1"
- name: VLLM_ALL2ALL_BACKEND
  value: deepep_high_throughput
- name: NVIDIA_GDRCOPY
  value: enabled
- name: NVSHMEM_REMOTE_TRANSPORT
  value: ibgda
- name: NVSHMEM_IB_ENABLE_IBGDA
  value: "true"
- name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
  value: eth0
- name: GLOO_SOCKET_IFNAME
  value: eth0
- name: NCCL_SOCKET_IFNAME
  value: eth0
- name: VLLM_LOGGING_LEVEL
  value: INFO
- name: CUDA_CACHE_PATH
  value: /var/cache/vllm/cuda
- name: CCACHE_DIR
  value: /var/cache/vllm/ccache
- name: VLLM_CACHE_ROOT
  value: /var/cache/vllm/vllm
- name: FLASHINFER_WORKSPACE_BASE
  value: /var/cache/vllm/flashinfer
- name: HF_HUB_CACHE
  value: /var/cache/huggingface
- name: HF_HUB_DISABLE_XET
  value: "1"
- name: NCCL_IB_HCA
  value: "ibp"
- name: NVSHMEM_HCA_PREFIX
  value: "ibp"
EOF

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/prefill.yaml
# Lines 40-47, 176-179: Volumes and mounts (same pattern as decode)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUMES=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUMES}
- name: preprocesses
  configMap:
    defaultMode: 0755
    name: llm-d-benchmark-preprocesses
- name: dshm
  emptyDir:
    medium: Memory
    sizeLimit: REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_PREFILL_SHM_MEM
- name: hf-cache
  emptyDir: {}
- name: jit-cache
  emptyDir: {}
EOF

export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUME_MOUNTS=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUME_MOUNTS}
- name: preprocesses
  mountPath: /setup/preprocess
- name: dshm
  mountPath: /dev/shm
- name: hf-cache
  mountPath: /var/cache/huggingface
- name: jit-cache
  mountPath: /var/cache/vllm
EOF

# =============================================================================
# SOURCE: CoreWeave infrastructure (same RDMA/security config as decode)
# =============================================================================
#### validation issue -  these should be elsewhere
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_CONTAINER_CONFIG=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_CONTAINER_CONFIG}
{
  "resources": {
    "limits": {
      "rdma/ib": "1"
    },
    "requests": {
      "rdma/ib": "1"
    }
  },
  "securityContext": {
    "capabilities": {
      "add": ["IPC_LOCK", "SYS_RAWIO"]
    },
    "runAsGroup": 0,
    "runAsUser": 0
  }
}
EOF

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/inferencepool.values.yaml
# Lines 13-15: GAIE plugins for P/D routing
# =============================================================================
# validated
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_PLUGINS_CONFIGFILE="custom-plugins.yaml"

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/inferencepool.values.yaml
# Lines 16-49: Custom GAIE plugin configuration for P/D disaggregation
# =============================================================================
# validated
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_CUSTOM_PLUGINS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_GAIE_CUSTOM_PLUGINS
custom-plugins.yaml: |
  apiVersion: inference.networking.x-k8s.io/v1alpha1
  kind: EndpointPickerConfig
  plugins:
  - type: prefill-header-handler
  - type: prefill-filter
  - type: decode-filter
  - type: random-picker
    parameters:
      maxNumOfEndpoints: 1
  - type: pd-profile-handler
    parameters:
      threshold: 0
      hashBlockSize: 5
  schedulingProfiles:
  - name: prefill
    plugins:
    - pluginRef: prefill-filter
    - pluginRef: random-picker
  - name: decode
    plugins:
    - pluginRef: decode-filter
    - pluginRef: random-picker
EOF

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/inferencepool.values.yaml
# Lines 3-6: GAIE flags
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_FLAGS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_GAIE_FLAGS
kv-cache-usage-percentage-metric: "vllm:kv_cache_usage_perc"
v: 1
EOF

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/inferencepool.values.yaml
# Lines 59-61: Gateway provider configuration
# =============================================================================
#### validation issue -- why is the default
export LLMDBENCH_VLLM_MODELSERVICE_GATEWAY_CLASS_NAME="istio"

# =============================================================================
# SOURCE: Benchmark framework defaults
# =============================================================================
export LLMDBENCH_HARNESS_NAME=inference-perf
export LLMDBENCH_HARNESS_EXPERIMENT_PROFILE=sanity_random.yaml
# validated
export LLMDBENCH_CONTROL_WORK_DIR=~/data/wide-ep-lws-coreweave


LLMDBENCH_VLLM_MODELSERVICE_INFERENCE_POOL_PROVIDER_CONFIG
LLMDBENCH_VLLM_COMMON_MAX_MODEL_LEN
LLMDBENCH_VLLM_COMMON_BLOCK_SIZE
LLMDBENCH_VLLM_COMMON_NUM_WORKERS_PARALLELISM
LLMDBENCH_VLLM_COMMON_EPHEMERAL_STORAGE
LLMDBENCH_VLLM_COMMON_ACCELERATOR_MEM_UTIL
LLMDBENCH_VLLM_COMMON_NETWORK_RESOURCE

LLMDBENCH_VLLM_MODELSERVICE_MULTINODE
LLMDBENCH_VLLM_MODELSERVICE_MOUNT_MODEL_VOLUME_OVERRIDE
LLMDBENCH_VLLM_MODELSERVICE_PREFILL_ACCELERATOR_NR