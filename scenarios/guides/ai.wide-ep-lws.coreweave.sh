# WIDE-EP-LWS (COREWEAVE) WELL LIT PATH
# Based on https://github.com/llm-d/llm-d/tree/main/guides/wide-ep-lws
# Variant: CoreWeave (H200 GPUs with RDMA networking and custom scheduler)
# Generated by llm-d-benchmark guide converter

# Kustomize Hierarchy:
# guides/wide-ep-lws/manifests/modelserver/coreweave/kustomization.yaml
#     └── ../base/kustomization.yaml
#             └── decode.yaml, prefill.yaml, serviceAccount.yaml

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Line 8:
#   llm-d.ai/model: DeepSeek-R1-0528
# =============================================================================
export LLMDBENCH_DEPLOY_MODEL_LIST="deepseek-ai/DeepSeek-R1-0528"

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Line 63:
#   image: ghcr.io/llm-d/llm-d-cuda:v0.5.0
# =============================================================================
export LLMDBENCH_LLMD_IMAGE_TAG="v0.5.0"

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/inferencepool.values.yaml
# Lines 7-9:
#   image:
#     name: llm-d-inference-scheduler
#     tag: v0.5.0
# =============================================================================
export LLMDBENCH_LLMD_INFERENCESCHEDULER_IMAGE_TAG="v0.5.0"

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Line 38:
#   image: ghcr.io/llm-d/llm-d-routing-sidecar:v0.5.0
# =============================================================================
export LLMDBENCH_LLMD_ROUTINGSIDECAR_IMAGE_TAG="v0.5.0"

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Lines 13-14:
#   spec:
#     replicas: 1
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_REPLICAS=1

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/prefill.yaml
# Lines 13-14:
#   spec:
#     replicas: 1
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_REPLICAS=1

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Lines 1-2, 15-16:
#   apiVersion: leaderworkerset.x-k8s.io/v1
#   kind: LeaderWorkerSet
#   leaderWorkerTemplate:
#     size: 2
#
# NOTE: LeaderWorkerSet with size=2 means 1 leader + 1 worker = 2 pods per group
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_MULTINODE=true
export LLMDBENCH_VLLM_COMMON_NUM_WORKERS_PARALLELISM=2

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Lines 125-126:
#   - name: DP_SIZE_LOCAL
#     value: "8"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_DATA_LOCAL_PARALLELISM=8

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Lines 127-128:
#   - name: TP_SIZE
#     value: "1"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM=1

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/prefill.yaml
# Lines 88-89:
#   - name: DP_SIZE_LOCAL
#     value: "8"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_DATA_LOCAL_PARALLELISM=8

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/prefill.yaml
# Lines 90-91:
#   - name: TP_SIZE
#     value: "1"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_TENSOR_PARALLELISM=1

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Lines 184-187:
#   requests:
#     cpu: 32
#     memory: 512Gi
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_NR=32
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_MEM=512Gi

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/prefill.yaml
# Lines 166-169:
#   requests:
#     cpu: 32
#     memory: 512Gi
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_CPU_NR=32
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_CPU_MEM=512Gi

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Lines 29-31:
#   emptyDir:
#     medium: Memory
#     sizeLimit: 2Gi
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_SHM_MEM=2Gi

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/prefill.yaml
# Lines 27-29:
#   emptyDir:
#     medium: Memory
#     sizeLimit: 2Gi
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_SHM_MEM=2Gi

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/coreweave/kustomization.yaml
# Lines 34-38:
#   - op: add
#     path: /spec/leaderWorkerTemplate/workerTemplate/spec/containers/0/resources/limits/rdma~1ib
#     value: "1"
#
# NOTE: CoreWeave uses rdma/ib resource type
# =============================================================================
export LLMDBENCH_VLLM_COMMON_NETWORK_RESOURCE="rdma/ib"
export LLMDBENCH_VLLM_COMMON_NETWORK_NR=1

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/coreweave/kustomization.yaml
# Lines 54-55:
#   - op: add
#     path: /spec/leaderWorkerTemplate/workerTemplate/spec/schedulerName
#     value: custom-binpack-scheduler
# =============================================================================
export LLMDBENCH_VLLM_COMMON_POD_SCHEDULER="custom-binpack-scheduler"

# =============================================================================
# SOURCE: Benchmark framework convention (not in guide)
# Using custom command to inject preprocessing and exact vLLM args
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_MODEL_COMMAND=custom
export LLMDBENCH_VLLM_COMMON_PREPROCESS="python3 /setup/preprocess/set_llmdbench_environment.py; source \$HOME/llmdbench_env.sh"
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS=$LLMDBENCH_VLLM_COMMON_PREPROCESS

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Lines 76-119 (container args):
#   vllm serve deepseek-ai/DeepSeek-R1-0528 \
#     --port 8200 \
#     --trust-remote-code \
#     --disable-uvicorn-access-log \
#     --data-parallel-hybrid-lb \
#     --enable-expert-parallel \
#     --tensor-parallel-size $TP_SIZE \
#     --data-parallel-size $((LWS_GROUP_SIZE * DP_SIZE_LOCAL)) \
#     --data-parallel-size-local $DP_SIZE_LOCAL \
#     --kv_transfer_config '{"kv_connector":"NixlConnector","kv_role":"kv_both","kv_load_failure_policy":"fail"}' \
#     --async-scheduling \
#     --enable-dbo \
#     --dbo-decode-token-threshold 32 \
#     --enable-eplb \
#     --eplb-config '{"window_size":"1000","step_interval":"3000","num_redundant_experts":"32","log_balancedness":"False"}' \
#     --compilation_config '{"cudagraph_mode": "FULL_DECODE_ONLY"}' \
#     --kv-cache-memory-bytes=${KV_CACHE_MEMORY_BYTES-}
#
# NOTE: LWS-specific args (--data-parallel-address, --data-parallel-rpc-port, --data-parallel-start-rank)
# are handled automatically by the modelservice Helm chart when multinode=true
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS
REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS; \
vllm serve /model-cache/models/REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--host 0.0.0.0 \
--served-model-name REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--port REPLACE_ENV_LLMDBENCH_VLLM_COMMON_METRICS_PORT \
--trust-remote-code \
--disable-uvicorn-access-log \
--data-parallel-hybrid-lb \
--enable-expert-parallel \
--tensor-parallel-size REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM \
--kv_transfer_config '{"kv_connector":"NixlConnector","kv_role":"kv_both","kv_load_failure_policy":"fail"}' \
--async-scheduling \
--enable-dbo \
--dbo-decode-token-threshold 32 \
--enable-eplb \
--eplb-config '{"window_size":"1000","step_interval":"3000","num_redundant_experts":"32","log_balancedness":"False"}' \
--compilation_config '{"cudagraph_mode": "FULL_DECODE_ONLY"}'
EOF

# =============================================================================
# SOURCE: Benchmark framework convention (not in guide)
# Using custom command to inject preprocessing and exact vLLM args
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_MODEL_COMMAND=custom
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_PREPROCESS=$LLMDBENCH_VLLM_COMMON_PREPROCESS

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/prefill.yaml
# Lines 55-96 (container args):
#   vllm serve deepseek-ai/DeepSeek-R1-0528 \
#     --port 8000 \
#     --trust-remote-code \
#     --disable-uvicorn-access-log \
#     --data-parallel-hybrid-lb \
#     --enable-expert-parallel \
#     --tensor-parallel-size $TP_SIZE \
#     --data-parallel-size $((LWS_GROUP_SIZE * DP_SIZE_LOCAL)) \
#     --data-parallel-size-local $DP_SIZE_LOCAL \
#     --kv_transfer_config '{"kv_connector":"NixlConnector","kv_role":"kv_both","kv_load_failure_policy":"fail"}' \
#     --async-scheduling \
#     --enable-dbo \
#     --dbo-prefill-token-threshold 32 \
#     --enable-eplb \
#     --eplb-config '{"window_size":"1000","step_interval":"3000","num_redundant_experts":"32","log_balancedness":"False"}' \
#     --gpu-memory-utilization 0.75
#
# NOTE: Prefill uses --port 8000 (INFERENCE_PORT) directly without routing sidecar
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_ARGS=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_ARGS
REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_PREFILL_PREPROCESS; \
vllm serve /model-cache/models/REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--host 0.0.0.0 \
--served-model-name REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--port REPLACE_ENV_LLMDBENCH_VLLM_COMMON_INFERENCE_PORT \
--trust-remote-code \
--disable-uvicorn-access-log \
--data-parallel-hybrid-lb \
--enable-expert-parallel \
--tensor-parallel-size REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_PREFILL_TENSOR_PARALLELISM \
--kv_transfer_config '{"kv_connector":"NixlConnector","kv_role":"kv_both","kv_load_failure_policy":"fail"}' \
--async-scheduling \
--enable-dbo \
--dbo-prefill-token-threshold 32 \
--enable-eplb \
--eplb-config '{"window_size":"1000","step_interval":"3000","num_redundant_experts":"32","log_balancedness":"False"}' \
--gpu-memory-utilization 0.75
EOF

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Lines 120-165 (environment variables)
#
# PLUS guides/wide-ep-lws/manifests/modelserver/coreweave/kustomization.yaml
# Lines 20-32 (CoreWeave-specific env vars added via patches):
#   - name: HF_HUB_DISABLE_XET
#     value: "1"
#   - name: NCCL_IB_HCA
#     value: "ibp"
#   - name: NVSHMEM_HCA_PREFIX
#     value: "ibp"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_TO_YAML=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_TO_YAML
- name: VLLM_MOE_DP_CHUNK_SIZE
  value: "384"
- name: DP_SIZE_LOCAL
  value: "8"
- name: TP_SIZE
  value: "1"
- name: TRITON_LIBCUDA_PATH
  value: /usr/lib64
- name: VLLM_SKIP_P2P_CHECK
  value: "1"
- name: VLLM_RANDOMIZE_DP_DUMMY_INPUTS
  value: "1"
- name: VLLM_USE_DEEP_GEMM
  value: "1"
- name: VLLM_ALL2ALL_BACKEND
  value: deepep_low_latency
- name: NVIDIA_GDRCOPY
  value: enabled
- name: NVSHMEM_REMOTE_TRANSPORT
  value: ibgda
- name: NVSHMEM_IB_ENABLE_IBGDA
  value: "true"
- name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
  value: eth0
- name: GLOO_SOCKET_IFNAME
  value: eth0
- name: NCCL_SOCKET_IFNAME
  value: eth0
- name: VLLM_LOGGING_LEVEL
  value: INFO
- name: VLLM_NIXL_SIDE_CHANNEL_HOST
  valueFrom:
    fieldRef:
      fieldPath: status.podIP
- name: CUDA_CACHE_PATH
  value: /var/cache/vllm/cuda
- name: CCACHE_DIR
  value: /var/cache/vllm/ccache
- name: VLLM_CACHE_ROOT
  value: /var/cache/vllm/vllm
- name: FLASHINFER_WORKSPACE_BASE
  value: /var/cache/vllm/flashinfer
- name: HF_HUB_CACHE
  value: /var/cache/huggingface
- name: HF_HUB_DISABLE_XET
  value: "1"
- name: NCCL_IB_HCA
  value: "ibp"
- name: NVSHMEM_HCA_PREFIX
  value: "ibp"
EOF

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/prefill.yaml
# Lines 97-145 (environment variables)
#
# PLUS guides/wide-ep-lws/manifests/modelserver/coreweave/kustomization.yaml
# Lines 20-32 (CoreWeave-specific env vars added via patches - same as decode)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_ENVVARS_TO_YAML=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_PREFILL_ENVVARS_TO_YAML
- name: DP_SIZE_LOCAL
  value: "8"
- name: TP_SIZE
  value: "1"
- name: TRITON_LIBCUDA_PATH
  value: /usr/lib64
- name: VLLM_SKIP_P2P_CHECK
  value: "1"
- name: VLLM_RANDOMIZE_DP_DUMMY_INPUTS
  value: "1"
- name: VLLM_USE_DEEP_GEMM
  value: "1"
- name: VLLM_ALL2ALL_BACKEND
  value: deepep_high_throughput
- name: NVIDIA_GDRCOPY
  value: enabled
- name: NVSHMEM_REMOTE_TRANSPORT
  value: ibgda
- name: NVSHMEM_IB_ENABLE_IBGDA
  value: "true"
- name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
  value: eth0
- name: GLOO_SOCKET_IFNAME
  value: eth0
- name: NCCL_SOCKET_IFNAME
  value: eth0
- name: VLLM_LOGGING_LEVEL
  value: INFO
- name: VLLM_NIXL_SIDE_CHANNEL_HOST
  valueFrom:
    fieldRef:
      fieldPath: status.podIP
- name: CUDA_CACHE_PATH
  value: /var/cache/vllm/cuda
- name: CCACHE_DIR
  value: /var/cache/vllm/ccache
- name: VLLM_CACHE_ROOT
  value: /var/cache/vllm/vllm
- name: FLASHINFER_WORKSPACE_BASE
  value: /var/cache/vllm/flashinfer
- name: HF_HUB_CACHE
  value: /var/cache/huggingface
- name: HF_HUB_DISABLE_XET
  value: "1"
- name: NCCL_IB_HCA
  value: "ibp"
- name: NVSHMEM_HCA_PREFIX
  value: "ibp"
EOF

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Lines 194-199 (volume mounts):
#   - name: dshm
#     mountPath: /dev/shm
#   - name: hf-cache
#     mountPath: /var/cache/huggingface
#   - name: jit-cache
#     mountPath: /var/cache/vllm
#
# PLUS benchmark framework standard mounts
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS
- name: dshm
  mountPath: /dev/shm
- name: preprocesses
  mountPath: /setup/preprocess
- name: hf-cache
  mountPath: /var/cache/huggingface
- name: jit-cache
  mountPath: /var/cache/vllm
EOF

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/prefill.yaml
# Lines 176-181 (volume mounts):
#   - name: dshm
#     mountPath: /dev/shm
#   - name: hf-cache
#     mountPath: /var/cache/huggingface
#   - name: jit-cache
#     mountPath: /var/cache/vllm
#
# PLUS benchmark framework standard mounts
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUME_MOUNTS=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUME_MOUNTS
- name: dshm
  mountPath: /dev/shm
- name: preprocesses
  mountPath: /setup/preprocess
- name: hf-cache
  mountPath: /var/cache/huggingface
- name: jit-cache
  mountPath: /var/cache/vllm
EOF

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Lines 26-34 (volumes):
#   - name: dshm
#     emptyDir:
#       medium: Memory
#       sizeLimit: 2Gi
#   - name: hf-cache
#     emptyDir: {}
#   - name: jit-cache
#     emptyDir: {}
#
# PLUS guides/wide-ep-lws/manifests/modelserver/coreweave/kustomization.yaml
# Lines 11-24 (CoreWeave hostPath volumes):
#   - name: hf-cache
#     hostPath:
#       path: /mnt/local/hf-cache
#       type: DirectoryOrCreate
#   - name: jit-cache
#     hostPath:
#       path: /mnt/local/jit-cache
#       type: DirectoryOrCreate
#
# PLUS benchmark framework standard volumes
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES
- name: preprocesses
  configMap:
    defaultMode: 0755
    name: llm-d-benchmark-preprocesses
- name: dshm
  emptyDir:
    medium: Memory
    sizeLimit: REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_SHM_MEM
- name: hf-cache
  hostPath:
    path: /mnt/local/hf-cache
    type: DirectoryOrCreate
- name: jit-cache
  hostPath:
    path: /mnt/local/jit-cache
    type: DirectoryOrCreate
EOF

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/prefill.yaml
# Lines 24-32 (volumes):
#   - name: dshm
#     emptyDir:
#       medium: Memory
#       sizeLimit: 2Gi
#   - name: hf-cache
#     emptyDir: {}
#   - name: jit-cache
#     emptyDir: {}
#
# PLUS guides/wide-ep-lws/manifests/modelserver/coreweave/kustomization.yaml
# Lines 11-24 (CoreWeave hostPath volumes - same as decode)
#
# PLUS benchmark framework standard volumes
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUMES=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUMES
- name: preprocesses
  configMap:
    defaultMode: 0755
    name: llm-d-benchmark-preprocesses
- name: dshm
  emptyDir:
    medium: Memory
    sizeLimit: REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_PREFILL_SHM_MEM
- name: hf-cache
  hostPath:
    path: /mnt/local/hf-cache
    type: DirectoryOrCreate
- name: jit-cache
  hostPath:
    path: /mnt/local/jit-cache
    type: DirectoryOrCreate
EOF

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/coreweave/kustomization.yaml
# Lines 42-52 (node affinity patch):
#   nodeAffinity:
#     requiredDuringSchedulingIgnoredDuringExecution:
#       nodeSelectorTerms:
#         - matchExpressions:
#           - key: gpu.nvidia.com/model
#             operator: In
#             values:
#               - H200
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_POD_CONFIG=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_POD_CONFIG
affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
        - matchExpressions:
          - key: gpu.nvidia.com/model
            operator: In
            values:
              - H200
EOF

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/coreweave/kustomization.yaml
# Lines 42-52 (node affinity patch - same as decode)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_POD_CONFIG=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_POD_CONFIG
affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
        - matchExpressions:
          - key: gpu.nvidia.com/model
            operator: In
            values:
              - H200
EOF

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Lines 35-49 (routing-proxy init container):
#   initContainers:
#     - name: routing-proxy
#       args:
#         - --port=8000
#         - --vllm-port=8200
#         - --connector=nixlv2
#         - --zap-log-level=1
#         - --secure-proxy=false
#         - --enable-prefiller-sampling
# =============================================================================
export LLMDBENCH_LLMD_ROUTINGSIDECAR_ENABLED=true
export LLMDBENCH_LLMD_ROUTINGSIDECAR_CONNECTOR=nixlv2

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/inferencepool.values.yaml
# Lines 14-15:
#   pluginsConfigFile: "custom-plugins.yaml"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_PLUGINS_CONFIGFILE="custom-plugins.yaml"

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/inferencepool.values.yaml
# Lines 16-39:
#   pluginsCustomConfig:
#     custom-plugins.yaml: |
#       apiVersion: inference.networking.x-k8s.io/v1alpha1
#       kind: EndpointPickerConfig
#       plugins:
#       - type: prefill-header-handler
#       - type: prefill-filter
#       - type: decode-filter
#       - type: random-picker
#         parameters:
#           maxNumOfEndpoints: 1
#       - type: pd-profile-handler
#         parameters:
#           threshold: 0
#           hashBlockSize: 5
#       schedulingProfiles:
#       - name: prefill
#         plugins:
#         - pluginRef: prefill-filter
#         - pluginRef: random-picker
#       - name: decode
#         plugins:
#         - pluginRef: decode-filter
#         - pluginRef: random-picker
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_CUSTOM_PLUGINS=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_GAIE_CUSTOM_PLUGINS
custom-plugins.yaml: |
  # ALWAYS DO PD IN THIS EXAMPLE (THRESHOLD 0)
  # This example uses random routing
  # since it's not yet possible to route to individual DP ranks
  apiVersion: inference.networking.x-k8s.io/v1alpha1
  kind: EndpointPickerConfig
  plugins:
  - type: prefill-header-handler
  - type: prefill-filter
  - type: decode-filter
  - type: random-picker
    parameters:
      maxNumOfEndpoints: 1
  - type: pd-profile-handler
    parameters:
      threshold: 0
      hashBlockSize: 5
  schedulingProfiles:
  - name: prefill
    plugins:
    - pluginRef: prefill-filter
    - pluginRef: random-picker
  - name: decode
    plugins:
    - pluginRef: decode-filter
    - pluginRef: random-picker
EOF

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/inferencepool.values.yaml
# Lines 2-4:
#   flags:
#     kv-cache-usage-percentage-metric: "vllm:kv_cache_usage_perc"
#     v: 1
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_FLAGS=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_GAIE_FLAGS
kv-cache-usage-percentage-metric: "vllm:kv_cache_usage_perc"
v: 1
EOF

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/inferencepool.values.yaml
# Lines 47-48:
#   provider:
#     name: istio
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GATEWAY_CLASS_NAME=istio

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/inferencepool.values.yaml
# Lines 49-63:
#   istio:
#     destinationRule:
#       host: llm-d-infpool-epp
#       trafficPolicy:
#         tls:
#           mode: SIMPLE
#           insecureSkipVerify: true
#         connectionPool:
#           http:
#             http1MaxPendingRequests: 256000
#             maxRequestsPerConnection: 256000
#             http2MaxRequests: 256000
#             idleTimeout: "900s"
#           tcp:
#             maxConnections: 256000
#             maxConnectionDuration: "1800s"
#             connectTimeout: "900s"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_INFERENCE_POOL_PROVIDER_CONFIG=$(mktemp)
cat << 'EOF' > $LLMDBENCH_VLLM_MODELSERVICE_INFERENCE_POOL_PROVIDER_CONFIG
destinationRule:
  host: llm-d-infpool-epp
  trafficPolicy:
    tls:
      mode: SIMPLE
      insecureSkipVerify: true
    connectionPool:
      http:
        http1MaxPendingRequests: 256000
        maxRequestsPerConnection: 256000
        http2MaxRequests: 256000
        idleTimeout: "900s"
      tcp:
        maxConnections: 256000
        maxConnectionDuration: "1800s"
        connectTimeout: "900s"
EOF

# =============================================================================
# SOURCE: Benchmark framework defaults
# =============================================================================
export LLMDBENCH_HARNESS_NAME=inference-perf
export LLMDBENCH_HARNESS_EXPERIMENT_PROFILE=sanity_random.yaml
export LLMDBENCH_CONTROL_WORK_DIR=~/data/wide-ep-lws-coreweave
