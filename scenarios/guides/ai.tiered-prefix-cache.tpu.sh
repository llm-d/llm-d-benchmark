# TIERED-PREFIX-CACHE WELL LIT PATH (TPU VARIANT)
# Based on https://github.com/llm-d/llm-d/tree/main/guides/tiered-prefix-cache
# Variant: tpu (Google Cloud TPU deployment with LMCache connector for CPU offloading)
# Generated by llm-d-benchmark guide converter

# =============================================================================
# SOURCE: guides/tiered-prefix-cache/cpu/manifests/vllm/lmcache-connector/kustomization.yaml
# Lines 21-22:
#   - "serve"
#   - "Qwen/Qwen3-32B"
# =============================================================================
export LLMDBENCH_DEPLOY_MODEL_LIST="Qwen/Qwen3-32B"

# =============================================================================
# SOURCE: guides/tiered-prefix-cache/cpu/manifests/inferencepool/values.yaml
# Line 28:
#   lruCapacityPerServer: 41000  # Allocating ~100GB for Qwen-32B
#
# NOTE: 41,000 blocks * 2.5MB/block (based on 160KB/token * 16 block size) = ~100GB
# This is for Qwen-32B model with substantial CPU RAM allocation
# =============================================================================
export LLMDBENCH_VLLM_COMMON_PVC_MODEL_CACHE_SIZE="80Gi"

# =============================================================================
# SOURCE: TPU variant configuration (derived from guide README)
# Guide indicates tiered prefix cache can be deployed on TPU infrastructure
# Setting accelerator type to google.com/tpu for TPU resources
# =============================================================================
export LLMDBENCH_VLLM_COMMON_ACCELERATOR_RESOURCE="google.com/tpu"

# =============================================================================
# SOURCE: guides/tiered-prefix-cache/cpu/manifests/vllm/lmcache-connector/kustomization.yaml
# Lines 23-24:
#   - "--tensor-parallel-size"
#   - "2"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM=2

# =============================================================================
# SOURCE: Benchmark framework default (not explicitly in guide)
# Guide doesn't specify replica count; using default of 1
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_REPLICAS=1

# =============================================================================
# SOURCE: guides/tiered-prefix-cache/cpu/manifests/vllm/lmcache-connector/kustomization.yaml
# Lines 54-56:
#   requests:
#     nvidia.com/gpu: 2
#     memory: 400G
#
# NOTE: For TPU, memory request maintained; GPU count derived from tensor parallelism
# CPU RAM of 400G supports ~100GB LMCache allocation plus model and overhead
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_MEM=400Gi

# =============================================================================
# SOURCE: Benchmark framework default (not in guide)
# Guide doesn't specify shared memory size; using reasonable default for large model
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_SHM_MEM=16Gi

# =============================================================================
# SOURCE: guides/tiered-prefix-cache/cpu/manifests/vllm/lmcache-connector/kustomization.yaml
# Lines 12-13:
#   path: /spec/template/spec/containers/0/image
#   value: ghcr.io/llm-d/llm-d-cuda:v0.5.0
# =============================================================================
export LLMDBENCH_LLMD_IMAGE_TAG="v0.5.0"

# =============================================================================
# SOURCE: guides/tiered-prefix-cache/cpu/manifests/vllm/lmcache-connector/kustomization.yaml
# Lines 32-33:
#   - "--gpu-memory-utilization"
#   - "0.8"
# =============================================================================
export LLMDBENCH_VLLM_COMMON_ACCELERATOR_MEM_UTIL=0.8

# =============================================================================
# SOURCE: guides/tiered-prefix-cache/cpu/manifests/vllm/lmcache-connector/kustomization.yaml
# Lines 27-28:
#   - "--max-num-seq"
#   - "1024"
# =============================================================================
export LLMDBENCH_VLLM_COMMON_MAX_NUM_SEQ=1024

# =============================================================================
# SOURCE: Benchmark framework convention (not in guide)
# Using custom command to inject preprocessing and exact vLLM args
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_MODEL_COMMAND=custom
export LLMDBENCH_VLLM_COMMON_PREPROCESS="python3 /setup/preprocess/set_llmdbench_environment.py; source \$HOME/llmdbench_env.sh"
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS=$LLMDBENCH_VLLM_COMMON_PREPROCESS

# =============================================================================
# SOURCE: guides/tiered-prefix-cache/cpu/manifests/vllm/lmcache-connector/kustomization.yaml
# Lines 15-33:
#   path: /spec/template/spec/containers/0/command
#   value:
#     - "vllm"
#   path: /spec/template/spec/containers/0/args
#   value:
#     - "serve"
#     - "Qwen/Qwen3-32B"
#     - "--tensor-parallel-size"
#     - "2"
#     - "--port"
#     - "8000"
#     - "--max-num-seq"
#     - "1024"
#     - "--kv-transfer-config"
#     - '{"kv_connector":"LMCacheConnectorV1","kv_role":"kv_both"}'
#     - "--enable_prefix_caching"
#     - "--gpu-memory-utilization"
#     - "0.8"
#
# NOTE: Port mapping - Guide uses port 8000, but benchmark uses proxy pattern:
#   - INFERENCE_PORT (8000): proxy/sidecar listens here
#   - METRICS_PORT (8200): vLLM actually listens here
# Preprocessing command added per benchmark framework convention
# kv-transfer-config enables LMCache with kv_both role for CPU offloading
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS
REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS; \
vllm serve /model-cache/models/REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--host 0.0.0.0 \
--served-model-name REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--port REPLACE_ENV_LLMDBENCH_VLLM_COMMON_METRICS_PORT \
--tensor-parallel-size REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM \
--max-num-seq REPLACE_ENV_LLMDBENCH_VLLM_COMMON_MAX_NUM_SEQ \
--kv-transfer-config '{"kv_connector":"LMCacheConnectorV1","kv_role":"kv_both"}' \
--enable_prefix_caching \
--gpu-memory-utilization REPLACE_ENV_LLMDBENCH_VLLM_COMMON_ACCELERATOR_MEM_UTIL
EOF

# =============================================================================
# SOURCE: guides/tiered-prefix-cache/cpu/manifests/vllm/lmcache-connector/kustomization.yaml
# Lines 34-48:
#   - op: add
#     path: /spec/template/spec/containers/0/env/-
#     value:
#       name: LMCACHE_MAX_LOCAL_CPU_SIZE
#       value: "200.0"
#   - op: add
#     path: /spec/template/spec/containers/0/env/-
#     value:
#       name: PYTHONHASHSEED
#       value: "123"
#   - op: add
#     path: /spec/template/spec/containers/0/env/-
#     value:
#       name: PROMETHEUS_MULTIPROC_DIR
#       value: "/tmp/lmcache_prometheus"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_TO_YAML=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_TO_YAML
- name: LMCACHE_MAX_LOCAL_CPU_SIZE
  value: "200.0"
- name: PYTHONHASHSEED
  value: "123"
- name: PROMETHEUS_MULTIPROC_DIR
  value: "/tmp/lmcache_prometheus"
EOF

# =============================================================================
# SOURCE: guides/tiered-prefix-cache/cpu/manifests/vllm/lmcache-connector/kustomization.yaml
# Lines 68-72:
#   - op: add
#     path: /spec/template/spec/containers/0/volumeMounts/-
#     value:
#       name: lmcache-prometheus-metrics
#       mountPath: /tmp/lmcache_prometheus
#
# PLUS benchmark framework standard volumes (preprocesses volume mount)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS
- name: dshm
  mountPath: /dev/shm
- name: preprocesses
  mountPath: /setup/preprocess
- name: lmcache-prometheus-metrics
  mountPath: /tmp/lmcache_prometheus
EOF

# =============================================================================
# SOURCE: guides/tiered-prefix-cache/cpu/manifests/vllm/lmcache-connector/kustomization.yaml
# Lines 63-67:
#   - op: add
#     path: /spec/template/spec/volumes/-
#     value:
#       name: lmcache-prometheus-metrics
#       emptyDir: {}
#
# PLUS benchmark framework standard volumes (preprocesses configMap - MUST be first)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES}
- name: preprocesses
  configMap:
    defaultMode: 0755
    name: llm-d-benchmark-preprocesses
- name: dshm
  emptyDir:
    medium: Memory
    sizeLimit: REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_SHM_MEM
- name: lmcache-prometheus-metrics
  emptyDir: {}
EOF

# =============================================================================
# SOURCE: Guide does not use P/D disaggregation
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_REPLICAS=0

# =============================================================================
# SOURCE: guides/tiered-prefix-cache/cpu/manifests/inferencepool/values.yaml
# Lines 8-10:
#   flags:
#     # in vLLM 10.0+, the metric is renamed while upstream GAIE is still using the old name as default.
#     kv-cache-usage-percentage-metric: "vllm:kv_cache_usage_perc"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_FLAGS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_GAIE_FLAGS
kv-cache-usage-percentage-metric: "vllm:kv_cache_usage_perc"
EOF

# =============================================================================
# SOURCE: guides/tiered-prefix-cache/cpu/manifests/inferencepool/values.yaml
# Line 14:
#   pluginsConfigFile: "custom-plugins.yaml"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_PLUGINS_CONFIGFILE="custom-plugins.yaml"

# =============================================================================
# SOURCE: guides/tiered-prefix-cache/cpu/manifests/inferencepool/values.yaml
# Lines 14-39:
#   pluginsConfigFile: "custom-plugins.yaml"
#   pluginsCustomConfig:
#     custom-plugins.yaml: |
#       apiVersion: inference.networking.x-k8s.io/v1alpha1
#       kind: EndpointPickerConfig
#       plugins:
#       - type: queue-scorer
#       - type: kv-cache-utilization-scorer
#       - type: prefix-cache-scorer
#         name: gpu-prefix-cache-scorer
#       - type: prefix-cache-scorer
#         name: cpu-prefix-cache-scorer
#         parameters:
#           autoTune: false  # vLLM doesn't have the CPU capacity metric to enable autoTune yet
#           lruCapacityPerServer: 41000  # Allocating ~100GB for Qwen-32B: 41,000 blocks * 2.5MB/block (based on 160KB/token * 16 block size).
#       schedulingProfiles:
#       - name: default
#         plugins:
#         - pluginRef: queue-scorer
#           weight: 2.0
#         - pluginRef: kv-cache-utilization-scorer
#           weight: 2.0
#         - pluginRef: gpu-prefix-cache-scorer
#           weight: 1.0
#         - pluginRef: cpu-prefix-cache-scorer
#           weight: 1.0
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_CUSTOM_PLUGINS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_GAIE_CUSTOM_PLUGINS
custom-plugins.yaml: |
  apiVersion: inference.networking.x-k8s.io/v1alpha1
  kind: EndpointPickerConfig
  plugins:
  - type: queue-scorer
  - type: kv-cache-utilization-scorer
  - type: prefix-cache-scorer
    name: gpu-prefix-cache-scorer
  - type: prefix-cache-scorer
    name: cpu-prefix-cache-scorer
    parameters:
      autoTune: false
      lruCapacityPerServer: 41000
  schedulingProfiles:
  - name: default
    plugins:
    - pluginRef: queue-scorer
      weight: 2.0
    - pluginRef: kv-cache-utilization-scorer
      weight: 2.0
    - pluginRef: gpu-prefix-cache-scorer
      weight: 1.0
    - pluginRef: cpu-prefix-cache-scorer
      weight: 1.0
EOF

# =============================================================================
# SOURCE: Benchmark framework defaults
# =============================================================================
export LLMDBENCH_HARNESS_NAME=inference-perf
export LLMDBENCH_HARNESS_EXPERIMENT_PROFILE=sanity_random.yaml
export LLMDBENCH_CONTROL_WORK_DIR=~/data/tiered-prefix-cache-tpu
