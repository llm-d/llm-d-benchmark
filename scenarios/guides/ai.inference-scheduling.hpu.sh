# INFERENCE SCHEDULING WELL LIT PATH (Intel Gaudi HPU)
# Based on https://github.com/llm-d/llm-d/tree/main/guides/inference-scheduling
# Variant: Intel Gaudi (HPU) with DRA
# Generated by llm-d-benchmark guide converter

# =============================================================================
# SOURCE: guides/inference-scheduling/ms-inference-scheduling/values-hpu.yaml
# Lines 17-18:
#   name: meta-llama/Llama-3.1-8B-Instruct
#   uri: "hf://meta-llama/Llama-3.1-8B-Instruct"
# =============================================================================
export LLMDBENCH_DEPLOY_MODEL_LIST="meta-llama/Llama-3.1-8B-Instruct"

# =============================================================================
# SOURCE: guides/inference-scheduling/ms-inference-scheduling/values-hpu.yaml
# Line 19:
#   size: 50Gi
# =============================================================================
export LLMDBENCH_VLLM_COMMON_PVC_MODEL_CACHE_SIZE="50Gi"

# =============================================================================
# SOURCE: guides/inference-scheduling/helmfile.yaml.gotmpl
# Lines referencing llm-d-infra chart version v1.3.6
# =============================================================================
export LLMDBENCH_VLLM_INFRA_CHART_VERSION="v1.3.6"

# =============================================================================
# SOURCE: guides/inference-scheduling/helmfile.yaml.gotmpl
# Lines referencing llm-d-modelservice chart version v0.4.5
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_CHART_VERSION="v0.4.5"

# =============================================================================
# SOURCE: guides/inference-scheduling/helmfile.yaml.gotmpl
# Lines referencing inferencepool (GAIE) chart version v1.3.0
# =============================================================================
export LLMDBENCH_VLLM_GAIE_CHART_VERSION="v1.3.0"

# =============================================================================
# SOURCE: guides/inference-scheduling/ms-inference-scheduling/values-hpu.yaml
# Line 42:
#   image: "opea/vllm-gaudi:1.22.0"
#
# NOTE: Using full image specification as the guide uses a different vendor image
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_IMAGE="opea/vllm-gaudi:1.22.0"

# =============================================================================
# SOURCE: guides/inference-scheduling/ms-inference-scheduling/values-hpu.yaml
# Lines 28-30:
#   accelerator:
#     type: intel-gaudi
#     dra: true
# =============================================================================
export LLMDBENCH_VLLM_COMMON_ACCELERATOR_RESOURCE="intel-gaudi"

# =============================================================================
# SOURCE: guides/inference-scheduling/ms-inference-scheduling/values-hpu.yaml
# Line 39:
#   replicas: 1
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_REPLICAS=1

# =============================================================================
# SOURCE: Benchmark framework convention (not in guide)
# Using custom command to inject preprocessing and exact vLLM args
#
# NOTE: Guide uses modelCommand: "imageDefault" which lets the chart auto-generate
# the vLLM command. For llm-d-benchmark, we convert to custom command pattern.
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_MODEL_COMMAND=custom
export LLMDBENCH_VLLM_COMMON_PREPROCESS="python3 /setup/preprocess/set_llmdbench_environment.py; source \$HOME/llmdbench_env.sh"
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS=$LLMDBENCH_VLLM_COMMON_PREPROCESS

# =============================================================================
# SOURCE: guides/inference-scheduling/ms-inference-scheduling/values-hpu.yaml
# Lines 43-49:
#   modelCommand: "imageDefault"
#   args:
#     - --block-size=128
#     - --max-num-seqs=256
#     - --max-seq-len-to-capture=2048
#     - --max-model-len=2048
#     - --max-num-batched-token=16000
#
# NOTE: Port mapping - Guide uses container port 8000 but benchmark uses proxy pattern:
#   - INFERENCE_PORT (8000): proxy/sidecar listens here (service port)
#   - METRICS_PORT (8200): vLLM actually listens here (container port)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS
REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS; \
vllm serve /model-cache/models/REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--host 0.0.0.0 \
--served-model-name REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--port REPLACE_ENV_LLMDBENCH_VLLM_COMMON_METRICS_PORT \
--block-size 128 \
--max-num-seqs 256 \
--max-seq-len-to-capture 2048 \
--max-model-len 2048 \
--max-num-batched-tokens 16000
EOF

# =============================================================================
# SOURCE: guides/inference-scheduling/ms-inference-scheduling/values-hpu.yaml
# Lines 50-58:
#   env:
#     - name: OMPI_MCA_btl_vader_single_copy_mechanism
#       value: "none"
#     - name: HABANA_LOGS
#       value: "/tmp/habana_logs"
#     - name: VLLM_SKIP_WARMUP
#       value: "true"
#     - name: DO_NOT_TRACK
#       value: "1"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_TO_YAML=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_TO_YAML
- name: OMPI_MCA_btl_vader_single_copy_mechanism
  value: "none"
- name: HABANA_LOGS
  value: "/tmp/habana_logs"
- name: VLLM_SKIP_WARMUP
  value: "true"
- name: DO_NOT_TRACK
  value: "1"
EOF

# =============================================================================
# SOURCE: Benchmark framework standard volumes
# NOTE: Guide does not define explicit volumes (besides model volume)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS}
- name: preprocesses
  mountPath: /setup/preprocess
EOF

# =============================================================================
# SOURCE: Benchmark framework standard volumes
# NOTE: Guide does not define explicit volumes (besides model volume)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES}
- name: preprocesses
  configMap:
    defaultMode: 0755
    name: llm-d-benchmark-preprocesses
EOF

# =============================================================================
# SOURCE: guides/inference-scheduling/ms-inference-scheduling/values-hpu.yaml
# Lines 87-88:
#   prefill:
#     create: false
# =============================================================================
# =============================================================================
# SOURCE: guides/inference-scheduling/ms-inference-scheduling/values*.yaml
# Lines referencing routing.proxy.enabled: false
# NOTE: Routing sidecar disabled - no P/D disaggregation in inference scheduling
# =============================================================================
export LLMDBENCH_LLMD_ROUTINGSIDECAR_ENABLED=false

export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_REPLICAS=0

# =============================================================================
# SOURCE: guides/inference-scheduling/gaie-inference-scheduling/values.yaml
# Lines 10-12:
#   name: llm-d-inference-scheduler
#   hub: ghcr.io/llm-d
#   tag: v0.5.0
# =============================================================================
export LLMDBENCH_LLMD_INFERENCESCHEDULER_IMAGE_NAME="llm-d-inference-scheduler"
export LLMDBENCH_LLMD_INFERENCESCHEDULER_IMAGE_REGISTRY="ghcr.io"
export LLMDBENCH_LLMD_INFERENCESCHEDULER_IMAGE_REPO="llm-d"
export LLMDBENCH_LLMD_INFERENCESCHEDULER_IMAGE_TAG="v0.5.0"

# =============================================================================
# SOURCE: guides/inference-scheduling/gaie-inference-scheduling/values.yaml
# Line 20:
#   pluginsConfigFile: "default-plugins.yaml"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_PLUGINS_CONFIGFILE="default-plugins.yaml"

# =============================================================================
# SOURCE: guides/inference-scheduling/gaie-inference-scheduling/values.yaml
# Lines 3-6:
#   flags:
#     # in vLLM 10.0+, the metric is renamed while upstream GAIE is still using the old name as default.
#     # See https://github.com/kubernetes-sigs/gateway-api-inference-extension/pull/1905.
#     kv-cache-usage-percentage-metric: "vllm:kv_cache_usage_perc"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_FLAGS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_GAIE_FLAGS
kv-cache-usage-percentage-metric: "vllm:kv_cache_usage_perc"
EOF

# =============================================================================
# SOURCE: Benchmark framework defaults
# =============================================================================
export LLMDBENCH_HARNESS_NAME=inference-perf
export LLMDBENCH_HARNESS_EXPERIMENT_PROFILE=sanity_random.yaml
export LLMDBENCH_CONTROL_WORK_DIR=~/data/inference-scheduling-hpu
