# INFERENCE SCHEDULING WELL LIT PATH - GAUDI VARIANT
# Based on https://github.com/llm-d/llm-d/tree/main/guides/inference-scheduling
# Variant: gaudi
# Generated by llm-d-benchmark guide converter
#
# This guide demonstrates inference scheduling with vLLM on Intel Gaudi accelerators.
# The GAIE (Gateway API Inference Extension) scheduler uses KV cache metrics to
# intelligently route requests to the most suitable backend pods.

# =============================================================================
# SOURCE: guides/inference-scheduling/ms-inference-scheduling/values.yaml
# Lines 3-5:
#   uri: "hf://Qwen/Qwen3-32B"
#   name: "Qwen/Qwen3-32B"
#   size: 80Gi
# =============================================================================
export LLMDBENCH_DEPLOY_MODEL_LIST="Qwen/Qwen3-32B"

# =============================================================================
# SOURCE: guides/inference-scheduling/ms-inference-scheduling/values.yaml
# Line 5: size: 80Gi
# =============================================================================
export LLMDBENCH_VLLM_COMMON_PVC_MODEL_CACHE_SIZE="80Gi"

# =============================================================================
# SOURCE: guides/inference-scheduling/ms-inference-scheduling/values.yaml
# Lines 25-26:
#   image: ghcr.io/llm-d/llm-d-cuda:v0.5.0
#
# NOTE: Converting from CUDA image to Gaudi. Using Gaudi-compatible image.
# =============================================================================
export LLMDBENCH_LLMD_IMAGE_NAME="llm-d-gaudi"
export LLMDBENCH_LLMD_IMAGE_TAG="v0.5.0"

# =============================================================================
# SOURCE: Benchmark framework standard - Gaudi accelerator configuration
# =============================================================================
export LLMDBENCH_VLLM_COMMON_ACCELERATOR_RESOURCE="habana.ai/gaudi"

# =============================================================================
# SOURCE: guides/inference-scheduling/ms-inference-scheduling/values.yaml
# Lines 19-21:
#   parallelism:
#     tensor: 2
#     data: 1
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM=2

# =============================================================================
# SOURCE: guides/inference-scheduling/ms-inference-scheduling/values.yaml
# Line 22: replicas: 8
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_REPLICAS=8

# =============================================================================
# SOURCE: guides/inference-scheduling/ms-inference-scheduling/values.yaml
# Lines 40-42:
#   resources:
#     requests:
#       cpu: '32'
#       memory: 100Gi
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_NR=32
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_MEM="100Gi"

# =============================================================================
# SOURCE: guides/inference-scheduling/ms-inference-scheduling/values.yaml
# Lines 62-65:
#   volumes:
#     - name: shm
#       emptyDir:
#         medium: Memory
#         sizeLimit: 20Gi
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_SHM_MEM="20Gi"

# =============================================================================
# SOURCE: guides/inference-scheduling/ms-inference-scheduling/values.yaml
# Line 30: "--gpu-memory-utilization=0.95"
# =============================================================================
export LLMDBENCH_VLLM_COMMON_ACCELERATOR_MEM_UTIL=0.95

# =============================================================================
# SOURCE: Benchmark framework convention (not in guide)
# Using custom command to inject preprocessing and exact vLLM args
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_MODEL_COMMAND=custom
export LLMDBENCH_VLLM_COMMON_PREPROCESS="python3 /setup/preprocess/set_llmdbench_environment.py; source \$HOME/llmdbench_env.sh"
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS=$LLMDBENCH_VLLM_COMMON_PREPROCESS

# =============================================================================
# SOURCE: guides/inference-scheduling/ms-inference-scheduling/values.yaml
# Lines 27-31 (container args and command):
#   modelCommand: vllmServe
#   args:
#     - "--disable-uvicorn-access-log"
#     - "--gpu-memory-utilization=0.95"
#
# NOTE: Using benchmark framework port mapping:
#   - INFERENCE_PORT (8000): proxy/sidecar would listen here (disabled in this guide)
#   - METRICS_PORT (8200): vLLM actually listens here
# However, this guide disables the proxy (routing.proxy.enabled: false), so vLLM
# listens directly on port 8000.
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS
REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS; \
vllm serve /model-cache/models/REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--host 0.0.0.0 \
--served-model-name REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--port 8000 \
--disable-uvicorn-access-log \
--gpu-memory-utilization REPLACE_ENV_LLMDBENCH_VLLM_COMMON_ACCELERATOR_MEM_UTIL \
--tensor-parallel-size REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM
EOF

# =============================================================================
# SOURCE: guides/inference-scheduling/ms-inference-scheduling/values.yaml
# Lines 46-58 (volumeMounts):
#   volumeMounts:
#     - name: metrics-volume
#       mountPath: /.config
#     - name: shm
#       mountPath: /dev/shm
#     - name: torch-compile-cache
#       mountPath: /.cache
#
# PLUS benchmark framework standard volume mounts (preprocesses)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS}
- name: dshm
  mountPath: /dev/shm
- name: preprocesses
  mountPath: /setup/preprocess
- name: metrics-volume
  mountPath: /.config
- name: torch-compile-cache
  mountPath: /.cache
EOF

# =============================================================================
# SOURCE: guides/inference-scheduling/ms-inference-scheduling/values.yaml
# Lines 62-68 (volumes):
#   volumes:
#     - name: metrics-volume
#       emptyDir: {}
#     - name: torch-compile-cache
#       emptyDir: {}
#     - name: shm
#       emptyDir:
#         medium: Memory
#         sizeLimit: 20Gi
#
# PLUS benchmark framework standard volumes (preprocesses configMap - MUST be first)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES}
- name: preprocesses
  configMap:
    defaultMode: 0755
    name: llm-d-benchmark-preprocesses
- name: metrics-volume
  emptyDir: {}
- name: torch-compile-cache
  emptyDir: {}
- name: dshm
  emptyDir:
    medium: Memory
    sizeLimit: REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_SHM_MEM
EOF

# =============================================================================
# SOURCE: guides/inference-scheduling/ms-inference-scheduling/values.yaml
# Line 71: create: false
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_REPLICAS=0

# =============================================================================
# SOURCE: guides/inference-scheduling/gaie-inference-scheduling/values.yaml
# Line 16: pluginsConfigFile: "default-plugins.yaml"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_PLUGINS_CONFIGFILE="default-plugins.yaml"

# =============================================================================
# SOURCE: guides/inference-scheduling/gaie-inference-scheduling/values.yaml
# Lines 2-6:
#   flags:
#     kv-cache-usage-percentage-metric: "vllm:kv_cache_usage_perc"
#
# NOTE: This flag is necessary for vLLM 10.0+ compatibility with GAIE's KV cache metrics
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_FLAGS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_GAIE_FLAGS
kv-cache-usage-percentage-metric: "vllm:kv_cache_usage_perc"
EOF

# =============================================================================
# SOURCE: guides/inference-scheduling/gaie-inference-scheduling/values.yaml
# Lines 7-16:
#   image:
#     name: llm-d-inference-scheduler
#     hub: ghcr.io/llm-d
#     tag: v0.5.0
# =============================================================================
export LLMDBENCH_LLMD_INFERENCESCHEDULER_IMAGE_NAME="llm-d-inference-scheduler"
export LLMDBENCH_LLMD_INFERENCESCHEDULER_IMAGE_REGISTRY="ghcr.io"
export LLMDBENCH_LLMD_INFERENCESCHEDULER_IMAGE_REPO="llm-d"
export LLMDBENCH_LLMD_INFERENCESCHEDULER_IMAGE_TAG="v0.5.0"

# =============================================================================
# SOURCE: guides/inference-scheduling/gaie-inference-scheduling/values.yaml
# Lines 17-24:
#   monitoring:
#     interval: "10s"
#     prometheus:
#       enabled: true
#       auth:
#         enabled: true
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_MONITORING_INTERVAL="10s"
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_MONITORING_PROMETHEUS_ENABLED=true
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_MONITORING_AUTH_ENABLED=true

# =============================================================================
# SOURCE: guides/inference-scheduling/ms-inference-scheduling/values.yaml
# Lines 13-16:
#   routing:
#     proxy:
#       enabled: false
#       targetPort: 8000
#
# NOTE: The routing proxy/sidecar is disabled in this guide because inference
# scheduling happens at the GAIE level, not via per-pod sidecars.
# =============================================================================
export LLMDBENCH_LLMD_ROUTINGSIDECAR_ENABLED=false

# =============================================================================
# SOURCE: Benchmark framework defaults
# =============================================================================
export LLMDBENCH_HARNESS_NAME=inference-perf
export LLMDBENCH_HARNESS_EXPERIMENT_PROFILE=sanity_random.yaml
export LLMDBENCH_CONTROL_WORK_DIR=~/data/inference-scheduling-gaudi
