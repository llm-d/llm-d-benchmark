# WIDE EXPERT PARALLELISM WITH LWS WELL LIT PATH
# Based on https://github.com/llm-d/llm-d/tree/main/guides/wide-ep-lws
# Variant: base
# Generated by llm-d-benchmark guide converter
#
# This guide demonstrates two-stage disaggregated inference with wide expert parallelism
# using DeepSeek-R1-0528 model across 32 GPUs (prefill: 16 GPUs, decode: 16 GPUs)

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Line 63: deepseek-ai/DeepSeek-R1-0528
# =============================================================================
export LLMDBENCH_DEPLOY_MODEL_LIST="deepseek-ai/DeepSeek-R1-0528"

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Line 57: image: ghcr.io/llm-d/llm-d-cuda:v0.5.0
# PLUS guides/wide-ep-lws/manifests/modelserver/base/prefill.yaml
# Line 28: image: ghcr.io/llm-d/llm-d-cuda:v0.5.0
# =============================================================================
export LLMDBENCH_LLMD_IMAGE_TAG="v0.5.0"

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Line 10: replicas: 1
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_REPLICAS=1

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Lines 99-100:
#   - name: TP_SIZE
#     value: "1"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM=1

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Lines 101-102:
#   - name: DP_SIZE_LOCAL
#     value: "8"
# NOTE: With LeaderWorkerSet size=2, this gives DP=16 total (8 per worker × 2 workers)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_DATA_LOCAL_PARALLELISM=8

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Lines 124-125:
#   requests:
#     memory: 512Gi
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_MEM=512Gi

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Lines 126-127:
#   requests:
#     cpu: "32"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_NR=32

# =============================================================================
# SOURCE: Benchmark framework convention (not in guide)
# Using custom command to inject preprocessing and exact vLLM args
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_MODEL_COMMAND=custom
export LLMDBENCH_VLLM_COMMON_PREPROCESS="python3 /setup/preprocess/set_llmdbench_environment.py; source \$HOME/llmdbench_env.sh"
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS=$LLMDBENCH_VLLM_COMMON_PREPROCESS

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Lines 61-85 (vLLM serve command and arguments):
#   exec vllm serve deepseek-ai/DeepSeek-R1-0528 \
#     --port 8200 \
#     --tensor-parallel-size 1 \
#     --data-parallel-size-local ${DP_SIZE_LOCAL} \
#     --enable-expert-parallel \
#     --enable-expert-load-balancing \
#     --expert-parallel-load-balancing-policy token_overhead \
#     --gpu-memory-utilization 0.75 \
#     --enable-dual-batch-overlap \
#     --dual-batch-overlap-threshold 32 \
#     --kv-transfer-config '{"kv_connector":"NixlConnector","kv_role":"kv_consumer","kv_parallel_size":1,"kv_buffer_size":1e9}' \
#     --max-model-len 4096 \
#     --disable-log-requests
#
# NOTE: Port mapping - Guide uses 8200 which matches METRICS_PORT in benchmark framework
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS
REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS; \
vllm serve /model-cache/models/REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--host 0.0.0.0 \
--served-model-name REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--port REPLACE_ENV_LLMDBENCH_VLLM_COMMON_METRICS_PORT \
--tensor-parallel-size REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM \
--data-parallel-size-local REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_DATA_LOCAL_PARALLELISM \
--enable-expert-parallel \
--enable-expert-load-balancing \
--expert-parallel-load-balancing-policy token_overhead \
--gpu-memory-utilization 0.75 \
--enable-dual-batch-overlap \
--dual-batch-overlap-threshold 32 \
--kv-transfer-config '{"kv_connector":"NixlConnector","kv_role":"kv_consumer","kv_parallel_size":1,"kv_buffer_size":1e9}' \
--max-model-len 4096 \
--disable-log-requests
EOF

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Lines 88-117 (environment variables):
#   - name: VLLM_LOGGING_LEVEL
#     value: INFO
#   - name: VLLM_MOE_DP_CHUNK_SIZE
#     value: "384"
#   - name: VLLM_USE_DEEP_GEMM
#     value: "1"
#   - name: VLLM_ALL2ALL_BACKEND
#     value: deepep_low_latency
#   - name: NVSHMEM_REMOTE_TRANSPORT
#     value: ibgda
#   - name: NCCL_SOCKET_IFNAME
#     value: eth0
#   [additional env vars for cache paths]
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_TO_YAML=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_TO_YAML
- name: VLLM_LOGGING_LEVEL
  value: INFO
- name: VLLM_MOE_DP_CHUNK_SIZE
  value: "384"
- name: VLLM_USE_DEEP_GEMM
  value: "1"
- name: VLLM_ALL2ALL_BACKEND
  value: deepep_low_latency
- name: NVSHMEM_REMOTE_TRANSPORT
  value: ibgda
- name: NCCL_SOCKET_IFNAME
  value: eth0
- name: DP_SIZE_LOCAL
  value: "8"
- name: TP_SIZE
  value: "1"
- name: CUDA_CACHE_PATH
  value: /tmp/jit-cache
- name: TORCH_NCCL_ASYNC_ERROR_HANDLING
  value: "1"
- name: TORCH_NCCL_AVOID_RECORD_STREAMS
  value: "1"
- name: NCCL_IB_QPS_PER_CONNECTION
  value: "8"
- name: HF_HOME
  value: /tmp/hf-cache
- name: TRITON_CACHE_DIR
  value: /tmp/jit-cache
- name: VLLM_CACHE_ROOT
  value: /tmp/vllm-cache
EOF

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Lines 46-52 (volumes):
#   - name: dshm
#     emptyDir:
#       medium: Memory
#       sizeLimit: 2Gi
#   - name: hf-cache
#     emptyDir: {}
#   - name: jit-cache
#     emptyDir: {}
#
# PLUS benchmark framework standard volumes (preprocesses, metrics-volume)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES}
- name: preprocesses
  configMap:
    defaultMode: 0755
    name: llm-d-benchmark-preprocesses
- name: dshm
  emptyDir:
    medium: Memory
    sizeLimit: 2Gi
- name: hf-cache
  emptyDir: {}
- name: jit-cache
  emptyDir: {}
EOF

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Lines 130-135 (volume mounts):
#   - name: dshm
#     mountPath: /dev/shm
#   - name: hf-cache
#     mountPath: /tmp/hf-cache
#   - name: jit-cache
#     mountPath: /tmp/jit-cache
#
# PLUS benchmark framework standard mounts (preprocesses)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS}
- name: preprocesses
  mountPath: /setup/preprocess
- name: dshm
  mountPath: /dev/shm
- name: hf-cache
  mountPath: /tmp/hf-cache
- name: jit-cache
  mountPath: /tmp/jit-cache
EOF

# =============================================================================
# PREFILL STAGE CONFIGURATION
# =============================================================================

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/prefill.yaml
# Line 11: replicas: 1
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_REPLICAS=1

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/prefill.yaml
# Lines 143-144:
#   - name: TP_SIZE
#     value: "1"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_TENSOR_PARALLELISM=1

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/prefill.yaml
# Lines 145-146:
#   - name: DP_SIZE_LOCAL
#     value: "8"
# NOTE: With LeaderWorkerSet size=2, this gives DP=16 total (8 per worker × 2 workers)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_DATA_LOCAL_PARALLELISM=8

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/prefill.yaml
# Lines 165-166:
#   requests:
#     memory: 512Gi
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_CPU_MEM=512Gi

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/prefill.yaml
# Lines 163-164:
#   requests:
#     cpu: "32"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_CPU_NR=32

# =============================================================================
# SOURCE: Benchmark framework convention (not in guide)
# Using custom command to inject preprocessing and exact vLLM args
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_MODEL_COMMAND=custom
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_PREPROCESS=$LLMDBENCH_VLLM_COMMON_PREPROCESS

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/prefill.yaml
# Lines 52-78 (vLLM serve command and arguments):
#   exec vllm serve deepseek-ai/DeepSeek-R1-0528 \
#     --port 8000 \
#     --tensor-parallel-size 1 \
#     --data-parallel-size-local ${DP_SIZE_LOCAL} \
#     --enable-expert-parallel \
#     --enable-expert-load-balancing \
#     --expert-parallel-load-balancing-policy token_overhead \
#     --gpu-memory-utilization 0.75 \
#     --kv-transfer-config '{"kv_connector":"NixlConnector","kv_role":"kv_producer","kv_parallel_size":1,"kv_buffer_size":1e9}' \
#     --max-model-len 4096 \
#     --disable-log-requests
#
# NOTE: Port mapping - Guide uses 8000 but benchmark uses proxy pattern:
#   - INFERENCE_PORT (8000): proxy/sidecar listens here
#   - METRICS_PORT (8200): vLLM actually listens here
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_ARGS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_ARGS
REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_PREFILL_PREPROCESS; \
vllm serve /model-cache/models/REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--host 0.0.0.0 \
--served-model-name REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--port REPLACE_ENV_LLMDBENCH_VLLM_COMMON_METRICS_PORT \
--tensor-parallel-size REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_PREFILL_TENSOR_PARALLELISM \
--data-parallel-size-local REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_PREFILL_DATA_LOCAL_PARALLELISM \
--enable-expert-parallel \
--enable-expert-load-balancing \
--expert-parallel-load-balancing-policy token_overhead \
--gpu-memory-utilization 0.75 \
--kv-transfer-config '{"kv_connector":"NixlConnector","kv_role":"kv_producer","kv_parallel_size":1,"kv_buffer_size":1e9}' \
--max-model-len 4096 \
--disable-log-requests
EOF

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/prefill.yaml
# Lines 130-162 (environment variables):
#   - name: VLLM_LOGGING_LEVEL
#     value: INFO
#   - name: VLLM_ALL2ALL_BACKEND
#     value: deepep_high_throughput
#   - name: NCCL_SOCKET_IFNAME
#     value: eth0
#   [additional env vars for cache paths]
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_ENVVARS_TO_YAML=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_PREFILL_ENVVARS_TO_YAML
- name: VLLM_LOGGING_LEVEL
  value: INFO
- name: VLLM_ALL2ALL_BACKEND
  value: deepep_high_throughput
- name: NCCL_SOCKET_IFNAME
  value: eth0
- name: DP_SIZE_LOCAL
  value: "8"
- name: TP_SIZE
  value: "1"
- name: CUDA_CACHE_PATH
  value: /tmp/jit-cache
- name: TORCH_NCCL_ASYNC_ERROR_HANDLING
  value: "1"
- name: TORCH_NCCL_AVOID_RECORD_STREAMS
  value: "1"
- name: NCCL_IB_QPS_PER_CONNECTION
  value: "8"
- name: HF_HOME
  value: /tmp/hf-cache
- name: TRITON_CACHE_DIR
  value: /tmp/jit-cache
- name: VLLM_CACHE_ROOT
  value: /tmp/vllm-cache
EOF

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/prefill.yaml
# Lines 18-25 (volumes):
#   - name: dshm
#     emptyDir:
#       medium: Memory
#       sizeLimit: 2Gi
#   - name: hf-cache
#     emptyDir: {}
#   - name: jit-cache
#     emptyDir: {}
#
# PLUS benchmark framework standard volumes (preprocesses)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUMES=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUMES}
- name: preprocesses
  configMap:
    defaultMode: 0755
    name: llm-d-benchmark-preprocesses
- name: dshm
  emptyDir:
    medium: Memory
    sizeLimit: 2Gi
- name: hf-cache
  emptyDir: {}
- name: jit-cache
  emptyDir: {}
EOF

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/prefill.yaml
# Lines 172-177 (volume mounts):
#   - name: dshm
#     mountPath: /dev/shm
#   - name: hf-cache
#     mountPath: /tmp/hf-cache
#   - name: jit-cache
#     mountPath: /tmp/jit-cache
#
# PLUS benchmark framework standard mounts (preprocesses)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUME_MOUNTS=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUME_MOUNTS}
- name: preprocesses
  mountPath: /setup/preprocess
- name: dshm
  mountPath: /dev/shm
- name: hf-cache
  mountPath: /tmp/hf-cache
- name: jit-cache
  mountPath: /tmp/jit-cache
EOF

# =============================================================================
# GAIE CONFIGURATION
# =============================================================================

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/inferencepool.values.yaml
# Lines 2-3:
#   inferenceExtension:
#     pluginsConfigFile: "custom-plugins.yaml"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_PLUGINS_CONFIGFILE="custom-plugins.yaml"

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/inferencepool.values.yaml
# Lines 4-52 (complete pluginsCustomConfig section):
#   pluginsCustomConfig:
#     custom-plugins.yaml: |
#       apiVersion: inference.networking.x-k8s.io/v1alpha1
#       kind: EndpointPickerConfig
#       ...
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_CUSTOM_PLUGINS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_GAIE_CUSTOM_PLUGINS
custom-plugins.yaml: |
  apiVersion: inference.networking.x-k8s.io/v1alpha1
  kind: EndpointPickerConfig
  plugins:
    - type: prefill-filter
      parameters:
        stageLabel: "llm-d.ai/stage"
        stage: "prefill"
    - type: decode-filter
      parameters:
        stageLabel: "llm-d.ai/stage"
        stage: "decode"
    - type: random-picker
      parameters:
        maxEndpoints: 1
    - type: pd-profile-handler
      parameters:
        prefillThreshold: 0
  schedulingProfiles:
    - name: prefill
      plugins:
        - pluginRef: prefill-filter
        - pluginRef: random-picker
        - pluginRef: pd-profile-handler
    - name: decode
      plugins:
        - pluginRef: decode-filter
        - pluginRef: random-picker
        - pluginRef: pd-profile-handler
EOF

# =============================================================================
# SOURCE: Benchmark framework defaults
# =============================================================================
export LLMDBENCH_HARNESS_NAME=inference-perf
export LLMDBENCH_HARNESS_EXPERIMENT_PROFILE=sanity_random.yaml
export LLMDBENCH_CONTROL_WORK_DIR=~/data/wide-ep-lws
