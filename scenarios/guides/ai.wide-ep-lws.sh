# WIDE EP LWS (Wide Expert Parallel with LeaderWorkerSet) WELL LIT PATH
# Based on https://github.com/llm-d/llm-d/tree/main/guides/wide-ep-lws
# Variant: base
# Generated by llm-d-benchmark guide converter
#
# Kustomize Hierarchy:
# guides/wide-ep-lws/manifests/modelserver/base/kustomization.yaml
#     └── decode.yaml, prefill.yaml (LeaderWorkerSet resources)

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Line 6 and prefill.yaml line 6: llm-d.ai/model: DeepSeek-R1-0528
# Also in vLLM args line 32 (decode), line 31 (prefill): deepseek-ai/DeepSeek-R1-0528
# =============================================================================
export LLMDBENCH_DEPLOY_MODEL_LIST="deepseek-ai/DeepSeek-R1-0528"

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Line 40: image: ghcr.io/llm-d/llm-d-cuda:v0.5.0
# Also in prefill.yaml line 30
# =============================================================================
export LLMDBENCH_LLMD_IMAGE_TAG="v0.5.0"

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/inferencepool.values.yaml
# Lines 9-11:
#   image:
#     name: llm-d-inference-scheduler
#     hub: ghcr.io/llm-d
#     tag: v0.5.0
# =============================================================================
export LLMDBENCH_LLMD_INFERENCESCHEDULER_IMAGE_TAG="v0.5.0"

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Line 28: image: ghcr.io/llm-d/llm-d-routing-sidecar:v0.5.0
# =============================================================================
export LLMDBENCH_LLMD_ROUTINGSIDECAR_IMAGE_TAG="v0.5.0"

# =============================================================================
# SOURCE: Benchmark framework default (not in guide)
# Guide uses HF_HUB_CACHE=/var/cache/huggingface with emptyDir volume.
# Benchmark uses PVC for model cache. Setting size for large model.
# =============================================================================
export LLMDBENCH_VLLM_COMMON_PVC_MODEL_CACHE_SIZE="300Gi"

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Lines 12-15:
#   spec:
#     replicas: 1
#     leaderWorkerTemplate:
#       size: 2
#
# NOTE: LeaderWorkerSet deployment with 1 LWS group, 2 workers per group
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_MULTINODE=true
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_REPLICAS=1
export LLMDBENCH_VLLM_COMMON_NUM_WORKERS_PARALLELISM=2

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/prefill.yaml
# Lines 12-15:
#   spec:
#     replicas: 1
#     leaderWorkerTemplate:
#       size: 2
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_REPLICAS=1

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Line 82 (env): name: DP_SIZE_LOCAL, value: "8"
# Line 84 (env): name: TP_SIZE, value: "1"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_DATA_LOCAL_PARALLELISM=8
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM=1

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/prefill.yaml
# Line 52 (env): name: DP_SIZE_LOCAL, value: "8"
# Line 54 (env): name: TP_SIZE, value: "1"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_DATA_LOCAL_PARALLELISM=8
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_TENSOR_PARALLELISM=1

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Lines 153-156 (resources.requests):
#   cpu: 32
#   memory: 512Gi
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_NR=32
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_MEM="512Gi"

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/prefill.yaml
# Lines 121-124 (resources.requests):
#   cpu: 32
#   memory: 512Gi
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_CPU_NR=32
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_CPU_MEM="512Gi"

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Lines 47-50:
#   - name: dshm
#     emptyDir:
#       medium: Memory
#       sizeLimit: 2Gi
# Also in prefill.yaml lines 26-29
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_SHM_MEM="2Gi"
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_SHM_MEM="2Gi"

# =============================================================================
# SOURCE: Benchmark framework convention (not in guide)
# Using custom command to inject preprocessing and exact vLLM args
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_MODEL_COMMAND=custom
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_MODEL_COMMAND=custom
export LLMDBENCH_VLLM_COMMON_PREPROCESS="python3 /setup/preprocess/set_llmdbench_environment.py; source \$HOME/llmdbench_env.sh"
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS=$LLMDBENCH_VLLM_COMMON_PREPROCESS
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_PREPROCESS=$LLMDBENCH_VLLM_COMMON_PREPROCESS

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Lines 59-77 (vLLM command args):
#   exec vllm serve \
#     deepseek-ai/DeepSeek-R1-0528 \
#     --port 8200 \
#     --trust-remote-code \
#     --disable-uvicorn-access-log \
#     --data-parallel-hybrid-lb \
#     --enable-expert-parallel \
#     --tensor-parallel-size $TP_SIZE \
#     --data-parallel-size $((LWS_GROUP_SIZE * DP_SIZE_LOCAL)) \
#     --data-parallel-size-local $DP_SIZE_LOCAL \
#     --data-parallel-address ${LWS_LEADER_ADDRESS} \
#     --data-parallel-rpc-port 5555 \
#     --data-parallel-start-rank $START_RANK \
#     --kv_transfer_config '{"kv_connector":"NixlConnector","kv_role":"kv_both","kv_load_failure_policy":"fail"}' \
#     --async-scheduling \
#     --enable-dbo \
#     --dbo-decode-token-threshold 32 \
#     --enable-eplb \
#     --eplb-config '{"window_size":"1000","step_interval":"3000","num_redundant_experts":"32","log_balancedness":"False"}' \
#     --compilation_config '{"cudagraph_mode": "FULL_DECODE_ONLY"}' \
#     --kv-cache-memory-bytes=${KV_CACHE_MEMORY_BYTES-}
#
# NOTE: Port 8200 is used for decode (routing proxy listens on 8000)
# NOTE: kv_role is "kv_both" (not consumer/producer) for flexible P/D routing
# NOTE: Expert parallelism with load balancing enabled
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS
REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS; \
vllm serve /model-cache/models/REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--host 0.0.0.0 \
--served-model-name REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--port REPLACE_ENV_LLMDBENCH_VLLM_COMMON_METRICS_PORT \
--trust-remote-code \
--disable-uvicorn-access-log \
--data-parallel-hybrid-lb \
--enable-expert-parallel \
--tensor-parallel-size REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM \
--kv_transfer_config '{"kv_connector":"NixlConnector","kv_role":"kv_both","kv_load_failure_policy":"fail"}' \
--async-scheduling \
--enable-dbo \
--dbo-decode-token-threshold 32 \
--enable-eplb \
--eplb-config '{"window_size":"1000","step_interval":"3000","num_redundant_experts":"32","log_balancedness":"False"}' \
--compilation_config '{"cudagraph_mode": "FULL_DECODE_ONLY"}'
EOF

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/prefill.yaml
# Lines 37-50 (vLLM command args):
#   exec vllm serve \
#     deepseek-ai/DeepSeek-R1-0528 \
#     --port 8000 \
#     --trust-remote-code \
#     --disable-uvicorn-access-log \
#     --data-parallel-hybrid-lb \
#     --enable-expert-parallel \
#     --tensor-parallel-size $TP_SIZE \
#     --data-parallel-size $((LWS_GROUP_SIZE * DP_SIZE_LOCAL)) \
#     --data-parallel-size-local $DP_SIZE_LOCAL \
#     --data-parallel-address ${LWS_LEADER_ADDRESS} \
#     --data-parallel-rpc-port 5555 \
#     --data-parallel-start-rank $START_RANK \
#     --kv_transfer_config '{"kv_connector":"NixlConnector","kv_role":"kv_both","kv_load_failure_policy":"fail"}' \
#     --async-scheduling \
#     --enable-dbo \
#     --dbo-prefill-token-threshold 32 \
#     --enable-eplb \
#     --eplb-config '{"window_size":"1000","step_interval":"3000","num_redundant_experts":"32","log_balancedness":"False"}' \
#     --gpu-memory-utilization 0.75
#
# NOTE: Port 8000 is used for prefill
# NOTE: Uses --dbo-prefill-token-threshold instead of decode threshold
# NOTE: Uses --gpu-memory-utilization 0.75
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_ACCELERATOR_MEM_UTIL=0.75

export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_ARGS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_ARGS
REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_PREFILL_PREPROCESS; \
vllm serve /model-cache/models/REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--host 0.0.0.0 \
--served-model-name REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--port REPLACE_ENV_LLMDBENCH_VLLM_COMMON_METRICS_PORT \
--trust-remote-code \
--disable-uvicorn-access-log \
--data-parallel-hybrid-lb \
--enable-expert-parallel \
--tensor-parallel-size REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_PREFILL_TENSOR_PARALLELISM \
--kv_transfer_config '{"kv_connector":"NixlConnector","kv_role":"kv_both","kv_load_failure_policy":"fail"}' \
--async-scheduling \
--enable-dbo \
--dbo-prefill-token-threshold 32 \
--enable-eplb \
--eplb-config '{"window_size":"1000","step_interval":"3000","num_redundant_experts":"32","log_balancedness":"False"}' \
--gpu-memory-utilization REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_PREFILL_ACCELERATOR_MEM_UTIL
EOF

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Lines 80-122 (environment variables):
#   - name: VLLM_MOE_DP_CHUNK_SIZE, value: "384"
#   - name: DP_SIZE_LOCAL, value: "8"
#   - name: TP_SIZE, value: "1"
#   - name: TRITON_LIBCUDA_PATH, value: /usr/lib64
#   - name: VLLM_SKIP_P2P_CHECK, value: "1"
#   - name: VLLM_RANDOMIZE_DP_DUMMY_INPUTS, value: "1"
#   - name: VLLM_USE_DEEP_GEMM, value: "1"
#   - name: VLLM_ALL2ALL_BACKEND, value: deepep_low_latency
#   - name: NVIDIA_GDRCOPY, value: enabled
#   - name: NVSHMEM_REMOTE_TRANSPORT, value: ibgda
#   - name: NVSHMEM_IB_ENABLE_IBGDA, value: "true"
#   - name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME, value: eth0
#   - name: GLOO_SOCKET_IFNAME, value: eth0
#   - name: NCCL_SOCKET_IFNAME, value: eth0
#   - name: VLLM_LOGGING_LEVEL, value: INFO
#   - name: VLLM_NIXL_SIDE_CHANNEL_HOST (fieldRef)
#   - name: CUDA_CACHE_PATH, value: /var/cache/vllm/cuda
#   - name: CCACHE_DIR, value: /var/cache/vllm/ccache
#   - name: VLLM_CACHE_ROOT, value: /var/cache/vllm/vllm
#   - name: FLASHINFER_WORKSPACE_BASE, value: /var/cache/vllm/flashinfer
#   - name: HF_HUB_CACHE, value: /var/cache/huggingface
#
# NOTE: Excluding DP_SIZE_LOCAL and TP_SIZE as they're set via parallelism vars
# NOTE: VLLM_NIXL_SIDE_CHANNEL_HOST uses fieldRef which isn't directly mappable
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_TO_YAML=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_TO_YAML
- name: VLLM_MOE_DP_CHUNK_SIZE
  value: "384"
- name: TRITON_LIBCUDA_PATH
  value: /usr/lib64
- name: VLLM_SKIP_P2P_CHECK
  value: "1"
- name: VLLM_RANDOMIZE_DP_DUMMY_INPUTS
  value: "1"
- name: VLLM_USE_DEEP_GEMM
  value: "1"
- name: VLLM_ALL2ALL_BACKEND
  value: deepep_low_latency
- name: NVIDIA_GDRCOPY
  value: enabled
- name: NVSHMEM_REMOTE_TRANSPORT
  value: ibgda
- name: NVSHMEM_IB_ENABLE_IBGDA
  value: "true"
- name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
  value: eth0
- name: GLOO_SOCKET_IFNAME
  value: eth0
- name: NCCL_SOCKET_IFNAME
  value: eth0
- name: VLLM_LOGGING_LEVEL
  value: INFO
- name: CUDA_CACHE_PATH
  value: /var/cache/vllm/cuda
- name: CCACHE_DIR
  value: /var/cache/vllm/ccache
- name: VLLM_CACHE_ROOT
  value: /var/cache/vllm/vllm
- name: FLASHINFER_WORKSPACE_BASE
  value: /var/cache/vllm/flashinfer
- name: HF_HUB_CACHE
  value: /var/cache/huggingface
- name: VLLM_NIXL_SIDE_CHANNEL_HOST
  valueFrom:
    fieldRef:
      fieldPath: status.podIP
EOF

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/prefill.yaml
# Lines 52-74 (environment variables):
#   - name: DP_SIZE_LOCAL, value: "8"
#   - name: TP_SIZE, value: "1"
#   - name: TRITON_LIBCUDA_PATH, value: /usr/lib64
#   - name: VLLM_SKIP_P2P_CHECK, value: "1"
#   - name: VLLM_RANDOMIZE_DP_DUMMY_INPUTS, value: "1"
#   - name: VLLM_USE_DEEP_GEMM, value: "1"
#   - name: VLLM_ALL2ALL_BACKEND, value: deepep_high_throughput
#   - name: NVIDIA_GDRCOPY, value: enabled
#   - name: NVSHMEM_REMOTE_TRANSPORT, value: ibgda
#   - name: NVSHMEM_IB_ENABLE_IBGDA, value: "true"
#   - name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME, value: eth0
#   - name: GLOO_SOCKET_IFNAME, value: eth0
#   - name: NCCL_SOCKET_IFNAME, value: eth0
#   - name: VLLM_LOGGING_LEVEL, value: INFO
#   - name: VLLM_NIXL_SIDE_CHANNEL_HOST (fieldRef)
#   (plus cache path env vars)
#
# NOTE: Prefill uses deepep_high_throughput instead of low_latency
# NOTE: Prefill does NOT have VLLM_MOE_DP_CHUNK_SIZE
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_ENVVARS_TO_YAML=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_PREFILL_ENVVARS_TO_YAML
- name: TRITON_LIBCUDA_PATH
  value: /usr/lib64
- name: VLLM_SKIP_P2P_CHECK
  value: "1"
- name: VLLM_RANDOMIZE_DP_DUMMY_INPUTS
  value: "1"
- name: VLLM_USE_DEEP_GEMM
  value: "1"
- name: VLLM_ALL2ALL_BACKEND
  value: deepep_high_throughput
- name: NVIDIA_GDRCOPY
  value: enabled
- name: NVSHMEM_REMOTE_TRANSPORT
  value: ibgda
- name: NVSHMEM_IB_ENABLE_IBGDA
  value: "true"
- name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
  value: eth0
- name: GLOO_SOCKET_IFNAME
  value: eth0
- name: NCCL_SOCKET_IFNAME
  value: eth0
- name: VLLM_LOGGING_LEVEL
  value: INFO
- name: CUDA_CACHE_PATH
  value: /var/cache/vllm/cuda
- name: CCACHE_DIR
  value: /var/cache/vllm/ccache
- name: VLLM_CACHE_ROOT
  value: /var/cache/vllm/vllm
- name: FLASHINFER_WORKSPACE_BASE
  value: /var/cache/vllm/flashinfer
- name: HF_HUB_CACHE
  value: /var/cache/huggingface
- name: VLLM_NIXL_SIDE_CHANNEL_HOST
  valueFrom:
    fieldRef:
      fieldPath: status.podIP
EOF

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Lines 47-53 (volumes):
#   - name: dshm (emptyDir with Memory and 2Gi limit)
#   - name: hf-cache (emptyDir)
#   - name: jit-cache (emptyDir)
#
# PLUS Benchmark framework standard volumes (preprocesses)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES}
- name: preprocesses
  configMap:
    defaultMode: 0755
    name: llm-d-benchmark-preprocesses
- name: dshm
  emptyDir:
    medium: Memory
    sizeLimit: REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_SHM_MEM
- name: hf-cache
  emptyDir: {}
- name: jit-cache
  emptyDir: {}
EOF

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Lines 158-164 (volumeMounts):
#   - name: dshm, mountPath: /dev/shm
#   - name: hf-cache, mountPath: /var/cache/huggingface
#   - name: jit-cache, mountPath: /var/cache/vllm
#
# PLUS Benchmark framework standard mounts (preprocesses)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS}
- name: dshm
  mountPath: /dev/shm
- name: preprocesses
  mountPath: /setup/preprocess
- name: hf-cache
  mountPath: /var/cache/huggingface
- name: jit-cache
  mountPath: /var/cache/vllm
EOF

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/prefill.yaml
# Lines 25-31 (volumes): Same as decode
# Lines 126-131 (volumeMounts): Same as decode
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUMES=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUMES}
- name: preprocesses
  configMap:
    defaultMode: 0755
    name: llm-d-benchmark-preprocesses
- name: dshm
  emptyDir:
    medium: Memory
    sizeLimit: REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_PREFILL_SHM_MEM
- name: hf-cache
  emptyDir: {}
- name: jit-cache
  emptyDir: {}
EOF

export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUME_MOUNTS=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUME_MOUNTS}
- name: dshm
  mountPath: /dev/shm
- name: preprocesses
  mountPath: /setup/preprocess
- name: hf-cache
  mountPath: /var/cache/huggingface
- name: jit-cache
  mountPath: /var/cache/vllm
EOF

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/modelserver/base/decode.yaml
# Lines 24-35 (routing proxy init container):
#   - name: routing-proxy
#     args:
#       - --port=8000
#       - --vllm-port=8200
#       - --connector=nixlv2
#       - --zap-log-level=1
#       - --secure-proxy=false
#       - --enable-prefiller-sampling
#     image: ghcr.io/llm-d/llm-d-routing-sidecar:v0.5.0
#
# NOTE: Decode uses routing proxy, prefill does not
# =============================================================================
export LLMDBENCH_LLMD_ROUTINGSIDECAR_ENABLED=true
export LLMDBENCH_LLMD_ROUTINGSIDECAR_CONNECTOR=nixlv2

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/inferencepool.values.yaml
# Lines 15-16:
#   pluginsConfigFile: "custom-plugins.yaml"
#   pluginsCustomConfig:
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_PLUGINS_CONFIGFILE="custom-plugins.yaml"

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/inferencepool.values.yaml
# Lines 17-49 (full custom plugin config):
#   custom-plugins.yaml: |
#     apiVersion: inference.networking.x-k8s.io/v1alpha1
#     kind: EndpointPickerConfig
#     plugins:
#     - type: prefill-header-handler
#     - type: prefill-filter
#     - type: decode-filter
#     - type: random-picker
#       parameters:
#         maxNumOfEndpoints: 1
#     - type: pd-profile-handler
#       parameters:
#         threshold: 0
#         hashBlockSize: 5
#     schedulingProfiles:
#     - name: prefill
#       plugins:
#       - pluginRef: prefill-filter
#       - pluginRef: random-picker
#     - name: decode
#       plugins:
#       - pluginRef: decode-filter
#       - pluginRef: random-picker
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_CUSTOM_PLUGINS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_GAIE_CUSTOM_PLUGINS
custom-plugins.yaml: |
  apiVersion: inference.networking.x-k8s.io/v1alpha1
  kind: EndpointPickerConfig
  plugins:
  - type: prefill-header-handler
  - type: prefill-filter
  - type: decode-filter
  - type: random-picker
    parameters:
      maxNumOfEndpoints: 1
  - type: pd-profile-handler
    parameters:
      threshold: 0
      hashBlockSize: 5
  schedulingProfiles:
  - name: prefill
    plugins:
    - pluginRef: prefill-filter
    - pluginRef: random-picker
  - name: decode
    plugins:
    - pluginRef: decode-filter
    - pluginRef: random-picker
EOF

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/inferencepool.values.yaml
# Lines 3-5:
#   flags:
#     kv-cache-usage-percentage-metric: "vllm:kv_cache_usage_perc"
#     v: 1
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_FLAGS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_GAIE_FLAGS
kv-cache-usage-percentage-metric: "vllm:kv_cache_usage_perc"
v: 1
EOF

# =============================================================================
# SOURCE: guides/wide-ep-lws/manifests/inferencepool.values.yaml
# Lines 57-58:
#   provider:
#     name: istio
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GATEWAY_CLASS_NAME=istio

# =============================================================================
# SOURCE: Benchmark framework defaults
# =============================================================================
export LLMDBENCH_HARNESS_NAME=inference-perf
export LLMDBENCH_HARNESS_EXPERIMENT_PROFILE=sanity_random.yaml
export LLMDBENCH_CONTROL_WORK_DIR=~/data/wide-ep-lws
