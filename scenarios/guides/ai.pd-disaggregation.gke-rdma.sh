# PD-DISAGGREGATION GKE RDMA WELL LIT PATH
# Based on https://github.com/llm-d/llm-d/tree/main/guides/pd-disaggregation
# Variant: GKE with RDMA/RoCE networking (gke_pd_rdma)
# Generated by llm-d-benchmark guide converter
#
# This configuration enables Prefill/Decode disaggregation on GKE with RoCE
# networking for high-performance KV cache transfer between stages.
# Validated on 8xH200 cluster (a3-ultragpu-8g VMs) with RoCE networking.

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_gke_rdma.yaml
# Lines 29-41:
#     args:
#       - |
#         source /usr/local/gib/scripts/set_nccl_env.sh
#         vllm serve openai/gpt-oss-120b \
#           --port 8200 \
#           --served-model-name "openai/gpt-oss-120b" \
#           --tensor-parallel-size 8 \
#           --block-size 128 \
#           --kv-transfer-config '{"kv_connector":"NixlConnector", "kv_role":"kv_both"}' \
#           --disable-log-requests \
#           --disable-uvicorn-access-log \
#           --max-model-len 32000
# =============================================================================
export LLMDBENCH_DEPLOY_MODEL_LIST="openai/gpt-oss-120b"

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_gke_rdma.yaml
# Lines 20-21:
#   containers:
#   - image: ghcr.io/llm-d/llm-d-cuda:v0.5.0
# =============================================================================
export LLMDBENCH_LLMD_IMAGE_TAG="v0.5.0"

# =============================================================================
# SOURCE: guides/pd-disaggregation/helmfile.yaml.gotmpl
# Lines 56-58:
#     chart: llm-d-infra/llm-d-infra
#     version: v1.3.6
# =============================================================================
export LLMDBENCH_VLLM_INFRA_CHART_VERSION="v1.3.6"

# =============================================================================
# SOURCE: guides/pd-disaggregation/helmfile.yaml.gotmpl
# Lines 79-81:
#     chart: oci://registry.k8s.io/gateway-api-inference-extension/charts/inferencepool
#     version: v1.3.0
# =============================================================================
export LLMDBENCH_VLLM_GAIE_CHART_VERSION="v1.3.0"

# =============================================================================
# SOURCE: guides/pd-disaggregation/helmfile.yaml.gotmpl
# Lines 108-110:
#     chart: llm-d-modelservice/llm-d-modelservice
#     version: v0.4.5
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_CHART_VERSION="v0.4.5"

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_gke_rdma.yaml
# Line 36:
#           --block-size 128
# =============================================================================
export LLMDBENCH_VLLM_COMMON_BLOCK_SIZE=128

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_gke_rdma.yaml
# Line 40:
#           --max-model-len 32000
# =============================================================================
export LLMDBENCH_VLLM_COMMON_MAX_MODEL_LEN=32000

# =============================================================================
# Decode Stage Configuration
# =============================================================================

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_gke_rdma.yaml
# Line 2:
#   replicas: 1
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_REPLICAS=1

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_gke_rdma.yaml
# Lines 13-14:
#   parallelism:
#     tensor: 8
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM=8

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_gke_rdma.yaml
# Lines 47-52:
#     resources:
#       limits:
#         cpu: "128"
#         memory: 1Ti
#         nvidia.com/gpu: "8"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_NR=128
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_MEM=1Ti

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_gke_rdma.yaml
# Lines 72-75:
#     - name: shm
#       emptyDir:
#         medium: Memory
#         sizeLimit: 250Gi
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_SHM_MEM=250Gi

# =============================================================================
# SOURCE: Benchmark framework convention (not in guide)
# Using custom command to inject preprocessing and exact vLLM args
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_MODEL_COMMAND=custom
export LLMDBENCH_VLLM_COMMON_PREPROCESS="python3 /setup/preprocess/set_llmdbench_environment.py; source \$HOME/llmdbench_env.sh"
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS=$LLMDBENCH_VLLM_COMMON_PREPROCESS

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_gke_rdma.yaml
# Lines 28-41 (decode container command and args):
#     args:
#       - |
#         source /usr/local/gib/scripts/set_nccl_env.sh
#         vllm serve openai/gpt-oss-120b \
#           --port 8200 \
#           --served-model-name "openai/gpt-oss-120b" \
#           --tensor-parallel-size 8 \
#           --block-size 128 \
#           --kv-transfer-config '{"kv_connector":"NixlConnector", "kv_role":"kv_both"}' \
#           --disable-log-requests \
#           --disable-uvicorn-access-log \
#           --max-model-len 32000
#
# NOTE: Guide uses port 8200 for decode. Benchmark uses proxy pattern:
#   - INFERENCE_PORT (8000): proxy/sidecar listens here
#   - METRICS_PORT (8200): vLLM listens here
# NOTE: Guide uses "kv_both" role (experimental) instead of standard "kv_consumer"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS
REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS; \
source /usr/local/gib/scripts/set_nccl_env.sh; \
vllm serve /model-cache/models/REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--host 0.0.0.0 \
--served-model-name REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--port REPLACE_ENV_LLMDBENCH_VLLM_COMMON_METRICS_PORT \
--tensor-parallel-size REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM \
--block-size REPLACE_ENV_LLMDBENCH_VLLM_COMMON_BLOCK_SIZE \
--max-model-len REPLACE_ENV_LLMDBENCH_VLLM_COMMON_MAX_MODEL_LEN \
--kv-transfer-config '{"kv_connector":"NixlConnector", "kv_role":"kv_both"}' \
--disable-log-requests \
--disable-uvicorn-access-log
EOF

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_gke_rdma.yaml
# Lines 42-46 (decode container environment variables):
#     env:
#       - name: VLLM_NIXL_SIDE_CHANNEL_HOST
#         valueFrom:
#           fieldRef:
#             fieldPath: status.podIP
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_TO_YAML=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_TO_YAML
- name: VLLM_NIXL_SIDE_CHANNEL_HOST
  valueFrom:
    fieldRef:
      fieldPath: status.podIP
EOF

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_gke_rdma.yaml
# Lines 3-12 (GKE RDMA networking annotations):
#   podAnnotations:
#     networking.gke.io/default-interface: eth0
#     networking.gke.io/interfaces: |
#       [
#         {"interfaceName":"eth0","network":"default"},
#         {"interfaceName":"eth2","network":"rdma-0"},
#         ...
#         {"interfaceName":"eth9","network":"rdma-7"}
#       ]
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_PODANNOTATIONS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_PODANNOTATIONS
networking.gke.io/default-interface: eth0
networking.gke.io/interfaces: |
  [
    {"interfaceName":"eth0","network":"default"},
    {"interfaceName":"eth2","network":"rdma-0"},
    {"interfaceName":"eth3","network":"rdma-1"},
    {"interfaceName":"eth4","network":"rdma-2"},
    {"interfaceName":"eth5","network":"rdma-3"},
    {"interfaceName":"eth6","network":"rdma-4"},
    {"interfaceName":"eth7","network":"rdma-5"},
    {"interfaceName":"eth8","network":"rdma-6"},
    {"interfaceName":"eth9","network":"rdma-7"}
  ]
EOF

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_gke_rdma.yaml
# Lines 56-65 (decode volume mounts):
#     volumeMounts:
#       - mountPath: /usr/local/nvidia
#         name: library-dir-host
#       - mountPath: /usr/local/gib
#         name: gib
#       - name: metrics-volume
#         mountPath: /.config
#       - name: shm
#         mountPath: /dev/shm
#       - name: torch-compile-cache
#         mountPath: /.cache
#
# PLUS benchmark framework standard volume mounts (preprocesses)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS}
- name: dshm
  mountPath: /dev/shm
- name: preprocesses
  mountPath: /setup/preprocess
- mountPath: /usr/local/nvidia
  name: library-dir-host
- mountPath: /usr/local/gib
  name: gib
- name: metrics-volume
  mountPath: /.config
- name: torch-compile-cache
  mountPath: /.cache
EOF

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_gke_rdma.yaml
# Lines 66-83 (decode volumes):
#   volumes:
#     - name: shm
#       emptyDir:
#         medium: Memory
#         sizeLimit: 250Gi
#     - hostPath:
#         path: /home/kubernetes/bin/nvidia
#         type: ""
#       name: library-dir-host
#     - hostPath:
#         path: /home/kubernetes/bin/gib
#         type: ""
#       name: gib
#     - name: metrics-volume
#       emptyDir: {}
#     - name: torch-compile-cache
#       emptyDir: {}
#
# PLUS benchmark framework standard volumes (preprocesses configMap)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES}
- name: preprocesses
  configMap:
    defaultMode: 0755
    name: llm-d-benchmark-preprocesses
- name: dshm
  emptyDir:
    medium: Memory
    sizeLimit: REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_SHM_MEM
- hostPath:
    path: /home/kubernetes/bin/nvidia
    type: ""
  name: library-dir-host
- hostPath:
    path: /home/kubernetes/bin/gib
    type: ""
  name: gib
- name: metrics-volume
  emptyDir: {}
- name: torch-compile-cache
  emptyDir: {}
EOF

# =============================================================================
# Prefill Stage Configuration
# =============================================================================

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_gke_rdma.yaml
# Line 85:
#   replicas: 1
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_REPLICAS=1

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_gke_rdma.yaml
# Lines 96-97:
#   parallelism:
#     tensor: 8
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_TENSOR_PARALLELISM=8

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_gke_rdma.yaml
# Lines 130-135:
#     resources:
#       limits:
#         cpu: "128"
#         memory: 1Ti
#         nvidia.com/gpu: "8"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_CPU_NR=128
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_CPU_MEM=1Ti

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_gke_rdma.yaml
# Lines 149-152:
#     - name: shm
#       emptyDir:
#         medium: Memory
#         sizeLimit: 250Gi
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_SHM_MEM=250Gi

# =============================================================================
# SOURCE: Benchmark framework convention (not in guide)
# Using custom command to inject preprocessing and exact vLLM args
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_MODEL_COMMAND=custom
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_PREPROCESS=$LLMDBENCH_VLLM_COMMON_PREPROCESS

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_gke_rdma.yaml
# Lines 105-118 (prefill container command and args):
#     args:
#       - |
#         source /usr/local/gib/scripts/set_nccl_env.sh
#         vllm serve openai/gpt-oss-120b \
#           --port 8000 \
#           --served-model-name "openai/gpt-oss-120b" \
#           --tensor-parallel-size 8 \
#           --block-size 128 \
#           --kv-transfer-config '{"kv_connector":"NixlConnector", "kv_role":"kv_both"}' \
#           --disable-log-requests \
#           --disable-uvicorn-access-log \
#           --max-model-len 32000
#
# NOTE: Guide uses port 8000 for prefill. In P/D, prefill typically uses different port.
# NOTE: Guide uses "kv_both" role (experimental) instead of standard "kv_producer"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_ARGS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_ARGS
REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_PREFILL_PREPROCESS; \
source /usr/local/gib/scripts/set_nccl_env.sh; \
vllm serve /model-cache/models/REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--host 0.0.0.0 \
--served-model-name REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--port REPLACE_ENV_LLMDBENCH_VLLM_COMMON_METRICS_PORT \
--tensor-parallel-size REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_PREFILL_TENSOR_PARALLELISM \
--block-size REPLACE_ENV_LLMDBENCH_VLLM_COMMON_BLOCK_SIZE \
--max-model-len REPLACE_ENV_LLMDBENCH_VLLM_COMMON_MAX_MODEL_LEN \
--kv-transfer-config '{"kv_connector":"NixlConnector", "kv_role":"kv_both"}' \
--disable-log-requests \
--disable-uvicorn-access-log
EOF

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_gke_rdma.yaml
# Lines 119-123 (prefill container environment variables):
#     env:
#       - name: VLLM_NIXL_SIDE_CHANNEL_HOST
#         valueFrom:
#           fieldRef:
#             fieldPath: status.podIP
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_ENVVARS_TO_YAML=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_PREFILL_ENVVARS_TO_YAML
- name: VLLM_NIXL_SIDE_CHANNEL_HOST
  valueFrom:
    fieldRef:
      fieldPath: status.podIP
EOF

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_gke_rdma.yaml
# Lines 86-95 (GKE RDMA networking annotations):
#   podAnnotations:
#     networking.gke.io/default-interface: eth0
#     networking.gke.io/interfaces: |
#       [
#         {"interfaceName":"eth0","network":"default"},
#         {"interfaceName":"eth2","network":"rdma-0"},
#         ...
#         {"interfaceName":"eth9","network":"rdma-7"}
#       ]
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_PODANNOTATIONS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_PREFILL_PODANNOTATIONS
networking.gke.io/default-interface: eth0
networking.gke.io/interfaces: |
  [
    {"interfaceName":"eth0","network":"default"},
    {"interfaceName":"eth2","network":"rdma-0"},
    {"interfaceName":"eth3","network":"rdma-1"},
    {"interfaceName":"eth4","network":"rdma-2"},
    {"interfaceName":"eth5","network":"rdma-3"},
    {"interfaceName":"eth6","network":"rdma-4"},
    {"interfaceName":"eth7","network":"rdma-5"},
    {"interfaceName":"eth8","network":"rdma-6"},
    {"interfaceName":"eth9","network":"rdma-7"}
  ]
EOF

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_gke_rdma.yaml
# Lines 136-145 (prefill volume mounts):
#     volumeMounts:
#       - mountPath: /usr/local/nvidia
#         name: library-dir-host
#       - mountPath: /usr/local/gib
#         name: gib
#       - name: metrics-volume
#         mountPath: /.config
#       - name: shm
#         mountPath: /dev/shm
#       - name: torch-compile-cache
#         mountPath: /.cache
#
# PLUS benchmark framework standard volume mounts (preprocesses)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUME_MOUNTS=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUME_MOUNTS}
- name: dshm
  mountPath: /dev/shm
- name: preprocesses
  mountPath: /setup/preprocess
- mountPath: /usr/local/nvidia
  name: library-dir-host
- mountPath: /usr/local/gib
  name: gib
- name: metrics-volume
  mountPath: /.config
- name: torch-compile-cache
  mountPath: /.cache
EOF

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_gke_rdma.yaml
# Lines 146-160 (prefill volumes):
#   volumes:
#     - name: shm
#       emptyDir:
#         medium: Memory
#         sizeLimit: 250Gi
#     - hostPath:
#         path: /home/kubernetes/bin/nvidia
#         type: ""
#       name: library-dir-host
#     - hostPath:
#         path: /home/kubernetes/bin/gib
#         type: ""
#       name: gib
#     - name: metrics-volume
#       emptyDir: {}
#     - name: torch-compile-cache
#       emptyDir: {}
#
# PLUS benchmark framework standard volumes (preprocesses configMap)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUMES=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUMES}
- name: preprocesses
  configMap:
    defaultMode: 0755
    name: llm-d-benchmark-preprocesses
- name: dshm
  emptyDir:
    medium: Memory
    sizeLimit: REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_PREFILL_SHM_MEM
- hostPath:
    path: /home/kubernetes/bin/nvidia
    type: ""
  name: library-dir-host
- hostPath:
    path: /home/kubernetes/bin/gib
    type: ""
  name: gib
- name: metrics-volume
  emptyDir: {}
- name: torch-compile-cache
  emptyDir: {}
EOF

# =============================================================================
# GAIE Configuration
# =============================================================================

# =============================================================================
# SOURCE: guides/pd-disaggregation/gaie-pd/values.yaml
# Lines 6-9:
#   image:
#     name: llm-d-inference-scheduler
#     hub: ghcr.io/llm-d
#     tag: v0.5.0
# =============================================================================
export LLMDBENCH_LLMD_INFERENCESCHEDULER_IMAGE_TAG="v0.5.0"

# =============================================================================
# SOURCE: guides/pd-disaggregation/gaie-pd/values.yaml
# Lines 2-5:
#   flags:
#     # in vLLM 10.0+, the metric is renamed while upstream GAIE is still using the old name as default.
#     # See https://github.com/kubernetes-sigs/gateway-api-inference-extension/pull/1905.
#     kv-cache-usage-percentage-metric: "vllm:kv_cache_usage_perc"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_FLAGS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_GAIE_FLAGS
kv-cache-usage-percentage-metric: "vllm:kv_cache_usage_perc"
EOF

# =============================================================================
# SOURCE: guides/pd-disaggregation/gaie-pd/values.yaml
# Line 12:
#   pluginsConfigFile: "pd-config.yaml"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_PLUGINS_CONFIGFILE="pd-config.yaml"

# =============================================================================
# SOURCE: guides/pd-disaggregation/gaie-pd/values.yaml
# Lines 13-46:
#   pluginsCustomConfig:
#     pd-config.yaml: |
#       apiVersion: inference.networking.x-k8s.io/v1alpha1
#       kind: EndpointPickerConfig
#       plugins:
#       - type: prefill-header-handler
#       - type: prefill-filter
#       - type: decode-filter
#       - type: queue-scorer
#       - type: pd-profile-handler
#         parameters:
#           threshold: 0
#           hashBlockSize: 5
#       schedulingProfiles:
#       - name: prefill
#         plugins:
#         - pluginRef: prefill-filter
#         - pluginRef: queue-scorer
#           weight: 1.0
#       - name: decode
#         plugins:
#         - pluginRef: decode-filter
#         - pluginRef: queue-scorer
#           weight: 1.0
#       # All profiles using max score picker by default
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_CUSTOM_PLUGINS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_GAIE_CUSTOM_PLUGINS
pd-config.yaml: |
  # ALWAYS DO PD IN THIS EXAMPLE (THRESHOLD 0)
  apiVersion: inference.networking.x-k8s.io/v1alpha1
  kind: EndpointPickerConfig
  plugins:
  - type: prefill-header-handler
  - type: prefill-filter
  - type: decode-filter
  - type: queue-scorer
  - type: pd-profile-handler
    parameters:
      threshold: 0
      hashBlockSize: 5
  schedulingProfiles:
  - name: prefill
    plugins:
    - pluginRef: prefill-filter
    - pluginRef: queue-scorer
      weight: 1.0
  - name: decode
    plugins:
    - pluginRef: decode-filter
    - pluginRef: queue-scorer
      weight: 1.0
  # All profiles using max score picker by default
EOF

# =============================================================================
# SOURCE: guides/pd-disaggregation/gaie-pd/values.yaml
# Lines 60-62:
#   targetPorts:
#     - number: 8000
# =============================================================================
export LLMDBENCH_VLLM_COMMON_INFERENCE_PORT=8000

# =============================================================================
# SOURCE: guides/pd-disaggregation/helmfile.yaml.gotmpl
# Lines 16-18 (gke environment configuration):
#   gke_pd_rdma: &GKE
#     values:
#       - ../prereq/gateway-provider/common-configurations/gke.yaml
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GATEWAY_CLASS_NAME="gke"

# =============================================================================
# Harness Configuration
# =============================================================================

# =============================================================================
# SOURCE: Benchmark framework defaults
# =============================================================================
export LLMDBENCH_HARNESS_NAME=inference-perf
export LLMDBENCH_HARNESS_EXPERIMENT_PROFILE=sanity_random.yaml
export LLMDBENCH_CONTROL_WORK_DIR=~/data/pd-disaggregation-gke-rdma
