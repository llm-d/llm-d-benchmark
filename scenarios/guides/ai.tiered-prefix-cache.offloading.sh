# TIERED PREFIX CACHE (Native Offloading Connector) WELL LIT PATH
# Based on https://github.com/llm-d/llm-d/tree/main/guides/tiered-prefix-cache/cpu
# Variant: offloading-connector
# Generated by llm-d-benchmark guide converter
#
# Kustomize Hierarchy:
# guides/tiered-prefix-cache/cpu/manifests/vllm/offloading-connector/kustomization.yaml
#     └── ../base/kustomization.yaml
#             └── ../../../../../recipes/vllm/standard/kustomization.yaml
#                     └── ../base/kustomization.yaml
#                             └── deployment.yaml (base deployment with replicas: 2)

# =============================================================================
# SOURCE: guides/tiered-prefix-cache/cpu/manifests/vllm/offloading-connector/kustomization.yaml
# Lines 11-17:
#   value: |-
#     exec vllm serve \
#       Qwen/Qwen3-32B \
#       --tensor-parallel-size 2 \
#       --kv-transfer-config '{"kv_connector":"OffloadingConnector","kv_role":"kv_both","kv_connector_extra_config":{"cpu_bytes_to_use":107374182400}}' \
#       --port 8000 \
#       --max-num-seq 1024
# =============================================================================
export LLMDBENCH_DEPLOY_MODEL_LIST="Qwen/Qwen3-32B"

# =============================================================================
# SOURCE: guides/recipes/vllm/standard/kustomization.yaml
# Lines 7-9:
#   - name: INFERENCE_SERVER_IMAGE
#     newName: ghcr.io/llm-d/llm-d-cuda-dev
#     newTag: sha-5a98b4f
#
# NOTE: Using development image from standard recipe base
# =============================================================================
export LLMDBENCH_LLMD_IMAGE_NAME="llm-d-cuda-dev"
export LLMDBENCH_LLMD_IMAGE_TAG="sha-5a98b4f"

# =============================================================================
# SOURCE: guides/recipes/vllm/base/deployment.yaml
# Line 6: replicas: 2
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_REPLICAS=2

# =============================================================================
# SOURCE: guides/tiered-prefix-cache/cpu/manifests/vllm/offloading-connector/kustomization.yaml
# Lines 15-16:
#   --tensor-parallel-size 2 \
#   --kv-transfer-config '{"kv_connector":"OffloadingConnector",...}' \
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM=2

# =============================================================================
# SOURCE: guides/tiered-prefix-cache/cpu/manifests/vllm/offloading-connector/kustomization.yaml
# Lines 26-32:
#   limits:
#     nvidia.com/gpu: 2
#   requests:
#     nvidia.com/gpu: 2
#     memory: 400G
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_MEM="400Gi"

# =============================================================================
# SOURCE: Benchmark framework convention (not in guide)
# Using custom command to inject preprocessing and exact vLLM args
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_MODEL_COMMAND=custom
export LLMDBENCH_VLLM_COMMON_PREPROCESS="python3 /setup/preprocess/set_llmdbench_environment.py; source \$HOME/llmdbench_env.sh"
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS=$LLMDBENCH_VLLM_COMMON_PREPROCESS

# =============================================================================
# SOURCE: guides/tiered-prefix-cache/cpu/manifests/vllm/offloading-connector/kustomization.yaml
# Lines 12-16 (container args):
#   exec vllm serve \
#     Qwen/Qwen3-32B \
#     --tensor-parallel-size 2 \
#     --kv-transfer-config '{"kv_connector":"OffloadingConnector","kv_role":"kv_both","kv_connector_extra_config":{"cpu_bytes_to_use":107374182400}}' \
#     --port 8000 \
#     --max-num-seq 1024
#
# NOTE: Port mapping - Guide uses port 8000 but benchmark uses proxy pattern:
#   - INFERENCE_PORT (8000): proxy/sidecar listens here
#   - METRICS_PORT (8200): vLLM actually listens here
# NOTE: OffloadingConnector offloads ~100GB to CPU RAM (107374182400 bytes)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS
REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS; \
vllm serve /model-cache/models/REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--host 0.0.0.0 \
--served-model-name REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--port REPLACE_ENV_LLMDBENCH_VLLM_COMMON_METRICS_PORT \
--tensor-parallel-size REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM \
--max-num-seq 1024 \
--kv-transfer-config '{"kv_connector":"OffloadingConnector","kv_role":"kv_both","kv_connector_extra_config":{"cpu_bytes_to_use":107374182400}}'
EOF

# =============================================================================
# SOURCE: guides/recipes/vllm/base/deployment.yaml
# Lines 53-59 (base volumes):
#   volumes:
#     - name: data
#       emptyDir: {}
#     - name: shm
#       emptyDir:
#         medium: Memory
#
# PLUS benchmark framework standard volumes (preprocesses, etc.)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES}
- name: preprocesses
  configMap:
    defaultMode: 0755
    name: llm-d-benchmark-preprocesses
- name: data
  emptyDir: {}
- name: dshm
  emptyDir:
    medium: Memory
    sizeLimit: REPLACE_ENV_LLMDBENCH_VLLM_COMMON_SHM_MEM
EOF

export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS}
- name: dshm
  mountPath: /dev/shm
- name: preprocesses
  mountPath: /setup/preprocess
- name: data
  mountPath: /data
EOF

# =============================================================================
# SOURCE: Guide does not use P/D disaggregation
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_REPLICAS=0

# =============================================================================
# SOURCE: guides/tiered-prefix-cache/cpu/manifests/inferencepool/values.yaml
# Lines 11-13:
#   monitoring:
#     prometheus:
#       enable: true
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_MONITORING_ENABLED=true

# =============================================================================
# SOURCE: guides/tiered-prefix-cache/cpu/manifests/inferencepool/values.yaml
# Line 14: pluginsConfigFile: "custom-plugins.yaml"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_PLUGINS_CONFIGFILE="custom-plugins.yaml"

# =============================================================================
# SOURCE: guides/tiered-prefix-cache/cpu/manifests/inferencepool/values.yaml
# Lines 8-10:
#   flags:
#     kv-cache-usage-percentage-metric: "vllm:kv_cache_usage_perc"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_FLAGS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_GAIE_FLAGS
kv-cache-usage-percentage-metric: "vllm:kv_cache_usage_perc"
EOF

# =============================================================================
# SOURCE: guides/tiered-prefix-cache/cpu/manifests/inferencepool/values.yaml
# Lines 15-47:
#   pluginsCustomConfig:
#     custom-plugins.yaml: |
#       apiVersion: inference.networking.x-k8s.io/v1alpha1
#       kind: EndpointPickerConfig
#       plugins:
#       - type: queue-scorer
#       - type: kv-cache-utilization-scorer
#       - type: prefix-cache-scorer
#         name: gpu-prefix-cache-scorer
#       - type: prefix-cache-scorer
#         name: cpu-prefix-cache-scorer
#         parameters:
#           autoTune: false
#           lruCapacityPerServer: 41000
#       schedulingProfiles:
#       - name: default
#         plugins:
#         - pluginRef: queue-scorer
#           weight: 2.0
#         - pluginRef: kv-cache-utilization-scorer
#           weight: 2.0
#         - pluginRef: gpu-prefix-cache-scorer
#           weight: 1.0
#         - pluginRef: cpu-prefix-cache-scorer
#           weight: 1.0
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_CUSTOM_PLUGINS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_GAIE_CUSTOM_PLUGINS
custom-plugins.yaml: |
  apiVersion: inference.networking.x-k8s.io/v1alpha1
  kind: EndpointPickerConfig
  plugins:
  - type: queue-scorer
  - type: kv-cache-utilization-scorer
  - type: prefix-cache-scorer
    name: gpu-prefix-cache-scorer
  - type: prefix-cache-scorer
    name: cpu-prefix-cache-scorer
    parameters:
      autoTune: false
      lruCapacityPerServer: 41000
  schedulingProfiles:
  - name: default
    plugins:
    - pluginRef: queue-scorer
      weight: 2.0
    - pluginRef: kv-cache-utilization-scorer
      weight: 2.0
    - pluginRef: gpu-prefix-cache-scorer
      weight: 1.0
    - pluginRef: cpu-prefix-cache-scorer
      weight: 1.0
EOF

# =============================================================================
# SOURCE: Benchmark framework defaults
# =============================================================================
export LLMDBENCH_HARNESS_NAME=inference-perf
export LLMDBENCH_HARNESS_EXPERIMENT_PROFILE=sanity_random.yaml
export LLMDBENCH_CONTROL_WORK_DIR=~/data/tiered-prefix-cache-offloading
