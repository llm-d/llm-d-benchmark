# PD-DISAGGREGATION TPU WELL LIT PATH
# Based on https://github.com/llm-d/llm-d/tree/main/guides/pd-disaggregation
# Variant: TPU (Google Cloud TPU v6e)
# Generated by llm-d-benchmark guide converter

# This guide demonstrates Prefill/Decode disaggregation using vLLM's TPUConnector
# on Google Cloud TPU v6e accelerators with ct6e-standard-8t machine type.
# Requires GKE cluster with TPU node pools configured.

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_tpu.yaml
# Lines 5-7:
#   modelArtifacts:
#     uri: "hf://meta-llama/Llama-3.3-70B-Instruct"
#     name: "meta-llama/Llama-3.3-70B-Instruct"
# =============================================================================
export LLMDBENCH_DEPLOY_MODEL_LIST="meta-llama/Llama-3.3-70B-Instruct"

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_tpu.yaml
# Line 6:
#   size: 200Gi
# =============================================================================
export LLMDBENCH_VLLM_COMMON_PVC_MODEL_CACHE_SIZE=200Gi

# =============================================================================
# SOURCE: guides/pd-disaggregation/helmfile.yaml.gotmpl
# Lines 38-41:
#   - name: infra-{$rn}
#     chart: llm-d-infra/llm-d-infra
#     version: v1.3.6
# =============================================================================
export LLMDBENCH_VLLM_INFRA_CHART_VERSION="v1.3.6"

# =============================================================================
# SOURCE: guides/pd-disaggregation/helmfile.yaml.gotmpl
# Lines 53-56:
#   - name: gaie-{$rn}
#     chart: oci://registry.k8s.io/gateway-api-inference-extension/charts/inferencepool
#     version: v1.3.0
# =============================================================================
export LLMDBENCH_VLLM_GAIE_CHART_VERSION="v1.3.0"

# =============================================================================
# SOURCE: guides/pd-disaggregation/helmfile.yaml.gotmpl
# Lines 83-86:
#   - name: ms-{$rn}
#     chart: llm-d-modelservice/llm-d-modelservice
#     version: v0.4.5
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_CHART_VERSION="v0.4.5"

# =============================================================================
# SOURCE: guides/pd-disaggregation/gaie-pd/values.yaml
# Lines 7-10:
#   image:
#     name: llm-d-inference-scheduler
#     hub: ghcr.io/llm-d
#     tag: v0.5.0
# =============================================================================
export LLMDBENCH_LLMD_INFERENCESCHEDULER_IMAGE_TAG="v0.5.0"

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_tpu.yaml
# Lines 3-4:
#   accelerator:
#     type: google
# =============================================================================
export LLMDBENCH_VLLM_COMMON_ACCELERATOR_RESOURCE="google.com/tpu"

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_tpu.yaml
# Lines 14-18:
#   routing:
#     servicePort: 8000
#     proxy:
#       image: ghcr.io/llm-d/llm-d-routing-sidecar:v0.5.0
#       connector: nixlv2
# =============================================================================
export LLMDBENCH_VLLM_COMMON_INFERENCE_PORT=8000
export LLMDBENCH_LLMD_ROUTINGSIDECAR_IMAGE_TAG="v0.5.0"
export LLMDBENCH_LLMD_ROUTINGSIDECAR_CONNECTOR="nixlv2"
export LLMDBENCH_LLMD_ROUTINGSIDECAR_ENABLED=true

# =============================================================================
# DECODE STAGE CONFIGURATION
# =============================================================================

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_tpu.yaml
# Lines 20-24:
#   decode:
#     parallelism:
#       tensor: 8
#     replicas: 1
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_REPLICAS=1
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM=8

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_tpu.yaml
# Lines 25-28:
#   extraConfig:
#     nodeSelector:
#       cloud.google.com/gke-tpu-topology: "2x4"
#       cloud.google.com/gke-tpu-accelerator: "tpu-v6e-slice"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_POD_CONFIG=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_POD_CONFIG
nodeSelector:
  cloud.google.com/gke-tpu-topology: "2x4"
  cloud.google.com/gke-tpu-accelerator: "tpu-v6e-slice"
EOF

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_tpu.yaml
# Lines 35-36:
#   containers:
#     - image: vllm/vllm-tpu:v0.13.2-ironwood
#
# NOTE: Using different decode and prefill images as specified in guide
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_IMAGE="vllm/vllm-tpu:v0.13.2-ironwood"

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_tpu.yaml
# Lines 60-64:
#   resources:
#     limits:
#       memory: 64Gi
#       cpu: "16"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_MEM=64Gi
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_NR=16

# =============================================================================
# SOURCE: Benchmark framework convention (not in guide)
# Using custom command to inject preprocessing and exact vLLM args
# =============================================================================
export LLMDBENCH_VLLM_COMMON_PREPROCESS="python3 /setup/preprocess/set_llmdbench_environment.py; source \$HOME/llmdbench_env.sh"
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_MODEL_COMMAND=custom
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS=$LLMDBENCH_VLLM_COMMON_PREPROCESS

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_tpu.yaml
# Lines 38-47:
#   args:
#     - "--tensor-parallel-size"
#     - "8"
#     - "--kv-transfer-config"
#     - '{"kv_connector":"TPUConnector", "kv_connector_module_path" : "tpu_inference.distributed.tpu_connector", "kv_role":"kv_both", "kv_ip" : "$(POD_IP)"}'
#     - "--disable-uvicorn-access-log"
#     - "--max-model-len"
#     - "32000"
#
# NOTE: Port mapping - vLLM listens on port 8000 for decode stage (different from typical 8200)
# TPU uses kv_both role for decode stage (can both produce and consume)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS
REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS; \
vllm serve /model-cache/models/REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--host 0.0.0.0 \
--served-model-name REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--port 8000 \
--tensor-parallel-size REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM \
--kv-transfer-config '{"kv_connector":"TPUConnector", "kv_connector_module_path" : "tpu_inference.distributed.tpu_connector", "kv_role":"kv_both", "kv_ip" : "\$(POD_IP)"}' \
--disable-uvicorn-access-log \
--max-model-len 32000
EOF

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_tpu.yaml
# Lines 48-56:
#   env:
#     - name: POD_IP
#       valueFrom:
#         fieldRef:
#           fieldPath: status.podIP
#     - name: TPU_SIDE_CHANNEL_PORT
#       value: "9600"
#     - name: TPU_KV_TRANSFER_PORT
#       value: "9100"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_TO_YAML=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_TO_YAML
- name: POD_IP
  valueFrom:
    fieldRef:
      fieldPath: status.podIP
- name: TPU_SIDE_CHANNEL_PORT
  value: "9600"
- name: TPU_KV_TRANSFER_PORT
  value: "9100"
EOF

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_tpu.yaml
# Lines 57-66:
#   ports:
#     - containerPort: 8000
#       name: vllm
#     - containerPort: 9100
#       name: tpu-kv-transfer
#     - containerPort: 9600
#       name: tpu-pd-coordination
#   volumeMounts:
#     - name: metrics-volume
#       mountPath: /.config
#
# PLUS Benchmark framework standard volumes (preprocesses)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS
- name: preprocesses
  mountPath: /setup/preprocess
- name: metrics-volume
  mountPath: /.config
- name: shm
  mountPath: /dev/shm
- name: torch-compile-cache
  mountPath: /.cache
EOF

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_tpu.yaml
# Lines 86-94:
#   volumes:
#     - name: metrics-volume
#       emptyDir: {}
#     - name: shm
#       emptyDir:
#         medium: Memory
#         sizeLimit: "16Gi"
#     - name: torch-compile-cache
#       emptyDir: {}
#
# PLUS Benchmark framework standard volumes (preprocesses configMap - MUST be first)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES
- name: preprocesses
  configMap:
    defaultMode: 0755
    name: llm-d-benchmark-preprocesses
- name: metrics-volume
  emptyDir: {}
- name: shm
  emptyDir:
    medium: Memory
    sizeLimit: 16Gi
- name: torch-compile-cache
  emptyDir: {}
EOF

# =============================================================================
# PREFILL STAGE CONFIGURATION
# =============================================================================

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_tpu.yaml
# Lines 96-100:
#   prefill:
#     parallelism:
#       tensor: 8
#     replicas: 2
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_REPLICAS=2
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_TENSOR_PARALLELISM=8

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_tpu.yaml
# Lines 101-104:
#   extraConfig:
#     nodeSelector:
#       cloud.google.com/gke-tpu-topology: "2x4"
#       cloud.google.com/gke-tpu-accelerator: "tpu-v6e-slice"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_POD_CONFIG=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_POD_CONFIG
nodeSelector:
  cloud.google.com/gke-tpu-topology: "2x4"
  cloud.google.com/gke-tpu-accelerator: "tpu-v6e-slice"
EOF

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_tpu.yaml
# Lines 111-112:
#   containers:
#     - image: vllm/vllm-tpu:v0.13.2
#
# NOTE: Prefill uses v0.13.2 while decode uses v0.13.2-ironwood
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_IMAGE="vllm/vllm-tpu:v0.13.2"

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_tpu.yaml
# Lines 136-140:
#   resources:
#     limits:
#       memory: 64Gi
#       cpu: "16"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_CPU_MEM=64Gi
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_CPU_NR=16

# =============================================================================
# SOURCE: Benchmark framework convention (not in guide)
# Using custom command to inject preprocessing and exact vLLM args
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_MODEL_COMMAND=custom
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_PREPROCESS=$LLMDBENCH_VLLM_COMMON_PREPROCESS

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_tpu.yaml
# Lines 114-122:
#   args:
#     - "--tensor-parallel-size"
#     - "8"
#     - "--kv-transfer-config"
#     - '{"kv_connector":"TPUConnector", "kv_connector_module_path" : "tpu_inference.distributed.tpu_connector", "kv_role":"kv_producer", "kv_ip" : "$(POD_IP)"}'
#     - "--disable-uvicorn-access-log"
#     - "--max-model-len"
#     - "32000"
#
# NOTE: Prefill uses kv_producer role (only produces KV cache)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_ARGS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_ARGS
REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_PREFILL_PREPROCESS; \
vllm serve /model-cache/models/REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--host 0.0.0.0 \
--served-model-name REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--port 8000 \
--tensor-parallel-size REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_PREFILL_TENSOR_PARALLELISM \
--kv-transfer-config '{"kv_connector":"TPUConnector", "kv_connector_module_path" : "tpu_inference.distributed.tpu_connector", "kv_role":"kv_producer", "kv_ip" : "\$(POD_IP)"}' \
--disable-uvicorn-access-log \
--max-model-len 32000
EOF

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_tpu.yaml
# Lines 123-131:
#   env:
#     - name: POD_IP
#       valueFrom:
#         fieldRef:
#           fieldPath: status.podIP
#     - name: TPU_SIDE_CHANNEL_PORT
#       value: "9600"
#     - name: TPU_KV_TRANSFER_PORT
#       value: "9100"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_ENVVARS_TO_YAML=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_PREFILL_ENVVARS_TO_YAML
- name: POD_IP
  valueFrom:
    fieldRef:
      fieldPath: status.podIP
- name: TPU_SIDE_CHANNEL_PORT
  value: "9600"
- name: TPU_KV_TRANSFER_PORT
  value: "9100"
EOF

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_tpu.yaml
# Lines 132-141:
#   ports:
#     - containerPort: 8000
#       name: vllm
#     - containerPort: 9100
#       name: tpu-kv-transfer
#     - containerPort: 9600
#       name: tpu-pd-coordination
#   volumeMounts:
#     - name: metrics-volume
#       mountPath: /.config
#
# PLUS Benchmark framework standard volumes (preprocesses)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUME_MOUNTS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUME_MOUNTS
- name: preprocesses
  mountPath: /setup/preprocess
- name: metrics-volume
  mountPath: /.config
- name: shm
  mountPath: /dev/shm
- name: torch-compile-cache
  mountPath: /.cache
EOF

# =============================================================================
# SOURCE: guides/pd-disaggregation/ms-pd/values_tpu.yaml
# Lines 162-170:
#   volumes:
#     - name: metrics-volume
#       emptyDir: {}
#     - name: shm
#       emptyDir:
#         medium: Memory
#         sizeLimit: "16Gi"
#     - name: torch-compile-cache
#       emptyDir: {}
#
# PLUS Benchmark framework standard volumes (preprocesses configMap - MUST be first)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUMES=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_VOLUMES
- name: preprocesses
  configMap:
    defaultMode: 0755
    name: llm-d-benchmark-preprocesses
- name: metrics-volume
  emptyDir: {}
- name: shm
  emptyDir:
    medium: Memory
    sizeLimit: 16Gi
- name: torch-compile-cache
  emptyDir: {}
EOF

# =============================================================================
# GAIE (GATEWAY API INFERENCE EXTENSION) CONFIGURATION
# =============================================================================

# =============================================================================
# SOURCE: guides/pd-disaggregation/gaie-pd/values.yaml
# Lines 12-13:
#   pluginsConfigFile: "pd-config.yaml"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_PLUGINS_CONFIGFILE="pd-config.yaml"

# =============================================================================
# SOURCE: guides/pd-disaggregation/gaie-pd/values.yaml
# Lines 14-46:
#   pluginsCustomConfig:
#     pd-config.yaml: |
#       apiVersion: inference.networking.x-k8s.io/v1alpha1
#       kind: EndpointPickerConfig
#       plugins:
#       - type: prefill-header-handler
#       - type: prefill-filter
#       - type: decode-filter
#       - type: queue-scorer
#       - type: pd-profile-handler
#         parameters:
#           threshold: 0
#           hashBlockSize: 5
#       schedulingProfiles:
#       - name: prefill
#         plugins:
#         - pluginRef: prefill-filter
#         - pluginRef: queue-scorer
#           weight: 1.0
#       - name: decode
#         plugins:
#         - pluginRef: decode-filter
#         - pluginRef: queue-scorer
#           weight: 1.0
#
# NOTE: This configuration enforces P/D disaggregation (threshold: 0)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_CUSTOM_PLUGINS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_GAIE_CUSTOM_PLUGINS
pd-config.yaml: |
  # ALWAYS DO PD IN THIS EXAMPLE (THRESHOLD 0)
  apiVersion: inference.networking.x-k8s.io/v1alpha1
  kind: EndpointPickerConfig
  plugins:
  - type: prefill-header-handler
  - type: prefill-filter
  - type: decode-filter
  - type: queue-scorer
  - type: pd-profile-handler
    parameters:
      threshold: 0
      hashBlockSize: 5
  schedulingProfiles:
  - name: prefill
    plugins:
    - pluginRef: prefill-filter
    - pluginRef: queue-scorer
      weight: 1.0
  - name: decode
    plugins:
    - pluginRef: decode-filter
    - pluginRef: queue-scorer
      weight: 1.0
  # All profiles using max score picker by default
EOF

# =============================================================================
# SOURCE: guides/pd-disaggregation/gaie-pd/values.yaml
# Lines 3-5:
#   flags:
#     kv-cache-usage-percentage-metric: "vllm:kv_cache_usage_perc"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_FLAGS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_GAIE_FLAGS
kv-cache-usage-percentage-metric: "vllm:kv_cache_usage_perc"
EOF

# =============================================================================
# SOURCE: guides/pd-disaggregation/helmfile.yaml.gotmpl
# Lines 63-65:
#   provider:
#     name: gke
#
# NOTE: TPU deployment uses GKE gateway provider
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GATEWAY_CLASS_NAME="gke"

# =============================================================================
# SOURCE: Benchmark framework defaults
# =============================================================================
export LLMDBENCH_HARNESS_NAME=inference-perf
export LLMDBENCH_HARNESS_EXPERIMENT_PROFILE=sanity_random.yaml
export LLMDBENCH_CONTROL_WORK_DIR=~/data/pd-disaggregation-tpu
