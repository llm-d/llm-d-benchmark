# A scenario to capture running inference-sim on a Kind cluster without requiring GPUs
export LLMDBENCH_DEPLOY_METHODS=modelservice
export LLMDBENCH_VLLM_COMMON_REPLICAS=1
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_ACCELERATOR_NR=0
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_ACCELERATOR_NR=0
export LLMDBENCH_VLLM_COMMON_AFFINITY=kubernetes.io/os:linux
export LLMDBENCH_LLMD_IMAGE_NAME="llm-d-inference-sim"
export LLMDBENCH_LLMD_ROUTINGSIDECAR_IMAGE_TAG="v0.2.0@sha256:a623a0752af0a71b7b05ebf95517848b5dbc3d8d235c1897035905632d5b7d80"
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_MODEL_COMMAND=imageDefault
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_MODEL_COMMAND=imageDefault
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS="[]"
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_ARGS="[]"
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_NR=0
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_CPU_NR=0
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_MEM=100Mi
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_CPU_MEM=100Mi
export LLMDBENCH_VLLM_MODELSERVICE_URI_PROTOCOL="hf"
export LLMDBENCH_DEPLOY_MODEL_LIST="facebook/opt-125m"
export LLMDBENCH_HARNESS_PVC_SIZE=3Gi
export LLMDBENCH_VLLM_MODELSERVICE_INFERENCE_MODEL=true

