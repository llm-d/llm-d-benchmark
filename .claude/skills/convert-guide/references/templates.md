# Templates Reference

This document contains the templates for generating scenario and experiment files from llm-d guides.

## Table of Contents

- [Scenario File Template](#scenario-file-template)
- [Experiment File Template](#experiment-file-template)
- [Kustomize Documentation Format](#kustomize-documentation-format)
- [Kustomize Patch Mappings](#kustomize-patch-mappings)
- [Display Format Examples](#display-format-examples)

## Scenario File Template

**CRITICAL: Source Traceability Requirements**

Every environment variable in the generated scenario file MUST include documentation showing its source. This enables:
- Verification that the conversion is accurate
- Easy updates when the guide changes
- Clear distinction between guide values and benchmark framework additions

**Documentation Format for Each Variable:**

```bash
# =============================================================================
# SOURCE: <relative-path-to-source-file>
# Lines <line-numbers>:
#   <original YAML/JSON content from source, indented>
#
# [Optional] NOTE: <any transformations or benchmark framework additions>
# =============================================================================
export LLMDBENCH_<VARIABLE_NAME>=<value>
```

**Source Categories:**

1. **From Guide Files**: Reference the exact file path and line numbers
   ```bash
   # SOURCE: guides/tiered-prefix-cache/cpu/manifests/vllm/lmcache-connector/kustomization.yaml
   # Lines 23-24:
   #   - "--tensor-parallel-size"
   #   - "2"
   ```

2. **From Base/Parent Files**: Include the kustomize hierarchy when relevant
   ```bash
   # SOURCE: guides/recipes/vllm/base/deployment.yaml
   # Line 6: replicas: 2
   ```

3. **From Benchmark Framework**: Clearly mark additions not in the guide
   ```bash
   # SOURCE: Benchmark framework convention (not in guide)
   # Using custom command to inject preprocessing and exact vLLM args
   ```

4. **From Framework Defaults**: Mark when using defaults
   ```bash
   # SOURCE: Benchmark framework default (not explicitly in guide)
   ```

### Full Scenario File Template

```bash
# <GUIDE_NAME> WELL LIT PATH
# Based on <SOURCE_URL_OR_PATH>
# Variant: <variant-name-if-applicable>
# Generated by llm-d-benchmark guide converter

# =============================================================================
# SOURCE: <path-to-kustomization-or-values.yaml>
# Lines <N-M>: args: ["serve", "<model-name>", ...]
# =============================================================================
export LLMDBENCH_DEPLOY_MODEL_LIST="<model_name>"

# =============================================================================
# SOURCE: <path-to-source-file>
# Lines <N-M>:
#   - op: replace
#     path: /spec/template/spec/containers/0/image
#     value: <image:tag>
# NOTE: <any notes about image handling>
# =============================================================================
export LLMDBENCH_LLMD_IMAGE_TAG="<version>"

# =============================================================================
# SOURCE: Benchmark framework default (not in guide)
# The guide doesn't specify PVC size; using framework default
# =============================================================================
export LLMDBENCH_VLLM_COMMON_PVC_MODEL_CACHE_SIZE=<size>

# =============================================================================
# SOURCE: <path-to-source-file>
# Lines <N-M>:
#   - "--gpu-memory-utilization"
#   - "<value>"
# =============================================================================
export LLMDBENCH_VLLM_COMMON_ACCELERATOR_MEM_UTIL=<value>

# =============================================================================
# SOURCE: <path-to-base-deployment.yaml>
# Line <N>: replicas: <value>
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_REPLICAS=<value>

# =============================================================================
# SOURCE: <path-to-source-file>
# Lines <N-M>:
#   - "--tensor-parallel-size"
#   - "<value>"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM=<value>

# =============================================================================
# SOURCE: <path-to-source-file>
# Lines <N-M>:
#   requests:
#     nvidia.com/gpu: <value>
#     memory: <value>
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_MEM=<value>

# =============================================================================
# SOURCE: Benchmark framework convention (not in guide)
# Using custom command to inject preprocessing and exact vLLM args
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_MODEL_COMMAND=custom
export LLMDBENCH_VLLM_COMMON_PREPROCESS="python3 /setup/preprocess/set_llmdbench_environment.py; source \$HOME/llmdbench_env.sh"
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS=$LLMDBENCH_VLLM_COMMON_PREPROCESS

# =============================================================================
# SOURCE: <path-to-source-file>
# Lines <N-M> (container command and args):
#   - op: replace
#     path: /spec/template/spec/containers/0/args
#     value:
#       - "serve"
#       - "<model>"
#       - "--tensor-parallel-size"
#       - "<value>"
#       - "--port"
#       - "<port>"
#       - "<additional-args>"
#
# NOTE: Port mapping - Guide uses <guide-port> but benchmark uses proxy pattern:
#   - INFERENCE_PORT (8000): proxy/sidecar listens here
#   - METRICS_PORT (8200): vLLM actually listens here
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS
REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_PREPROCESS; \
vllm serve /model-cache/models/REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--host 0.0.0.0 \
--served-model-name REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--port REPLACE_ENV_LLMDBENCH_VLLM_COMMON_METRICS_PORT \
<...vllm flags extracted from guide...>
EOF

# =============================================================================
# SOURCE: <path-to-source-file>
# Lines <N-M> (environment variables added via JSON patches):
#   - op: add
#     path: /spec/template/spec/containers/0/env/-
#     value:
#       name: <ENV_VAR_NAME>
#       value: "<value>"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_TO_YAML=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_TO_YAML
- name: <ENV_VAR_NAME>
  value: "<value>"
EOF

# =============================================================================
# SOURCE: <path-to-source-file>
# Lines <N-M> (volume added via JSON patch):
#   - op: add
#     path: /spec/template/spec/volumes/-
#     value:
#       name: <volume-name>
#       emptyDir: {}
#
# PLUS <path-to-base-deployment.yaml> Lines <N-M> (base volumes):
#   volumes:
#     - name: data
#       emptyDir: {}
#     - name: shm
#       emptyDir:
#         medium: Memory
#
# PLUS benchmark framework standard volumes (preprocesses, metrics-volume, etc.)
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES}
- name: preprocesses
  configMap:
    defaultMode: 0755
    name: llm-d-benchmark-preprocesses
<...additional volumes...>
EOF

# =============================================================================
# SOURCE: Guide does not use P/D disaggregation
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_REPLICAS=0

# =============================================================================
# SOURCE: <path-to-inferencepool-values.yaml>
# Line <N>: pluginsConfigFile: "<filename>"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_PLUGINS_CONFIGFILE="<filename>"

# =============================================================================
# SOURCE: <path-to-inferencepool-values.yaml>
# Lines <N-M>:
#   inferenceExtension:
#     flags:
#       <flag-name>: "<value>"
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_FLAGS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_GAIE_FLAGS
<flag-name>: "<value>"
EOF

# =============================================================================
# SOURCE: <path-to-inferencepool-values.yaml>
# Lines <N-M>:
#   pluginsConfigFile: "<filename>"
#   pluginsCustomConfig:
#     <filename>: |
#       <full-yaml-content>
# =============================================================================
export LLMDBENCH_VLLM_MODELSERVICE_GAIE_CUSTOM_PLUGINS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_GAIE_CUSTOM_PLUGINS
<filename>: |
  <full-yaml-content>
EOF

# =============================================================================
# SOURCE: Benchmark framework defaults
# =============================================================================
export LLMDBENCH_HARNESS_NAME=<harness>
export LLMDBENCH_HARNESS_EXPERIMENT_PROFILE=<profile>.yaml
export LLMDBENCH_CONTROL_WORK_DIR=~/data/<guide_name>
```

## Experiment File Template

Create a YAML file for parameter sweeps with constants and factors:

```yaml
setup:
  # Constants: Fixed infrastructure parameters for all setup treatments
  constants:
    - LLMDBENCH_VLLM_COMMON_MAX_MODEL_LEN: <value>
    - LLMDBENCH_VLLM_COMMON_BLOCK_SIZE: <value>
  # Factors: Parameters to vary across setup treatments
  factors:
    - <FACTOR_NAME>
  levels:
    <FACTOR_NAME>: "<value1>, <value2>, ..."
  treatments:
    - "<value1>"
    - "<value2>"
    # ...

run:
  # Constants: Fixed workload parameters for all run treatments
  constants:
    - streaming: <true|false>
    - <other_fixed_param>: <value>
  # Factors: Parameters to vary across run treatments
  factors:
    - <run_factor>
  levels:
    <run_factor>: "<values>"
  treatments:
    <label1>: "<treatment1>"  # Named treatments for clarity
    <label2>: "<treatment2>"
```

**Constants vs Factors:**
- **Constants**: Fixed values that apply to ALL treatments in their section (setup or run)
  - Setup constants: Infrastructure configs (max_model_len, block_size, etc.)
  - Run constants: Workload configs (streaming, rate, etc.)
- **Factors**: Parameters that vary across treatments (the experiment variables)
- **Treatments**: Can have optional labels for clarity (e.g., `long: "40,8000"`)

**When to Use Constants:**

Use **setup constants** when you want to:
- Override scenario file values for the entire experiment
- Ensure consistency across all infrastructure variations
- Example: Fix max_model_len=12000 and block_size=64 for all GAIE plugin tests

Use **run constants** when you want to:
- Set fixed workload parameters across all run treatments
- Override workload profile defaults consistently
- Example: Set streaming=true for all workload variations

**Important:** Constants override both scenario file values and workload profile defaults. They ensure the specified value is used for ALL treatments in their section.

## Kustomize Documentation Format

When the guide uses kustomize, document the full inheritance chain at the top of the file:

```bash
# Kustomize Hierarchy:
# <guide>/manifests/vllm/lmcache-connector/kustomization.yaml
#     └── ../base/kustomization.yaml
#             └── ../../../../../recipes/vllm/standard/kustomization.yaml
#                     └── ../base/kustomization.yaml
#                             └── deployment.yaml (the actual base deployment)
```

**Common Kustomize Patterns:**

```
guides/<guide-name>/manifests/vllm/<variant>/kustomization.yaml
    └── ../base/kustomization.yaml
            └── ../../../../../recipes/vllm/standard/kustomization.yaml
                    └── ../base/kustomization.yaml
                            └── deployment.yaml
```

## Kustomize Patch Mappings

| Kustomize Patch Path | LLMDBENCH Variable |
|---------------------|-------------------|
| `/spec/template/spec/containers/0/image` | `LLMDBENCH_LLMD_IMAGE_TAG` or `LLMDBENCH_VLLM_MODELSERVICE_DECODE_IMAGE` |
| `/spec/template/spec/containers/0/args` | `LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS` |
| `/spec/template/spec/containers/0/env/-` | `LLMDBENCH_VLLM_MODELSERVICE_DECODE_ENVVARS_TO_YAML` |
| `/spec/template/spec/containers/0/resources` | `LLMDBENCH_VLLM_MODELSERVICE_DECODE_CPU_MEM`, `_TENSOR_PARALLELISM` |
| `/spec/template/spec/volumes/-` | `LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES` |
| `/spec/template/spec/containers/0/volumeMounts/-` | `LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS` |
| `/spec/replicas` | `LLMDBENCH_VLLM_MODELSERVICE_DECODE_REPLICAS` |

## Display Format Examples

**If NO testing specifications provided (scenario file only):**

```
=== Conversion Summary ===

## Scenario File: scenarios/guides/<guide_name>.sh

Source configuration files:
- GAIE config: <path-to-gaie-values-file>
- ModelService config: <path-to-ms-values-file-or-kustomization>

<scenario content>

[If --auto NOT present] Would you like me to write this file? (yes/no)
```

**If testing specifications provided (scenario + experiment files):**

```
=== Conversion Summary ===

## 1. Scenario File: scenarios/guides/<guide_name>.sh

Source configuration files:
- GAIE config: <path-to-gaie-values-file>
- ModelService config: <path-to-ms-values-file-or-kustomization>

<scenario content>

## 2. Experiment File: experiments/<guide_name>.yaml

<experiment content>

[If --auto NOT present] Would you like me to write these files? (yes/no)
```

**Notes on Source Files:**
- For GAIE config: List the actual values.yaml or kustomization file path that contains `inferenceExtension` configuration
- For ModelService config: List the values.yaml, kustomization.yaml, or patch file that contains model server configuration
- Use relative paths from the guide root when possible (e.g., `cpu/manifests/inferencepool/values.yaml`)
- If configuration comes from multiple files (e.g., base + kustomization patches), list the primary file that was used

**Success Output (only show AFTER Write tool succeeds):**

```
✅ Scenario file created successfully!

File written: scenarios/guides/<guide_name>.sh

Source configuration files:
- GAIE config: <path>
- ModelService config: <path>

Key features:
- <highlight 1>
- <highlight 2>
- <highlight 3>

Usage:
./setup/standup.sh -c scenarios/guides/<guide_name>.sh
```
