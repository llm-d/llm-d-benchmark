---
name: convert-guide
description: Convert llm-d guides to benchmark scenario and experiment files
---

# Convert llm-d Guide to Benchmark Files

## Purpose

This skill converts llm-d deployment guides (Helm values files) into llm-d-benchmark scenario and experiment files. It parses guide configurations and generates properly formatted shell scripts and YAML files that can be used with the benchmark framework.

The converter:
- Extracts configuration from Helm values YAML
- Maps values to `LLMDBENCH_*` environment variables
- Generates scenario files (shell scripts with exports)
- Generates experiment files (YAML for parameter sweeps)
- Supports variations for Design of Experiments (DOE)

## Usage

```
/convert-guide <url-or-path>
/convert-guide <url-or-path> with <harness> <profile>
/convert-guide <url-or-path> varying <parameter> from <start> to <end>
```

### Examples

```bash
# Convert a guide from GitHub URL
/convert-guide https://github.com/llm-d/llm-d/tree/main/guides/inference-scheduling

# Convert a local guide file
/convert-guide ~/guides/my-deployment-values.yaml

# Specify harness and profile
/convert-guide https://github.com/llm-d/llm-d/tree/main/guides/pd-disaggregation with inference-perf shared_prefix_synthetic

# Add parameter variations for experiments
/convert-guide ~/guides/config.yaml varying decode_replicas from 1 to 4
```

## Workflow

When this skill is invoked, follow these steps:

### Step 1: Parse the Input

Extract from the user's command:
- **Source**: URL or local file path
- **Harness**: Load generator (default: `inference-perf`)
- **Profile**: Workload profile (default: `sanity_random.yaml`)
- **Variations**: Parameter sweep specifications (if any)

### Step 2: Read Reference Files

Read these reference files to understand the mapping rules and defaults:

1. **Mapping Rules**: `docs/guide-converter-mappings.md`
   - Contains Helm path to LLMDBENCH variable mappings
   - Notes on transformations and special handling

2. **Default Values**: `setup/env.sh`
   - Current default values for all LLMDBENCH variables
   - Only override values that differ from defaults

3. **Example Scenario**: `scenarios/guides/inference-scheduling.sh`
   - Reference format for scenario files
   - Pattern for heredocs and complex values

4. **Example Experiment**: `experiments/inference-scheduling.yaml`
   - Reference format for experiment files
   - Structure for setup/run sections

### Step 3: Fetch Guide Contents

Obtain the guide YAML content:

- **For URLs**: Use the WebFetch tool to retrieve the content
  - GitHub URLs may need to be converted to raw URLs
  - Example: `https://github.com/llm-d/llm-d/tree/main/guides/inference-scheduling` -> fetch the values.yaml file

- **For Local Paths**: Use the Read tool to read the file directly

### Step 4: Extract Configuration from Helm Values

Parse the YAML and extract key configuration sections:

1. **Model Configuration**
   - `modelArtifacts.name` -> Model identifier
   - `modelArtifacts.size` -> PVC size

2. **Decode Stage**
   - `decode.create`, `decode.replicas`
   - `decode.parallelism.*` (tensor, data, dataLocal, workers)
   - `decode.containers[0].args` -> vLLM arguments
   - `decode.containers[0].env` -> Environment variables
   - `decode.volumes`, `decode.containers[0].volumeMounts`

3. **Prefill Stage** (if present)
   - Same structure as decode
   - `prefill.create`, `prefill.replicas`, etc.

4. **Routing/Gateway**
   - `routing.proxy.*` -> Routing sidecar config
   - GAIE plugin configuration

5. **vLLM Launch Arguments**
   - Extract from `args` arrays
   - Common: `--max-model-len`, `--tensor-parallel-size`, `--gpu-memory-utilization`
   - Special: `--kv-transfer-config`, `--enable-prefix-caching`

### Step 5: Map to LLMDBENCH Variables

Using the mapping rules from `docs/guide-converter-mappings.md`:

1. For each extracted value, find the corresponding `LLMDBENCH_*` variable
2. Compare against defaults from `setup/env.sh`
3. Only include variables that differ from defaults
4. Note any unmappable values in comments

**Key Mappings:**
| Helm Path | LLMDBENCH Variable |
|-----------|-------------------|
| `modelArtifacts.name` | `LLMDBENCH_DEPLOY_MODEL_LIST` |
| `decode.replicas` | `LLMDBENCH_VLLM_MODELSERVICE_DECODE_REPLICAS` |
| `decode.parallelism.tensor` | `LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM` |
| `prefill.replicas` | `LLMDBENCH_VLLM_MODELSERVICE_PREFILL_REPLICAS` |
| Args: `--max-model-len` | `LLMDBENCH_VLLM_COMMON_MAX_MODEL_LEN` |

### Step 6: Generate Scenario File

Create a shell script following this template:

```bash
# <GUIDE_NAME> WELL LIT PATH
# Based on <SOURCE_URL_OR_PATH>
# Generated by llm-d-benchmark guide converter

# IMPORTANT NOTE
# All parameters not defined here will use default values from setup/env.sh

# Model parameters
export LLMDBENCH_DEPLOY_MODEL_LIST="<model_name>"

# PVC parameters
export LLMDBENCH_VLLM_COMMON_PVC_MODEL_CACHE_SIZE=<size>

# Common vLLM parameters
export LLMDBENCH_VLLM_COMMON_MAX_MODEL_LEN=<value>
export LLMDBENCH_VLLM_COMMON_TENSOR_PARALLELISM=<value>

# Decode parameters
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_REPLICAS=<value>
# ... additional decode settings

# Prefill parameters (if applicable)
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_REPLICAS=<value>
# ... additional prefill settings

# Workload parameters
export LLMDBENCH_HARNESS_NAME=<harness>
export LLMDBENCH_HARNESS_EXPERIMENT_PROFILE=<profile>.yaml

# Local directory for results
export LLMDBENCH_CONTROL_WORK_DIR=~/data/<guide_name>
```

### Step 7: Generate Experiment File

Create a YAML file for parameter sweeps:

```yaml
setup:
  factors:
    - <FACTOR_NAME>
  levels:
    <FACTOR_NAME>: "<value1>, <value2>, ..."
  treatments:
    - "<value1>"
    - "<value2>"
    # ...

run:
  factors:
    - <run_factor>
  levels:
    <run_factor>: "<values>"
  treatments:
    - "<treatment1>"
    - "<treatment2>"
```

If variations were specified, incorporate them into the appropriate section.

### Step 8: Dry-Run Display

Before writing files, display the generated content to the user:

```
=== Generated Scenario File ===
File: scenarios/guides/<guide_name>.sh

<scenario content>

=== Generated Experiment File ===
File: experiments/<guide_name>.yaml

<experiment content>

Would you like me to write these files? (yes/no)
```

### Step 9: Write Files

On user confirmation:

1. Write the scenario file to `scenarios/guides/<guide_name>.sh`
2. Write the experiment file to `experiments/<guide_name>.yaml`
3. Report success with file paths

## Available Harnesses and Profiles

### Harnesses

| Harness | Description |
|---------|-------------|
| `inference-perf` | Default harness, comprehensive performance testing |
| `guidellm` | Alternative load generator |
| `vllm-benchmark` | vLLM native benchmarking |
| `nop` | No-op harness for testing |

### Profiles (inference-perf)

| Profile | Use Case |
|---------|----------|
| `sanity_random.yaml` | Quick validation tests |
| `chatbot_synthetic.yaml` | Chat workload simulation |
| `chatbot_sharegpt.yaml` | Real conversation patterns |
| `shared_prefix_synthetic.yaml` | Prefix caching tests |
| `shared_prefix_multi_turn_chat.yaml` | Multi-turn conversations |
| `summarization_synthetic.yaml` | Long context summarization |
| `code_completion_synthetic.yaml` | Code completion patterns |
| `random_concurrent.yaml` | Concurrent request stress |

### Profiles (guidellm)

| Profile | Use Case |
|---------|----------|
| `sanity_random.yaml` | Basic validation |
| `sanity_concurrent.yaml` | Concurrent validation |
| `chatbot_synthetic.yaml` | Chat simulation |
| `shared_prefix_synthetic.yaml` | Prefix caching |
| `summarization_synthetic.yaml` | Summarization |

### Profiles (vllm-benchmark)

| Profile | Use Case |
|---------|----------|
| `sanity_random.yaml` | Basic validation |
| `random_concurrent.yaml` | Concurrent stress test |
| `sharegpt.yaml` | ShareGPT dataset |

## Handling Complex Configurations

### Extra Args and Commands

When the guide uses `modelCommand: custom` with complex args, use heredoc pattern:

```bash
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_MODEL_COMMAND=custom

export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS
vllm serve /model-cache/models/REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL \
--host 0.0.0.0 \
--port REPLACE_ENV_LLMDBENCH_VLLM_COMMON_METRICS_PORT \
--max-model-len REPLACE_ENV_LLMDBENCH_VLLM_COMMON_MAX_MODEL_LEN \
--tensor-parallel-size REPLACE_ENV_LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM
EOF
```

### Extra Volumes and Mounts

For additional volumes:

```bash
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUMES}
- name: dshm
  emptyDir:
    medium: Memory
    sizeLimit: REPLACE_ENV_LLMDBENCH_VLLM_COMMON_SHM_MEM
EOF

export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS=$(mktemp)
cat << EOF > ${LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_VOLUME_MOUNTS}
- name: dshm
  mountPath: /dev/shm
EOF
```

### Environment Variables

For container environment variables:

```bash
export LLMDBENCH_VLLM_COMMON_ENVVARS_TO_YAML=$(mktemp)
cat << EOF > $LLMDBENCH_VLLM_COMMON_ENVVARS_TO_YAML
- name: VLLM_LOGGING_LEVEL
  value: INFO
- name: UCX_TLS
  value: "sm,cuda_ipc,cuda_copy,tcp"
EOF
```

## Error Handling

1. **Invalid URL**: If WebFetch fails, report the error and suggest checking the URL
2. **Missing File**: If local path doesn't exist, report clearly
3. **Invalid YAML**: If parsing fails, show the error and problematic section
4. **Unknown Helm Values**: Log unmapped values as comments in the scenario file
5. **Missing Required Values**: Warn if critical values (like model name) are missing

## Deriving Guide Name

Extract the guide name from the source:

1. **From URL**: Use the last path segment before `values.yaml`
   - `https://github.com/llm-d/llm-d/tree/main/guides/inference-scheduling` -> `inference-scheduling`
   - `https://example.com/pd-disaggregation/values.yaml` -> `pd-disaggregation`

2. **From Local Path**: Use the filename without extension, or parent directory name
   - `/path/to/my-guide.yaml` -> `my-guide`
   - `/path/to/inference-scheduling/values.yaml` -> `inference-scheduling`

3. **Sanitization**: Replace spaces and special characters with hyphens, lowercase

## Reference Files

- Mapping Rules: `docs/guide-converter-mappings.md`
- Default Values: `setup/env.sh`
- Example Scenario: `scenarios/guides/inference-scheduling.sh`
- Example Experiment: `experiments/inference-scheduling.yaml`
