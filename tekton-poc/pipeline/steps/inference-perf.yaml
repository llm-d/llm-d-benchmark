apiVersion: tekton.dev/v1beta1
kind: StepAction
metadata:
  name: inference-perf-run
spec:
  params:
    - name: harnessName
    - name: harnessProfile
    - name: pipelineUID
  env:
    - name: REQUESTED_HARNESS_NAME
      value: "$(params.harnessName)"
    - name: MY_HARNESS_NAME
      value: "inference-perf"
    - name: HARNESS_PROFILE
      value: "$(params.harnessProfile)"

    - name: GIT_REPO_URL
      value: "https://github.com/kubernetes-sigs/inference-perf.git"
    - name: GIT_REVISION
      value: "main"
    - name: GIT_COMMIT
      value: "1ccc48b6bb9c9abb61558b719041fb000b265e59"

    - name: DATA_ROOT_DIR
      value: $(workspaces.data.path)
    - name: MY_PIPELINE_UID
      value: $(params.pipelineUID)
    - name: MY_TASK_NAME
      value: $(context.taskRun.name)

  # https://github.com/llm-d/llm-d-benchmark/blob/main/build/Dockerfile#L1C6-L1C33
  image: python:3.12.9-slim-bookworm
  script: |
    #!/usr/bin/env bash

    # https://github.com/llm-d/llm-d-benchmark/blob/main/workload/harnesses/inference-perf-llm-d-benchmark.sh

    if [ "${REQUESTED_HARNESS_NAME}" != "${MY_HARNESS_NAME}" ]; then
      echo "Requested harness not ${MY_HARNESS_NAME}, skipping"
      exit 0
    fi

    # https://github.com/llm-d/llm-d-benchmark/blob/main/build/Dockerfile#L56-L62
    # https://github.com/llm-d/llm-d-benchmark/blob/main/setup/run.sh

    EXPERIMENT_ID="experiment-$(echo -n ${MY_PIPELINE_UID} | cut -c1-8)"
    RESULTS_DIR="${DATA_ROOT_DIR}/${MY_HARNESS_NAME}_${EXPERIMENT_ID}_${MY_TASK_NAME}"
    CONTROL_DIR="${DATA_ROOT_DIR}/${MY_HARNESS_NAME}_${EXPERIMENT_ID}_${MY_TASK_NAME}"
    RUN_DIR=$(pwd)

    # TODO figure out which are actually needed for each step
    echo "üîÑ Installing required tools"
    apt-get update
    apt-get install -y \
      git \
      pip \
      yq \
      && apt-get clean && rm -rf /var/cache/apt

    echo "üîÑ Cloning and installing harness: ${MY_HARNESS_NAME}"
    git clone --branch ${GIT_REVISION} ${GIT_REPO_URL}
    cd inference-perf
    git checkout ${GIT_COMMIT}
    pip install .

    # profile name and location
    workload=$(echo ${HARNESS_PROFILE} | sed 's^\.yaml^^g' )
    workload_profile=${workload}.yaml
    workload_profile_path=${CONTROL_DIR}/workload/profiles/${MY_HARNESS_NAME}/${workload_profile}

    # update .storage.local_storage.path in profile
    pushd "$RESULTS_DIR"
    yq '.storage["local_storage"]["path"] = '\"${RESULTS_DIR}\" <"${workload_profile_path}" -y >${workload_profile}

    # run inference-perf
    inference-perf --config_file "$(realpath ./${workload_profile})" > >(tee -a ${RESULTS_DIR}/stdout.log) 2> >(tee -a ${RESULTS_DIR}/stderr.log >&2)
    export LLMDBENCH_RUN_EXPERIMENT_HARNESS_RC=$?

    # If benchmark harness returned with an error, exit here
    if [[ $LLMDBENCH_RUN_EXPERIMENT_HARNESS_RC -ne 0 ]]; then
      echo "‚ùå Harness returned with error $LLMDBENCH_RUN_EXPERIMENT_HARNESS_RC"
      exit $LLMDBENCH_RUN_EXPERIMENT_HARNESS_RC
    fi
    echo "‚úÖ Harness completed successfully."
---
apiVersion: tekton.dev/v1beta1
kind: StepAction
metadata:
  name: inference-perf-analyze-results
spec:
  params:
    - name: harnessName
    - name: pipelineUID
  env:
    - name: REQUESTED_HARNESS_NAME
      value: "$(params.harnessName)"
    - name: MY_HARNESS_NAME
      value: "inference-perf"

    - name: GIT_REPO_URL
      value: "https://github.com/kubernetes-sigs/inference-perf.git"
    - name: GIT_REVISION
      value: "main"
    - name: GIT_COMMIT
      value: "1ccc48b6bb9c9abb61558b719041fb000b265e59"

    - name: DATA_ROOT_DIR
      value: $(workspaces.data.path)
    - name: MY_PIPELINE_UID
      value: $(params.pipelineUID)
    - name: MY_TASK_NAME
      value: $(context.taskRun.name)

# https://github.com/llm-d/llm-d-benchmark/blob/main/build/Dockerfile#L1C6-L1C33
  image: python:3.12.9-slim-bookworm
  script: |
    #!/usr/bin/env bash

    EXPERIMENT_ID="experiment-$(echo -n ${MY_PIPELINE_UID} | cut -c1-8)"
    RESULTS_DIR="${DATA_ROOT_DIR}/${MY_HARNESS_NAME}_${EXPERIMENT_ID}_${MY_TASK_NAME}"
 
    if [ "${REQUESTED_HARNESS_NAME}" != "${MY_HARNESS_NAME}" ]; then
      echo "Requested harness not ${MY_HARNESS_NAME}, skipping"
      exit 0
    fi
     
    echo "üîÑ Installing requirements"
    apt-get update
    apt-get install -y \
      git \
      pip \
      && apt-get clean && rm -rf /var/cache/apt

    git clone --branch ${GIT_REVISION} ${GIT_REPO_URL}
    cd inference-perf
    git checkout ${GIT_COMMIT}
    pip install .

    cat <<EOF > requirements-analysis.txt
    matplotlib>=3.7.0
    numpy>=2.3.1
    seaborn>=0.12.0
    pandas>=2.2.3
    pydantic>=2.11.7
    PyYAML>=6.0.2
    scipy>=1.16.0
    requests>=2.32.5
    EOF

    cat requirements-analysis.txt
    pip --version
    pip install --no-cache-dir \
      --disable-pip-version-check  \
      --upgrade \
      -r ./requirements-analysis.txt \
      --root-user-action=ignore
    pip list

    # Download covert python from llm-d-benchmark
    # TBD: should the python be embedded in the step? A separate step perhaps.
    export ROOT_DIR=workload/report
    export BRANCH=main

    cat <<EOF | python
    import os
    import requests

    # TBD these should be parameters
    ROOT_DIR = os.getenv("ROOT_DIR")
    BRANCH = os.getenv("BRANCH")

    api = f"https://api.github.com/repos/llm-d/llm-d-benchmark/contents/{ROOT_DIR}?ref={BRANCH}"
    headers = {}

    resp = requests.get(api, headers={})
    resp.raise_for_status()
    for item in resp.json():
        if item.get("type") == "file" and item["name"].endswith(".py"):
            url = item["download_url"]
            r = requests.get(url, headers=headers)
            r.raise_for_status()
            with open(item["name"], "wb") as f:
                f.write(r.content)
            print("Downloaded", item["name"])
    EOF

    chmod +x convert.py schema.py
    ls -l

    # https://github.com/llm-d/llm-d-benchmark/blob/main/workload/harnesses/inference-perf-llm-d-benchmark.sh#L17C1-L26C5
    echo "üîÑ Convert results into universal format"
    for result in $(find $RESULTS_DIR -maxdepth 1 -name 'stage_*.json'); do
      result_fname=$(echo $result | rev | cut -d '/' -f 1 | rev)
      ./convert.py $result -w inference-perf $RESULTS_DIR/benchmark_report,_$result_fname.yaml 2> >(tee -a $RESULTS_DIR/stderr.log >&2)
      # Report errors but don't quit
      export RUN_EXPERIMENT_CONVERT_RC=$?
      if [[ $RUN_EXPERIMENT_CONVERT_RC -ne 0 ]]; then
        echo "./convert.py returned with error $RUN_EXPERIMENT_CONVERT_RC converting: $result"
      fi
    done

    # Define function to call analysis so can call multiple times
    # https://github.com/llm-d/llm-d-benchmark/blob/main/analysis/inference-perf-analyze_results.sh
    analyze_results () {
      mkdir -p $RESULTS_DIR/analysis
      sleep 60
      tm=$(date)
      inference-perf --analyze "$RESULTS_DIR"
      ec=$?
      find $RESULTS_DIR -type f -newermt "${tm}" -exec mv -t "$RESULTS_DIR"/analysis {} +
      return $ec
    }

    # https://github.com/llm-d/llm-d-benchmark/blob/main/build/llm-d-benchmark.sh#L63-L74
    echo "üîÑ Running analysis"
    # Try to run analysis twice then give up
    analyze_results
    ec=$?
    if [[ $ec -ne 0 ]]; then
      echo "execution of analyzer failed, wating 120 seconds and trying again"
      sleep 120
      set -x
      analyze_results
    fi
    # Return with error code of first iteration of experiment analyzer
    # TBD modify this message depending on success
    echo "‚úÖ Results analyzed and reports generated"
    exit $ec
