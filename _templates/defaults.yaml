# --------------------------------
images:
  - name: llm-d-benchmark
    registry: ghcr.io
    repo: llm-d
    image: llm-d-benchmark
    tag: .auto # list all tags, pick the latest
  - name: llm-d
    registry: ghcr.io
    repo: llm-d
    image: llm-d-cuda
    tag: .auto # list all tags, pick the latest
  - name: llm-d-model-service
    registry: ghcr.io
    repo: llm-d
    image: llm-d-model-service
    tag: .auto # list all tags, pick the latest
  - name: llm-d-inference-scheduler
    registry: ghcr.io
    repo: llm-d
    image: llm-d-inference-scheduler
    tag: .auto # list all tags, pick the latest
  - name: llm-d-routing-sidecar
    registry: ghcr.io
    repo: llm-d
    image: llm-d-routing-sidecar
    tag: .auto # list all tags, pick the latest
  - name: llm-d-inference-sim
    registry: ghcr.io
    repo: llm-d
    image: llm-d-inference-sim
    tag: .auto # list all tags, pick the latest
  - name: vllm
    registry: docker.io
    repo: vllm
    image: vllm-openai
    tag: latest
# --------------------------------------------------------------------------------------------------------------------------------
charts:
  - chart: kgateway-crds
    url: oci://cr.kgateway.dev/kgateway-dev/charts/kgateway-crds
    version: 2.0.3
  - chart: kgateway
    url: oci://cr.kgateway.dev/kgateway-dev/charts/kgateway
    version: 2.0.3
  - chart: istio
    url: oci://gcr.io/istio-testing/charts
    version: 1.28-alpha.89f30b26ba71bf5e538083a4720d0bc2d8c06401
  - chart: llm-d-infra
    url: https://llm-d-incubation.github.io/llm-d-infra
    version: 1.3.0
  - chart: gateway-api-inference-extension
    url: oci://registry.k8s.io/gateway-api-inference-extension/charts/inferencepool
    version: .auto # list all versions, pick the latest
    #oci helm repos do not output a list of versions. Use "skopeo list-tags docker://registry.k8s.io/gateway-api-inference-extension/charts/inferencepool"
  - chart: llm-d-modelservice
    url: https://llm-d-incubation.github.io/llm-d-modelservice/
    version: .auto # list all versions, pick the latest
# --------------------------------------------------------------------------------------------------------------------------------
prepare:
  dependencies:
    gateway :
      provider :
        - type: kgateway
          helm:
            - chart: .charts.kgateway-crds
            - chart: .charts.kgateway
          deploy: true
          check: true
        - type: istio
          helm:
            - chart: .charts.istio
          deploy: false
          check: false
      api:
        url: https://github.com/kubernetes-sigs/gateway-api-inference-extension/config/crd
        version: 1.3.0
        deploy: true
        check: true
        inference_extension:
          url: https://github.com/kubernetes-sigs/gateway-api-inference-extension/config/crd
          version: 1.0.1
          deploy: true
          check: true
      wva:
        namespace: workload-variant-autoscaler-system
        helm: oci://ghcr.io/llm-d/workload-variant-autoscaler
        image: ghcr.io/llm-d/workload-variant-autoscaler:auto
        replicas: 1
        version: 0.1.0
        autoscaling:
          enabled: true
          slo:
            tpot: 30
            ttft: 1000
        hpa:
          enabled: true
          max_replicas: 10
          target_avg_value: 1
        vllm:
          enabled: true
          node_port_min: 30000
          node_port_max: 32767
          interval: 15
        workload_monitoring:
          namespace: openshift-user-workload-monitoring
          url: https://thanos-querier.openshift-monitoring.svc.cluster.local
          port: 9091
      deploy: true
      check: true
    storage:
      - name: model-storage
        namespace: .standup[0].parameters.namespace
        class: default
        size: 300Gi
        download:
          url: <model list>
          enabled: true
          timeout: 3600
        deploy: true
        check: true
      - name: replay-pvc
        namespace: .harness[0].parameters.namespace
        class: default
        size: 300Gi
        download:
          url: https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/blob/main/ShareGPT_V3_unfiltered_cleaned_split.json
          enabled: false
          timeout: 3600
        deploy: true
        check: true
      - name: workload-pvc
        namespace: .harness[0].parameters.namespace
        class: default
        size: 300Gi
        deploy: true
        check: true
    secrets:
      - name: llm-d-hf-token
        secret: HF_TOKEN
        contents: <base64>
    files:
      - name: llm-d-benchmark-preprocesses
        path: BASE_DIR/setup/preprocess # BASE_DIR gets expanded into
    monitoring:
#      TBD
# --------------------------------------------------------------------------------------------------------------------------------
standup:
  - stack: "stack-1"
    parameters:
      model:
        - name: facebook/opt-125m
          label: label(.standup[0].parameters.model[0].name) # function which creates the label
          maxlen: 16384
          blocksize: 64
      namespace: stackbenchns
      release: stackbenchr
      gaie:
        plugins: "default-plugins.yaml"
      volumes:
        - name: model-storage
          type: pvc
          mount: model-storage
          size: .prepare.dependencies.storage[]|select(name=.standup[0].parameters.volumes[0].name).size
        - name: dshm
          type: Memory
          mount: /dev/shm
          size: 16Gi
      accelerators:
        common:
          key: nvidia.com/gpu.product
          value: NVIDIA-H100-80GB-HBM3
        standalone:
          key: .standup[0].parameters.accelerators.common.key
          value: .standup[0].parameters.accelerators.common.value
        decode:
          key: .standup[0].parameters.accelerators.common.key
          value: .standup[0].parameters.accelerators.common.value
        prefill:
          key: .standup[0].parameters.accelerators.common.key
          value: .standup[0].parameters.accelerators.common.value
      replicas:
        common: 1
        standalone: .standup[0].parameters.replicas.common
        decode: .standup[0].parameters.replicas.common
        prefill: .standup[0].parameters.replicas.common
      parallelism:
        common:
          data: 1
          tensor: 1
        standalone:
          data: .standup[0].parameters.parallelism.common.data
          tensor: .standup[0].parameters.parallelism.common.tensor
        decode:
          data: .standup[0].parameters.parallelism.common.data
          tensor: .standup[0].parameters.parallelism.common.tensor
        prefill:
          data: .standup[0].parameters.parallelism.common.data
          tensor: .standup[0].parameters.parallelism.common.tensor
      resources:
        common:
          memory: 40Gi
          cpu: "4"
          nvidia.com/gpu: "1"
          ephemeral-storage: 20Gi
        standalone:
          memory: .standup[0].parameters.resources.common.memory
          cpu: .standup[0].parameters.resources.common.cpu
          nvidia.com/gpu: .standup[0].parameters.resources.common.\"nvidia.com/gpu\"
          ephemeral-storage: .standup[0].parameters.resources.common.ephemeral-storage
        decode:
          memory: .standup[0].parameters.resources.common.memory
          cpu: .standup[0].parameters.resources.common.cpu
          nvidia.com/gpu: .standup[0].parameters.resources.common.\"nvidia.com/gpu\"
          ephemeral-storage: .standup[0].parameters.resources.common.ephemeral-storage
        prefill:
          memory: .standup[0].parameters.resources.common.memory
          cpu: .standup[0].parameters.resources.common.cpu
          nvidia.com/gpu: .standup[0].parameters.resources.common.\"nvidia.com/gpu\"
          ephemeral-storage: .standup[0].parameters.resources.common.ephemeral-storage
      annotations:
          deployed-by: ".auto" # just use current user name
          modelservice: "llm-d-benchmark"
      labels:
          app: .standup[0].parameters.model[0].label
          stood-up-by: ".auto" # just use current user name
          stood-up-from: llm-d-benchmark
          stood-up-via: "standalone"
      command:
        standalone:
          - |
            true ;
            vllm serve .standup[0].parameters.model[0].name" \
              --enable-sleep-mode \
              --load-format auto \
              --port .standup[0].parameters.ports.service \
              --max-model-len .standup[0].parameters.model[0].maxlen \
              --disable-log-requests \
              --gpu-memory-utilization 0.95 \
              --tensor-parallel-size .standup[0].parameters.parallelism.common.tensor \
              --model-loader-extra-config "$LLMDBENCH_VLLM_STANDALONE_MODEL_LOADER_EXTRA_CONFIG"
        decode:
          type: vllmServe
          args:
            - "--disable-log-requests"
            - "--max-model-len"
            - ".standup[0].parameters.model[0].maxlen"
            - "--tensor-parallel-size"
            - ".standup[0].parameters.parallelism.decode.tensor"
        prefill:
          type: vllmServe
          args:
            - "--disable-log-requests"
            - "--max-model-len"
            - ".standup[0].parameters.model[0].maxlen"
            - "--tensor-parallel-size"
            - ".standup[0].parameters.parallelism.prefill.tensor"
      env:
        common:
          - name: VLLM_NIXL_SIDE_CHANNEL_HOST
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
          - name: UCX_TLS
            value: "rc,sm,cuda_ipc,cuda_copy,tcp"
          - name: UCX_SOCKADDR_TLS_PRIORITY
            value: "tcp"
          ###- name: UCX_NET_DEVICES
          ###  value: mlx5_1:1
          ###- name: NCCL_IB_HCA
          ###  value: mlx5_1
          - name: VLLM_NIXL_SIDE_CHANNEL_PORT
            value: ".standup[0].parameters.ports.nixl"
          - name: VLLM_ALLOW_LONG_MAX_MODEL_LEN
            value: "1"
          - name: VLLM_SERVER_DEV_MODE
            value: "1"
        standalone:
          - name: LLMDBENCH_VLLM_STANDALONE_MODEL
            value: ".standup[0].parameters.model[0].name"
          - name: LLMDBENCH_VLLM_STANDALONE_VLLM_LOAD_FORMAT
            value: "auto"
          - name: LLMDBENCH_VLLM_STANDALONE_MODEL_LOADER_EXTRA_CONFIG
            value: "{}"
          - name: VLLM_LOGGING_LEVEL
            value: "INFO"
          - name: HF_HOME
            value: /.standup[0].parameters.volumes[0].mount
  #          - name: LLMDBENCH_VLLM_COMMON_AFFINITY
  #            value: "nvidia.com/gpu.product:NVIDIA-H100-80GB-HBM3"
          - name: HUGGING_FACE_HUB_TOKEN
            valueFrom:
              secretKeyRef:
                name: .prepare.dependencies.secrets[0].name
                key: .prepare.dependencies.secrets[0].secret
          - .standup[0].parameters.env.common
        decode: .standup[0].parameters.env.common
        prefill: .standup[0].parameters.env.common
      ports:
        service: 8000
        extra: 9002
        readiness: 8200
        zmq: 5557
        nixl: 5557
# ----------------------------------------------------------------
    infra:
      - helm: .charts.llm-d-infra
        contents:
          gateway:
            gatewayClassName: .auto # logic to decide the provider name based on deploy=true on .prepare.dependencies.gateway.provider
            service:
              type: NodePort
            gatewayParameters:
              enabled: true
        deploy: true
        check: true
# ----------------------------------------------------------------
    gaie:
      - helm: .charts.gateway-api-inference-extension
        contents:
          imports:
            - BASE_DIR/_templates/gateway-api-inference-extension.yaml
        deploy: true
        check: true
# ----------------------------------------------------------------
    modelservice:
      - helm: .charts.llm-d-modelservice
        contents:
          imports:
            - BASE_DIR/_templates/modelservice.yaml
        deploy: true
        check: true
  # ----------------------------------------------------------------
    standalone :
      - type: deployment+service
        contents:
          imports:
            - BASE_DIR/_templates/standalone.yaml
        deploy: true
        check: true
# --------------------------------------------------------------------------------------------------------------------------------
harness:
    - runner: "runner-1"
      parameters:
        namespace: harnessns
        harness:
          name: inference-perf
          profile: sanity_random.yaml
        executable: llm-d-benchmark.sh
        timeout: 3600
        resources:
          memory: 32Gi
          cpu: "16"
        volumes:
          - name: workload-pvc
            type: pvc
            mount: /requests
          - name: replay-pvc
            type: pvc
            mount: /data
        env:
          - name: LLMDBENCH_RUN_EXPERIMENT_LAUNCHER
            value: "1"
          - name: LLMDBENCH_RUN_DATASET_URL
            value: ".prepare.dependencies.storage[]|select(name=replay-pvc).download.url"
          - name: LLMDBENCH_RUN_WORKSPACE_DIR
            value: "/workspace"
          - name: LLMDBENCH_HARNESS_NAME
            value: ".harness[0].parameters.harness.name"
#          - name: LLMDBENCH_CONTROL_WORK_DIR
#            value: "${LLMDBENCH_RUN_EXPERIMENT_RESULTS_DIR}"
          - name: LLMDBENCH_HARNESS_NAMESPACE
            value: ".harness[0].parameters.namespace"
#          - name: LLMDBENCH_HARNESS_STACK_TYPE
#            value: "auto" # a function will determine if "modelservice" or "standlone" was set
          - name: LLMDBENCH_HARNESS_STACK_ENDPOINT_URL
            value: "${LLMDBENCH_HARNESS_STACK_ENDPOINT_URL}"
          - name: LLMDBENCH_HARNESS_STACK_NAME
            value: "${LLMDBENCH_HARNESS_SANITIZED_STACK_NAME}"
          - name: LLMDBENCH_DEPLOY_METHODS
            value: "${LLMDBENCH_DEPLOY_METHODS}"
          - name: LLMDBENCH_MAGIC_ENVAR
            value: "harness_pod"
          - name: HF_TOKEN_SECRET
            value: "${LLMDBENCH_VLLM_COMMON_HF_TOKEN_NAME}"
          - name: .prepare.dependencies.secrets[0].secret
            valueFrom:
              secretKeyRef:
                name: .prepare.dependencies.secrets[0].name
                key: .prepare.dependencies.secrets[0].secret
      contents:
        apiVersion: v1
        kind: Pod
        metadata:
          name: .harness[0].parameters.harness.name-launcher
          namespace: .harness[0].parameters.namespace
        labels:
          app: .harness[0].parameters.harness.name-launcher
        spec:
          containers:
          - name: harness
            image: ".images[]|select(.name==\"llm-d\").registry/.images[]|select(.name==\"llm-d\").repo/.images[]|select(.name==\"llm-d\").image:.images[]|select(.name==\"llm-d\").tag"
            imagePullPolicy: Always
            securityContext:
              runAsUser: 0
            command: ["sh", "-c"]
            args:
            - ".harness[0].parameters.executable"
            resources:
              limits:
                cpu: ".harness[0].parameters.resources.cpu"
                memory: .harness[0].parameters.resources.memory
              requests:
                cpu: ".harness[0].parameters.resources.cpu"
                memory: .harness[0].parameters.resources.memory
            env: .harness[0].parameters.env
            volumeMounts:
            - name: results
              mountPath: .harness[0].parameters.volumes[0].mount
            - name: replay
              mountPath: .harness[0].parameters.volumes[1].mount
          - mountPath: /workspace/profiles/.harness[0].parameters.harness.name
            name: inference-perf-profiles
          volumes:
            - name: results
              persistentVolumeClaim:
                claimName: .harness[0].parameters.volumes[0].name
            - name: results
              persistentVolumeClaim:
                claimName: .harness[0].parameters.volumes[1].name
            - configMap:
                defaultMode: 420
                name: .harness[0].parameters.harness.name-profiles
              name: .harness[0].parameters.harness.name-profiles