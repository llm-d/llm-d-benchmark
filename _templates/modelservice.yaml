fullnameOverride: .experiments[0].user-overrides.inference-engine.model[0].label
multinode: False
modelArtifacts:
  uri: .experiments[0].user-overrides.inference-engine.volumes[0].type://.experiments[0].user-overrides.inference-engine.volumes[0].mount/models/.experiments[0].user-overrides.inference-engine.model[0].name
  size: .prepare.dependencies.storage[]|select(name=.standup[0].stack.parameters.volumes[0].name).size
  authSecretName: ".prepare.dependencies.secrets[0].name"
  name: .experiments[0].user-overrides.inference-engine.model[0].name
routing:
  servicePort: .experiments[0].user-overrides.inference-engine.ports.service
  parentRefs:
    - group: gateway.networking.k8s.io
      kind: Gateway
      name: infra-.standup[0].release-inference-gateway
  proxy:
    image: ".images.user-overrides[]|select(.name==\"llm-d-routing-sidecar\").registry/.images.user-overrides[]|select(.name==\"llm-d-routing-sidecar\").repo/.images.user-overrides[]|select(.name==\"llm-d-routing-sidecar\").image:.images.user-overrides[]|select(.name==\"llm-d-routing-sidecar\").tag"
    secure: false
    connector: nixlv2
    debugLevel: 3
  inferencePool:
    create: False
    name: .experiments[0].user-overrides.inference-engine.model[0].label
  httpRoute:
    create: True
    rules:
    - backendRefs:
      - group: inference.networking.x-k8s.io
        kind: InferencePool
        name: .experiments[0].user-overrides.inference-engine.model[0].label
        port: .experiments[0].user-overrides.inference-engine.ports.service
        weight: 1
      timeouts:
        backendRequest: 0s
        request: 0s
      matches:
      - path:
          type: PathPrefix
          value: /.experiments[0].user-overrides.inference-engine.model[0].name/
      filters:
      - type: URLRewrite
        urlRewrite:
          path:
            type: ReplacePrefixMatch
            replacePrefixMatch: /
    - backendRefs:
      - group: inference.networking.x-k8s.io
        kind: InferencePool
        name: .experiments[0].user-overrides.inference-engine.model[0].label
        port: .experiments[0].user-overrides.inference-engine.ports.service
        weight: 1
      timeouts:
        backendRequest: 0s
        request: 0s
  epp:
    create: False
decode:
  create: bool(.experiments[0].user-overrides.inference-engine.replicas.decode)
  replicas: .experiments[0].user-overrides.inference-engine.replicas.decode
  acceleratorTypes:
    labelKey: .experiments[0].user-overrides.inference-engine.accelerators.decode.key
    labelValues:
      - .experiments[0].user-overrides.inference-engine.accelerators.decode.value
  parallelism:
    data: .experiments[0].user-overrides.inference-engine.parallelism.decode.data
    tensor: .experiments[0].user-overrides.inference-engine.parallelism.decode.tensor
  annotations: .experiments[0].user-overrides.inference-engine.annotations
  podAnnotations: .experiments[0].user-overrides.inference-engine.annotations
  extraConfig:
  containers:
  - name: "vllm"
    mountModelVolume: true
    image: ".images.user-overrides[]|select(.name==\"llm-d\").registry/.images.user-overrides[]|select(.name==\"llm-d\").repo/.images.user-overrides[]|select(.name==\"llm-d\").image:.images.user-overrides[]|select(.name==\"llm-d\").tag"
    modelCommand: .experiments[0].user-overrides.inference-engine.command.decode.type
    args: .experiments[0].user-overrides.inference-engine.command.decode.args
    env: .experiments[0].user-overrides.inference-engine.env.decode
    resources:
      limits: .experiments[0].user-overrides.inference-engine.resources.decode
      requests: .experiments[0].user-overrides.inference-engine.resources.decode
    extraConfig:
      startupProbe:
        httpGet:
          path: /health
          port: .experiments[0].user-overrides.inference-engine.ports.service
        failureThreshold: 60
        initialDelaySeconds: 30
        periodSeconds: 30
        timeoutSeconds: 5
      livenessProbe:
        tcpSocket:
          port: .experiments[0].user-overrides.inference-engine.ports.service
        failureThreshold: 3
        periodSeconds: 5
      readinessProbe:
        httpGet:
          path: /health
          port: .experiments[0].user-overrides.inference-engine.ports.readiness
        failureThreshold: 3
        periodSeconds: 5
    volumeMounts:
  volumes:
prefill:
  create: bool(.experiments[0].user-overrides.inference-engine.replicas.prefill)
  replicas: .experiments[0].user-overrides.inference-engine.replicas.prefill]
  acceleratorTypes:
    labelKey: .experiments[0].user-overrides.inference-engine.accelerators.prefill.key
    labelValues:
      - .experiments[0].user-overrides.inference-engine.accelerators.prefill.value
  parallelism:
    data: .experiments[0].user-overrides.inference-engine.parallelism.prefill.data
    tensor: .experiments[0].user-overrides.inference-engine.parallelism.prefill.tensor
  annotations: .experiments[0].user-overrides.inference-engine.annotations
  podAnnotations: .experiments[0].user-overrides.inference-engine.annotations
  extraConfig:
  containers:
  - name: "vllm"
    mountModelVolume: true
    image: ".images.user-overrides[]|select(.name==\"llm-d\").registry/.images.user-overrides[]|select(.name==\"llm-d\").repo/.images.user-overrides[]|select(.name==\"llm-d\").image:.images.user-overrides[]|select(.name==\"llm-d\").tag"
    modelCommand: .experiments[0].user-overrides.inference-engine.command.prefill.type
    args: .experiments[0].user-overrides.inference-engine.command.prefill.args
    env: .experiments[0].user-overrides.inference-engine.env.prefill
    resources:
      limits: .experiments[0].user-overrides.inference-engine.resources.prefill
      requests: .experiments[0].user-overrides.inference-engine.resources.prefill
    extraConfig:
      startupProbe:
        httpGet:
          path: /health
          port: .experiments[0].user-overrides.inference-engine.ports.service
        failureThreshold: 60
        initialDelaySeconds: 30
        periodSeconds: 30
        timeoutSeconds: 5
      livenessProbe:
        tcpSocket:
          port: .experiments[0].user-overrides.inference-engine.ports.service
        failureThreshold: 3
        periodSeconds: 5
      readinessProbe:
        httpGet:
          path: /health
          port: .experiments[0].user-overrides.inference-engine.ports.service
        failureThreshold: 3
        periodSeconds: 5
    volumeMounts:
      - name: .experiments[0].user-overrides.volumes[1].name
        mountPath: .experiments[0].user-overrides.volumes[1].mount
  volumes:
      - name: .experiments[0].user-overrides.volumes[1].name
        emptyDir:
          medium: .experiments[0].user-overrides.volumes[1].type
          sizeLimit: .experiments[0].user-overrides.volumes[1].size