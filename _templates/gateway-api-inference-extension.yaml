inferenceExtension:
  replicas: 1
  image:
    name: .images[]|select(.name=="llm-d-inference-scheduler").name
    hub: .images[]|select(.name=="llm-d-inference-scheduler").registry/.images[]|select(.name=="llm-d-inference-scheduler").repo
    tag: .auto # automatically detects latest tag
    pullPolicy: Always
  extProcPort: .standup[0].parameters.ports.extra
  extraContainerPorts:
    - name: zmq
      containerPort: .standup[0].parameters.ports.zmq
      protocol: TCP
  extraServicePorts:
    - name: zmq
      port: .standup[0].parameters.ports.zmq
      targetPort: .standup[0].parameters.ports.zmq
      protocol: TCP
  env:
    - name: .prepare.dependencies.secrets[0].secret
      valueFrom:
        secretKeyRef:
          name: .prepare.dependencies.secrets[0].name
          key: .prepare.dependencies.secrets[0].secret
  pluginsConfigFile: ".standup[0].parameters.gaie.plugins"
inferencePool:
  targetPortNumber: .standup[0].parameters.ports.service
  modelServerType: vllm
  apiVersion: "inference.networking.x-k8s.io/v1alpha2"
  modelServers:
    matchLabels:
      llm-d.ai/inferenceServing: "true"
      llm-d.ai/model: .standup[0].parameters.model[0].label
provider:
  name: .auto # logic to decide the provider name based on deploy=true on .prepare.dependencies.gateway.provider
