apiVersion: v1
kind: ConfigMap
metadata:
  name: fmperf-workload-config
data:
  # Small model configuration (Llama-3.2-3B-Instruct)
  baseline_lmbench_llama32b_instruct.yaml: |
    model_name: "meta-llama/Llama-3.2-3B-Instruct"
    scenarios: "long-input"
    qps_values: "0.1 0.25 0.5"
    image: "lmcache/lmcache-benchmark:main"
    service_account: "fmperf-runner"
    pvc_name: "baseline-results-pvc"
    num_users_warmup: 1
    num_users: 1
    num_rounds: 1
    system_prompt: 256
    chat_history: 1024
    answer_len: 32
    init_user_id: 1
    test_duration: 60
    use_chat_completions: false

  # Small model configuration (Llama-3.2-3B-Instruct)
  llmd_lmbench_llama32b_instruct.yaml: |
    model_name: "meta-llama/Llama-3.2-3B-Instruct"
    scenarios: "long-input"
    qps_values: "0.1 0.25 0.5"
    image: "lmcache/lmcache-benchmark:main"
    service_account: "fmperf-runner"
    pvc_name: "llm-d-results-pvc"
    num_users_warmup: 1
    num_users: 1
    num_rounds: 1
    system_prompt: 256
    chat_history: 1024
    answer_len: 32
    init_user_id: 1
    test_duration: 60
    use_chat_completions: false

  # Small model configuration (Llama-3.2-3B-Instruct)
  single_llmd_lmbench_llama32b_instruct.yaml: |
    model_name: "meta-llama/Llama-3.2-3B-Instruct"
    scenarios: "long-input"
    qps_values: "0.1 0.25 0.5"
    image: "lmcache/lmcache-benchmark:main"
    service_account: "fmperf-runner"
    pvc_name: "fmperf-results-pvc"
    num_users_warmup: 1
    num_users: 1
    num_rounds: 1
    system_prompt: 256
    chat_history: 1024
    answer_len: 32
    init_user_id: 1
    test_duration: 60
    use_chat_completions: false

  # Medium model configuration (7B)
  lmbench_llama7b_single.yaml: |
    model_name: "llama-7b"
    scenarios: "long-input"
    qps_values: "0.1 0.25 0.5 1 2 3 4 5"
    image: "lmcache/lmcache-benchmark:main"
    service_account: "default"
    pvc_name: "fmperf-results-pvc"
    num_users_warmup: 5
    num_users: 3
    num_rounds: 5
    system_prompt: 512
    chat_history: 2048
    answer_len: 64
    init_user_id: 1
    test_duration: 300
    use_chat_completions: true

  # Large model configuration (H100)
  lmbench_llama70b_2replica_H100.yaml: |
    model_name: "meta-llama/Llama-3.2-70B-Instruct"
    scenarios: "long-input"
    qps_values: "0.1 0.25 0.5 1 2 3 4 5"
    image: "lmcache/lmcache-benchmark:main"
    service_account: "fmperf-runner"
    pvc_name: "fmperf-results-pvc"
    num_users_warmup: 5
    num_users: 3
    num_rounds: 5
    system_prompt: 256
    chat_history: 1024
    answer_len: 32
    init_user_id: 1
    test_duration: 300
    use_chat_completions: true 
