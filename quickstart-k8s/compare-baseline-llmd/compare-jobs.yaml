---
apiVersion: batch/v1
kind: Job
metadata:
  name: baseline-fmperf-benchmark
  namespace: fmperf
spec:
  backoffLimit: 0
  template:
    spec:
      serviceAccountName: fmperf-runner
      containers:
      - name: fmperf
        # built from this PR: https://github.com/fmperf-project/fmperf/pull/41
        # switch to fmperf-project/fmperf when merged
        image: quay.io/sallyom/fmperf:llmd
        imagePullPolicy: Always
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
              - ALL
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        command: ["python", "/app/run_benchmark.py"]
        env:
        - name: FMPERF_RESULTS_DIR
          value: "/requests"
        - name: FMPERF_NAMESPACE
          value: "fmperf"
        - name: FMPERF_WORKLOAD_FILE
          value: "baseline_lmbench_llama32b_instruct.yaml"  # Default to Llama-3.2-3B-Instruct
        - name: FMPERF_STACK_NAME
          value: "baseline-32b"
        - name: FMPERF_STACK_TYPE
          value: "vllm"
        - name: FMPERF_ENDPOINT_URL
          value: "http://llama32-3b.vanilla-vllm.svc.cluster.local:8000"  # Internal service URL
        - name: FMPERF_REPETITION
          value: "1"
        # Add HF_TOKEN_SECRET to be used by the injection script
        - name: HF_TOKEN_SECRET
          value: "huggingface-secret"  # Update with your actual secret name
        # Optional: Add these if you need to configure the model further
        - name: FMPERF_BATCH_SIZE
          value: "1"
        - name: FMPERF_SEQUENCE_LENGTH
          value: "256"
        - name: FMPERF_MAX_TOKENS
          value: "32"
        # New benchmark parameters
        - name: FMPERF_NUM_USERS_WARMUP
          value: "5"
        - name: FMPERF_NUM_USERS
          value: "3"
        - name: FMPERF_NUM_ROUNDS
          value: "5"
        - name: FMPERF_SYSTEM_PROMPT
          value: "256"
        - name: FMPERF_CHAT_HISTORY
          value: "1024"
        - name: FMPERF_ANSWER_LEN
          value: "32"
        - name: FMPERF_TEST_DURATION
          value: "60"
        # Add this to ensure the job has a unique ID for evaluation job name
        - name: FMPERF_JOB_ID
          value: "baseline-32b"
        volumeMounts:
        - name: workload-config
          mountPath: /app/yamls
        - name: logs
          mountPath: /app/logs
        #- name: results
        #  mountPath: /requests
      volumes:
      - name: workload-config
        configMap:
          name: fmperf-workload-config
      #- name: results
      #  persistentVolumeClaim:
      #    claimName: baseline-results-pvc
      - name: logs
        emptyDir: {}
      restartPolicy: Never
---
apiVersion: batch/v1
kind: Job
metadata:
  name: llmd-fmperf-benchmark
  namespace: fmperf
spec:
  backoffLimit: 0
  template:
    spec:
      serviceAccountName: fmperf-runner
      containers:
      - name: fmperf
        image: quay.io/sallyom/fmperf:llmd
        imagePullPolicy: Always
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
              - ALL
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        command: ["python", "/app/run_benchmark.py"]
        env:
        - name: FMPERF_RESULTS_DIR
          value: "/requests"
        - name: FMPERF_NAMESPACE
          value: "fmperf"
        - name: FMPERF_WORKLOAD_FILE
          value: "llmd_lmbench_llama32b_instruct.yaml"  # Default to Llama-3.2-3B-Instruct
        - name: FMPERF_STACK_NAME
          value: "llmd-32b"
        - name: FMPERF_STACK_TYPE
          value: "llm-d"
        - name: FMPERF_ENDPOINT_URL
          value: "http://llm-d-inference-gateway.llm-d.svc.cluster.local:80"  # Internal service URL
        - name: FMPERF_REPETITION
          value: "1"
        # Add HF_TOKEN_SECRET to be used by the injection script
        - name: HF_TOKEN_SECRET
          value: "huggingface-secret"  # Update with your actual secret name
        # Optional: Add these if you need to configure the model further
        - name: FMPERF_BATCH_SIZE
          value: "1"
        - name: FMPERF_SEQUENCE_LENGTH
          value: "256"
        - name: FMPERF_MAX_TOKENS
          value: "32"
        # New benchmark parameters
        - name: FMPERF_NUM_USERS_WARMUP
          value: "5"
        - name: FMPERF_NUM_USERS
          value: "3"
        - name: FMPERF_NUM_ROUNDS
          value: "5"
        - name: FMPERF_SYSTEM_PROMPT
          value: "256"
        - name: FMPERF_CHAT_HISTORY
          value: "1024"
        - name: FMPERF_ANSWER_LEN
          value: "32"
        - name: FMPERF_TEST_DURATION
          value: "60"
        # Add this to ensure the job has a unique ID for evaluation job name
        - name: FMPERF_JOB_ID
          value: "llmd-32b"
        volumeMounts:
        - name: workload-config
          mountPath: /app/yamls
        - name: logs
          mountPath: /app/logs
        #- name: results
        #  mountPath: /requests
      volumes:
      - name: workload-config
        configMap:
          name: fmperf-workload-config
      #- name: results
      #  persistentVolumeClaim:
      #    claimName: llm-d-results-pvc
      - name: logs
        emptyDir: {}
      restartPolicy: Never
