apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama3-8b-prefiller
  namespace: $LLMDBENCH_CLUSTER_NAMESPACE
  labels:
    app: llama3-8b-prefiller
    llm-d.ai/inferenceServing: "true"
    llm-d.ai/model: "llama3-8b"
    llm-d.ai/role: "prefill"
spec:
  replicas: 1  # Number of prefiller pods
  selector:
    matchLabels:
      app: llama3-8b-prefiller
  template:
    metadata:
      labels:
        app: llama3-8b-prefiller
        llm-d.ai/inferenceServing: "true"
        llm-d.ai/model: "llama3-8b"
        llm-d.ai/role: "prefill"
    spec:
      imagePullSecrets:
        - name: $LLMDBENCH_QUAY_SECRET
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: nvidia.com/gpu.product
                    operator: In
                    values:
                      - $LLMDBENCH_GPU_MODEL
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - llama3-8b-prefiller
                - llama3-8b-decoder
            topologyKey: kubernetes.io/hostname
      containers:
        - name: vllm
          image: quay.io/llm-d/llm-d-dev:vllm-nixl-0.0.6
          securityContext:
            allowPrivilegeEscalation: false
          args:
            - "--model"
            - "meta-llama/Llama-3.1-8B-Instruct"
            - "--port"
            - "8000"
            - "--enforce-eager"
            - "--kv-transfer-config"
            - '{"kv_connector":"NixlConnector","kv_role":"kv_both"}'
          env:
            - name: UCX_TLS
              value: "cuda_ipc,cuda_copy,tcp"
            - name: VLLM_NIXL_SIDE_CHANNEL_PORT
              value: "5557"
            - name: VLLM_NIXL_SIDE_CHANNEL_HOST
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: VLLM_LOGGING_LEVEL
              value: DEBUG
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: $LLMDBENCH_HF_SECRET
                  key: token
          volumeMounts:
            - name: model-cache
              mountPath: /root/.cache/huggingface
          ports:
            - containerPort: 8000
              protocol: TCP
            - containerPort: 5557
              protocol: TCP
          resources:
            limits:
              nvidia.com/gpu: 1
            requests:
              cpu: "16"
              memory: 40Gi
              nvidia.com/gpu: 1
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 30
            periodSeconds: 10
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 30
            periodSeconds: 10
      volumes:
        - name: model-cache
          persistentVolumeClaim:
            claimName: model-cache
---
apiVersion: v1
kind: Service
metadata:
  name: llama3-8b-prefiller
  namespace: $LLMDBENCH_CLUSTER_NAMESPACE
spec:
  selector:
    app: llama3-8b-prefiller
  ports:
    - port: 8000
      targetPort: 8000
      protocol: TCP
  type: ClusterIP 