# Follows precise prefix scheduling well-lit path
# https://llm-d.ai/blog/kvcache-wins-you-can-see
#
# X-70R-6kSys — 8×307,328 KV; target 70%-80% total usage
#
# CLUSTER
# - Pods: 8
# - KV/pod: 307,328 → Cluster capacity C_cap = 2,458,624
#
# SHAPE
# - system_prompt_len = 6,000
# - question_len      = 1,200      # ≤ 1,500; cached per user
# - num_prompts_per_group = 5      # users per group
# - output_len        = 1,000      # observed live ≈ 200–230 tokens/running request
#
# RESIDENT SIZING
# - Resident per group R_group = 6,000 + 5×1,200 = 12,000
# - num_groups G = 150
#   Resident total = 150 × 12,000 = 1,800,000 → 1,800,000 / 2,458,624 ≈ 73.2%
#
# LIVE SIZING
#
# WORKING SET
# - One FULL SET S = G × P = 150 × 5 = 750 requests
#
load:
  type: poisson
  stages:
    # Warmup — seat residents (~1×S)
    - rate: 15
      duration: 50            # 15*50  = 750

    # Main ladder
    - rate: 3
      duration: 20
    - rate: 10
      duration: 20
    - rate: 15
      duration: 20
    - rate: 20
      duration: 38           
    - rate: 22
      duration: 34           
    - rate: 25
      duration: 30           
    - rate: 30
      duration: 25           
    - rate: 35
      duration: 21           
    - rate: 40               
      duration: 38           
    - rate: 43
      duration: 36           
    - rate: 46
      duration: 33           
    - rate: 49
      duration: 30           
    - rate: 52
      duration: 29           
    - rate: 55
      duration: 27           
    - rate: 57
      duration: 26           
    - rate: 60
      duration: 25           
    
api:
  type: completion
  streaming: true

server:
  type: vllm
  model_name: REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL
  base_url: REPLACE_ENV_LLMDBENCH_HARNESS_STACK_ENDPOINT_URL
  ignore_eos: true

tokenizer:
  pretrained_model_name_or_path: REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL

data:
  type: shared_prefix
  shared_prefix:
    num_groups: 150               # Number of distinct shared prefixes
    num_prompts_per_group: 5      # Number of unique questions per shared prefix
    system_prompt_len: 6000       # Length of the shared prefix (in tokens)
    question_len: 1200            # Length of the unique question part (in tokens)
    output_len: 1000              # Target length for the model's generated output (in tokens)

report:
  request_lifecycle:
    summary: true
    per_stage: true
    per_request: true

storage:
  local_storage:
    path: /workspace