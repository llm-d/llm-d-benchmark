#LMBenchmark Workload Specification Example
model_name: "REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL"  # Model identifier
scenarios: "long-input"  # Scenarios to run (all, or sharegpt, long-input, short-input)
qps_values: "0.5 0.7 0.9 1 1.1 1.3 1.5 1.7 1.9 2 2.25 2.5 2.75 3 3.5 4 4.5 5"  # Space-separated list of QPS values to test
image: "lmcache/lmcache-benchmark:main"  # Container image to use
service_account: "REPLACE_ENV_LLMDBENCH_CLUSTER_SERVICE_ACCOUNT"  # Service account to use for the job
num_users_warmup: "REPLACE_NUM_USERS_DURING_WARMUP"
num_users: "REPLACE_NUM_USERS_FOR_SYNTHETIC_PREFIX_REUSE"
num_rounds: ""REPLACE_NUM_OF_ROUNDS_OF_USER_PREFIX_REUSE"
system_prompt: "REPLACE_SYSTEM_PROMPT_LENGTH"
chat_history: "REPLACE_CHAT_HISTORY_LENGTH"
answer_len: "REPLACE_ANSWER_LENGTH"
init_user_id: "REPLACE_INITIAL_USER_ID"
test_duration: "REPLACE_THE_DURATION_OF_BENCHMARKING_PERIOD_IN_SECONDS"
