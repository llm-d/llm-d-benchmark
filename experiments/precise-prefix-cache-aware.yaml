setup:
  constants:
    - LLMDBENCH_VLLM_COMMON_MAX_MODEL_LEN: 12000
    - LLMDBENCH_VLLM_COMMON_BLOCK_SIZE: 64
  factors:
    - LLMDBENCH_VLLM_MODELSERVICE_GAIE_PLUGINS_CONFIGFILE
  levels:
    LLMDBENCH_VLLM_MODELSERVICE_GAIE_PLUGINS_CONFIGFILE: default,prefix-cache-estimate-config,prefix-cache-tracking-config
  treatments:
    default: default
    cache_estimate: prefix-cache-estimate-config
    cache_tracking: prefix-cache-tracking-config
run:
# Harness and workload profile are defined on scenarios/guides
  constants:
    - streaming: true
  factors:
    - question_len
    - output_len
  levels:
    question_len: "100,300,1000"
    output_len: "100,300,1000"
  treatments:
    - "100,100"
    - "100,300"
    - "100,1000"
    - "300,100"
    - "300,300"
    - "300,1000"
    - "1000,100"
    - "1000,300"
    - "1000,1000"