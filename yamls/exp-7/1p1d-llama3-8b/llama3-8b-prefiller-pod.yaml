apiVersion: v1
kind: Pod
metadata:
  name: llama3-8b-prefiller
  namespace: vllm-prod
  labels:
    app: llama3-8b-prefiller
    llm-d.ai/inferenceServing: "true"
    llm-d.ai/model: "llama3-8b"
    llm-d.ai/role: "prefill"
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: nvidia.com/gpu.product
                operator: In
                values:
                  - NVIDIA-A100-SXM4-80GB
  containers:
    - name: vllm
      image: quay.io/llm-d/llm-d-dev:vllm-nixl-0.0.6
      securityContext:
        allowPrivilegeEscalation: false
      args:
        - "--model"
        - "meta-llama/Llama-3.1-8B-Instruct"
        - "--port"
        - "80"
        - "--enforce-eager"
        - "--kv-transfer-config"
        - '{"kv_connector":"NixlConnector","kv_role":"kv_both"}'
      env:
        - name: CUDA_VISIBLE_DEVICES
          value: "0"
        - name: UCX_TLS
          value: "cuda_ipc,cuda_copy,tcp"
        - name: VLLM_NIXL_SIDE_CHANNEL_PORT
          value: "5557"
        - name: VLLM_NIXL_SIDE_CHANNEL_HOST
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: VLLM_LOGGING_LEVEL
          value: DEBUG
        - name: HF_TOKEN
          valueFrom:
            secretKeyRef:
              name: vllm-standalone-hf-token
              key: token
      volumeMounts:
        - name: model-cache
          mountPath: /root/.cache/huggingface
      ports:
        - containerPort: 8000
          protocol: TCP
        - containerPort: 5557
          protocol: TCP
      resources:
        limits:
          nvidia.com/gpu: 1
        requests:
          cpu: "16"
          memory: 40Gi
          nvidia.com/gpu: 1
  volumes:
    - name: model-cache
      persistentVolumeClaim:
        claimName: model-cache
  restartPolicy: Never